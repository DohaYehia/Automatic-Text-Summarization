{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scispacy\n",
    "#!pip install rouge_score\n",
    "#!pip install gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install nltk\n",
    "#!pip install abbreviations\n",
    "#!pip install text-preprocessing\n",
    "#!pip install glove-python-binary\n",
    "#!pip install statistics\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download ('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (4.50.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import nltk.data\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, remove_email, remove_url, lemmatize_word,expand_contraction\n",
    "from text_preprocessing import remove_punctuation as punct\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "## importing libraries\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk import pos_tag\n",
    "#from sklearn.datasets import make_blobs\n",
    "#from sklearn.metrics import silhouette_score\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import statistics as st\n",
    "#from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "#from sklearn import metrics\n",
    "#from sklearn.decomposition import PCA\n",
    "#from scipy.cluster import hierarchy\n",
    "#from sklearn.cluster import AgglomerativeClustring\n",
    "from rouge_score import rouge_scorer\n",
    "#!pip install abbreviations\n",
    "from abbreviations import schwartz_hearst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scispacy.abbreviation.AbbreviationDetector at 0x19b81fc7640>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "def create_lang_detector(nlp, name):\n",
    "    return abbreviation_pipe\n",
    "Language.factory(\"abbreviation_pipe\", func=create_lang_detector)\n",
    "nlp.add_pipe(\"abbreviation_pipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path of the dataset DUC2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "doc_path = \"DucDataset/DUC2001_Summarization_Documents/data/test/docs\"\n",
    "summ1_path = \"DucDataset/DUC2001_Summarization_Documents/data/test/original.summaries\"\n",
    "summ2_path = \"DucDataset/DUC2001_Summarization_Documents/data/test/duplicate.summaries\"\n",
    "files = os.listdir(doc_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in the different types of file each has it's own way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_perdocs_xml(path):\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        xml = fp.read()\n",
    "\n",
    "    header_patt = r'<SUM.*?TYPE=\"PERDOC\"\\s+SIZE=\"(.*?)\"\\s+DOCREF=\"(.*?)\"' \\\n",
    "        '\\s+SELECTOR=\"(.*?)\"\\s+SUMMARIZER=\"(.*?)\"\\s*>'\n",
    "    summary_patt = header_patt + r'(.*?)</SUM>'\n",
    "\n",
    "    headers = [m for m in re.findall(header_patt, xml, flags=re.DOTALL)]\n",
    "    headers_found = len(headers)\n",
    "\n",
    "    data = {}\n",
    "    for match in re.findall(summary_patt, xml, flags=re.DOTALL):\n",
    "        size, doc_id, selector, summarizer, summary_text = match\n",
    "        doc_id = doc_id.strip()\n",
    "        sentences = summary_text\n",
    "        sentences = sentences.strip().replace('\\n', ' ')\n",
    "        \n",
    "        sentences = sent_tokenize(sentences)\n",
    "        data[doc_id] = sentences\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "    soup = BeautifulSoup(file)\n",
    "    text = []\n",
    "    for a in soup.find_all('text'):\n",
    "        for s in a.find_all('s'):\n",
    "            text.append(s.string)\n",
    "    return text\n",
    "    \n",
    "def parse_input_docs(paths,fn):\n",
    "\n",
    "    if fn.startswith(\"WSJ\"):\n",
    "        doc_id, Headline, text  = parse_wsj(paths)\n",
    "    elif fn.startswith(\"SJMN\"):\n",
    "        doc_id, Headline, text  = parse_sjmn(paths)\n",
    "    elif fn.startswith(\"FT\"):\n",
    "        doc_id, Headline, text = parse_ft(paths)\n",
    "    elif fn.startswith(\"AP\"):\n",
    "        doc_id, Headline, text  = parse_ap(paths)\n",
    "    elif fn.startswith(\"LA\"):\n",
    "        doc_id, Headline, text  = parse_la(paths)\n",
    "    elif fn.startswith(\"FBIS\"):\n",
    "        doc_id, Headline, text  = parse_fbis(paths)\n",
    "    else:\n",
    "        doc_id=\" \"\n",
    "        Headline = \" \"\n",
    "        text = \" \" \n",
    "    #if text != \" \":\n",
    "        #text = parse(text_path)\n",
    "\n",
    "    data = {\"doc_id\": doc_id,\"title\": Headline ,\"input_data\": text}\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_fbis(file_path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADER>(.*?)</HEADER>+', file)\n",
    "    if len(Headline[0].split('Document Title:')) > 1:\n",
    "        Headline = Headline[0].split('Document Title:')[-1]\n",
    "    else:\n",
    "        Headline = []\n",
    "    \n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = re.sub(r\"</?F.*?>\", r\" \", text) \n",
    "    text = sent_tokenize(text)       \n",
    "    return doc_id, Headline, text \n",
    "\n",
    "def parse_sjmn(path):\n",
    "\n",
    "    sentences = []\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)\n",
    "    headline = Headline\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "    out = text\n",
    "    return doc_id, headline, out\n",
    "\n",
    "def parse_wsj(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HL>(.*?)</HL>+', file)\n",
    "    headline = Headline\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, headline, text \n",
    "\n",
    "def parse_ft(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)[0]\n",
    "    headline = Headline.split(\"/\")[1:]\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, headline, text \n",
    "\n",
    "def parse_ap(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEAD>(.*?)</HEAD>+', file)\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, Headline, text \n",
    "def parse_la(file_path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)\n",
    "    Headline = re.findall(r'<P>(.*?)</P>+', Headline[0])\n",
    "\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = re.findall(r'<P>(.*?)</P>+', text)\n",
    "    #text = tokenizer.tokenize(text)\n",
    "            \n",
    "    return doc_id, Headline, text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1_files = os.listdir(summ1_path) \n",
    "ref_data1 = {}\n",
    "for file in ref1_files:\n",
    "    check = summ1_path + \"/\" + file + \"/perdocs\"\n",
    "    data =  parse_perdocs_xml(check)\n",
    "    input_path = doc_path + \"/\" +file[:-1]\n",
    "    in_ids = os.listdir(input_path)\n",
    "    ref_data1[file] = data\n",
    "    \n",
    "ref2_files = os.listdir(summ2_path) \n",
    "ref_data2 = {}\n",
    "for file in ref2_files:\n",
    "    check = summ2_path + \"/\" + file + \"/perdocs\"\n",
    "    data =  parse_perdocs_xml(check)\n",
    "    input_path = doc_path + \"/\" +file[:-1]\n",
    "    in_ids = os.listdir(input_path)\n",
    "    ref_data2[file] = data\n",
    "\n",
    "files = os.listdir(doc_path)\n",
    "input_data = {}\n",
    "Text_in = {}\n",
    "Summaries = {}\n",
    "for file in files:\n",
    "    file_path = doc_path + \"/\" + file\n",
    "    in_files = os.listdir(file_path)\n",
    "    file_data = {}\n",
    "    for In_in_file in in_files:\n",
    "        file_path = doc_path + \"/\" + file + \"/\" + In_in_file\n",
    "        In_data = parse_input_docs(file_path, In_in_file)\n",
    "        if In_data['input_data'] :\n",
    "            Text_in[In_in_file] = In_data\n",
    "        file_data[In_in_file] = In_data\n",
    "    input_data[file] = file_data\n",
    "\n",
    "input_summ1 = []\n",
    "len_Summaries=0\n",
    "for key,value in ref_data1.items():\n",
    "    for key_d , value_d in value.items():\n",
    "        summ = value_d\n",
    "        if key_d  in Text_in.keys():\n",
    "            if key_d  in Summaries.keys() :\n",
    "                Summaries[key_d].append(summ)\n",
    "                len_Summaries = len_Summaries + 1\n",
    "            else:\n",
    "                Summaries[key_d] = [summ]\n",
    "                len_Summaries = len_Summaries +1\n",
    "\n",
    "input_summ2 = []\n",
    "for key,value in ref_data2.items():\n",
    "    for key_d , value_d in value.items():\n",
    "        summ = value_d\n",
    "        if key_d  in Text_in.keys():\n",
    "            if key_d  in Summaries.keys():\n",
    "                    Summaries[key_d].append(summ)\n",
    "                    len_Summaries = len_Summaries + 1\n",
    "            else:\n",
    "                    Summaries[key_d] = [summ]\n",
    "                    len_Summaries = len_Summaries +1\n",
    "Inputs = Text_in\n",
    "Text_in = {}\n",
    "for Key, value in Summaries.items():\n",
    "    Text_in[Key] = Inputs[Key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for extracting variables needed from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(i,id_in):\n",
    "    \n",
    "    title_line = i['title']\n",
    "    text = i['input_data']\n",
    "    Summary = Summaries[id_in]\n",
    "    \n",
    "    title = []\n",
    "    keywords =[]\n",
    "    if(len(title_line) != 0):\n",
    "        for i in title_line:\n",
    "            title = title + word_tokenize(i)\n",
    "            \n",
    "    title = [item.lower() for item in title]\n",
    "        \n",
    "    lang_Dict = []\n",
    "    for line in text:\n",
    "        for word in line.split(\" \"):\n",
    "            if word not in lang_Dict:\n",
    "                lang_Dict.append(word)\n",
    "    iter_dict = lang_Dict\n",
    "    lang_Dict = []\n",
    "    for i in iter_dict:\n",
    "        if '\\n' in i:\n",
    "            lang_Dict = lang_Dict + i.split(\"\\n\")\n",
    "        else:\n",
    "            lang_Dict.append(i)\n",
    "     \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    \n",
    "    lang_dict = []\n",
    "    for word in lang_Dict:\n",
    "        if word.strip():\n",
    "            word = word.lower()\n",
    "            word = expand_contraction(word)\n",
    "            word = re.sub(pattern, '', word)\n",
    "            lang_dict.append(word)\n",
    "            \n",
    "    lang_Dict = []\n",
    "    for word in lang_dict:\n",
    "        if \" \" in word:\n",
    "            for w in word.split(\" \"):\n",
    "                if w.strip():\n",
    "                    lang_Dict.append(w)\n",
    "        else:\n",
    "            lang_Dict.append(word)\n",
    "    keyvalue = dict() \n",
    "\n",
    "    index= 0\n",
    "    for line in text:\n",
    "        if(line and (line.strip()) ):\n",
    "            i = index\n",
    "            keyvalue[i] = line\n",
    "            index = index + 1\n",
    "\n",
    "    key_value_list =[]\n",
    "    for item in keyvalue.items():\n",
    "        key_value_list.append(item)\n",
    "    \n",
    "    return title,keywords,keyvalue, key_value_list,Summary, lang_Dict\n",
    "title,keywords,keyvalue, key_value_list,Summary, lang_Dict = read_input(Text_in['FT923-5089'],'FT923-5089')\n",
    "#print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  functions needed for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronymss(keyvalue):\n",
    "        sent_dict = dict() \n",
    "        whole_text = \"\"\n",
    "        for key, text in keyvalue.items():\n",
    "            whole_text = whole_text + \"# \" + text\n",
    "        whole_text = whole_text.lower()\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=whole_text, most_common_definition=True)\n",
    "        for key,value in pairs.items():\n",
    "            whole_text = whole_text.replace(value, key)\n",
    "        output_text = whole_text.split(\"# \")\n",
    "        i = 0;\n",
    "        for l in output_text:\n",
    "            if l != \"\":\n",
    "                sentence = Brackets(l)\n",
    "                sent_dict[i] = sentence\n",
    "                i = i+1\n",
    "        return sent_dict\n",
    "\n",
    "#remove hyphens and tokenizing sentence\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = expand_contraction(word)\n",
    "        if word.lower() == \"n't\":\n",
    "            word = \"not\"\n",
    "        if word != '':\n",
    "            word = word.lower()\n",
    "            #word = expand_contraction(word)\n",
    "            new_words.append(word)\n",
    "    #print(words)\n",
    "    #print(\"after remove punck\")\n",
    "    return new_words\n",
    "\n",
    "def tokenize_sent(sent_dict):\n",
    "    keyvalueToken = dict() \n",
    "    TorF = True\n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        words = nltk.word_tokenize(expand_contraction(value))\n",
    "        words = [re.sub(r'(\\n-)','',word) for word in words]\n",
    "        words = [re.sub(r'(\\n)',' ',word) for word in words]\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                 new_words = new_words + word.split(\" \")\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        \n",
    "        keyvalueToken[key] = new_words\n",
    "        \n",
    "    return keyvalueToken\n",
    "#continue sent reduction\n",
    "def conjunctions(tokenize_sent_dict):\n",
    "    new_sent_dict = dict()\n",
    "    conjunctive_adverbs= [\"accordingly\",\"comparatively\",\"contrarily\", \n",
    "                      \"also\",\" further\", \"nevertheless\", \"indeed\",\" otherwise\",\n",
    "                      \"result\",\"as\",\"equally\" ,\"conversely\" ,\"besides\",\n",
    "                      \"furthermore\" ,\"nonetheless\" ,\"fact\" ,\"consequently\",\n",
    "                     \" likewise\" ,\"however\" ,\"addition\" \",moreover\" ,\"surprisingly\",\n",
    "                     \" hence\" ,\"similarly \",\"comparison\" ,\"still \",\"therefore \",\n",
    "                      \"contrast\" ,\"thus\" ,\"instead\"  ,\"rather\"\n",
    "                      , \"other\", \"additionally\", \"addition\",'although','already',\"including\",'according',\n",
    "                    'totally','likely','unlikely','perhaps',\"s\",'naturally',\n",
    "                      \"finally\", \"based\",\"on\",\"that\" , \"besides\",\"namely\",\"anyway\",\n",
    "                     \"then\",\"next\",\"thereafter\",\"certainly\",\"now\",\"finally\",\"meanwhile\",\n",
    "                     \"subsequently\",\"yet\",\"elsewhere\",\"thereafter\",\"undoubtedly\",\n",
    "                     \"incidentally\",\"otherwise\",\"regardless\",\"begin\",\"in\"]\n",
    "    \n",
    "    for key, text in tokenize_sent_dict.items():\n",
    "        new_sent = [word for word in text if word.lower() not in conjunctive_adverbs]\n",
    "        new_sent_dict[key] = new_sent\n",
    "\n",
    "    return new_sent_dict\n",
    "\n",
    "#d Sentence reduction \n",
    "def Brackets(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def get_key(val,my_dict): \n",
    "    for key, values in my_dict.items(): \n",
    "        for value in values:\n",
    "            if val == value: \n",
    "                return key \n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "def replace_word_synonym(new_sent_dict):\n",
    "    \n",
    "    text = []\n",
    "    sent_dict = dict()\n",
    "    dict_synonym = Synonym_replace(new_sent_dict)\n",
    "    for key, value in new_sent_dict.items():\n",
    "        \n",
    "        text = []\n",
    "        for word in value:\n",
    "            if get_key(word,dict_synonym) != \"key doesn't exist\":\n",
    "                text.append(get_key(word,dict_synonym))\n",
    "            else:\n",
    "                text.append(word)\n",
    "        sent_dict[key] = text\n",
    "    \n",
    "    return sent_dict\n",
    "\n",
    "def Synonym_replace(new_sent_dict):\n",
    "    synonymList = []\n",
    "    text = {}\n",
    "    for key, sentence in new_sent_dict.items():\n",
    "        for word in sentence:\n",
    "            if word not in synonymList:\n",
    "                wordNetSynset =  wn.synsets(word)\n",
    "                values = []\n",
    "                for synSet in wordNetSynset:\n",
    "                    for synWords in synSet.lemmas():\n",
    "                        if synWords.name() in synonymList:\n",
    "                            continue\n",
    "                        else:\n",
    "                            synonymList.append(synWords.name())\n",
    "                            values.append(synWords.name())\n",
    "                text[word] = values\n",
    "    return text\n",
    "#f Use words N-grams\n",
    "def ngramise(sequence):\n",
    "    bigrams = []\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def remove_hyfen(wordslist):\n",
    "    words_new = []\n",
    "    for word in wordslist:\n",
    "        digit = False\n",
    "        lang = False\n",
    "        if \"-\" in word:\n",
    "            words = word.split(\"-\")\n",
    "            for w in words:\n",
    "                if w.isdigit():\n",
    "                    digit = True\n",
    "            if digit:\n",
    "                newwords = [w for w in words]\n",
    "                words_new = words_new + newwords\n",
    "            else:\n",
    "                for w in words:\n",
    "                    if w in lang_Dict:\n",
    "                        lang = True\n",
    "                if lang:\n",
    "                    newwords = [w for w in words]\n",
    "                    words_new = words_new + newwords\n",
    "                else:\n",
    "                    newwords = [word]\n",
    "                    words_new = words_new + newwords\n",
    "        else:\n",
    "            newwords = [word]\n",
    "            words_new = words_new + newwords\n",
    "    return words_new\n",
    "#4.3. Word frequency computation\n",
    "def EF(sent_dict):\n",
    "    final_sent = dict()\n",
    "    all_words = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #word_tokens = word_tokenize(text) \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    \n",
    "    for key, value in sent_dict.items():\n",
    "        value = remove_hyfen(value)\n",
    "        value = remove_punctuation(value)\n",
    "        filtered_sentence = [w for w in value if w not in stopWords] \n",
    "        #filtered_sentence = [re.sub(r'[\" \"]+', '', word) for word in filtered_sentence]\n",
    "        #replace everything not in the list with space\n",
    "        filtered_sentence = [re.sub(\"'\", '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub('\"', '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(pattern, '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(r'[0-9]+', '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word for word in filtered_sentence if len(word.strip()) >1]\n",
    "        filtered_sentence = [word for word in filtered_sentence if (re.sub(r\"[^a-zA-Z-]+\", '', word).strip())]\n",
    "        #filtered_sentence = [re.sub(r\"[^a-zA-Z-]+\", '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word.strip() for word in filtered_sentence]\n",
    "\n",
    "        wordnet_lemmatizer = WordNetLemmatizer() \n",
    "            \n",
    "        filtered_sentence = [wordnet_lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in filtered_sentence if  word != \" \" and word != '']\n",
    "        \n",
    "        final_sent[key] = filtered_sentence\n",
    "        all_words = all_words + filtered_sentence\n",
    "    word_frequency = list(Counter(all_words).items())\n",
    "    return word_frequency, final_sent , all_words\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    #detection of appreviation\n",
    "    sent_dict = replace_acronymss(keyvalue)\n",
    "\n",
    "    tokenize_sent_dict = tokenize_sent(sent_dict)\n",
    "    \n",
    "    new_sent_dict = conjunctions(tokenize_sent_dict)\n",
    "    \n",
    "    #e Replacement of word synonyms\n",
    "    sent_dict = replace_word_synonym(new_sent_dict)\n",
    "    sent_list =[]\n",
    "    for item in sent_dict.items():\n",
    "        sent_list.append(item)\n",
    "\n",
    "    all_bigrams = []\n",
    "    for key, value in sent_dict.items():\n",
    "        bigrams = ngramise(value)\n",
    "        all_bigrams = all_bigrams+bigrams\n",
    "\n",
    "    bi_count = dict(Counter(all_bigrams))\n",
    "    delete = [key for key,value in bi_count.items() if value < 2]\n",
    "    # delete the key \n",
    "    for key in delete: del bi_count[key]\n",
    "    bi_count_list =[]\n",
    "    for key , value in bi_count.items():\n",
    "        bi_count_list.append([(key),value])\n",
    "        \n",
    "    \n",
    "    word_freq ,final_sent, all_words = EF(sent_dict)\n",
    "    Sentences_list = []\n",
    "    for item in final_sent.items():\n",
    "        Sentences_list.append(item)     \n",
    "    \n",
    "    #word frequencies\n",
    "    #freq of words in title\n",
    "    title_freq = Counter(title)\n",
    "\n",
    "    Keyword_freq = Counter(keywords)\n",
    "    #Keyword_freq = keywords\n",
    "    #print(Keyword_freq)\n",
    "    w_freq = Counter(all_words)\n",
    "    final_Word_freq = dict()\n",
    "    for word in all_words:\n",
    "        if word not in title_freq.keys():\n",
    "            title_freq[word] = 0\n",
    "        if word not in Keyword_freq.keys():\n",
    "            Keyword_freq[word] = 0\n",
    "        final_Word_freq[word] = title_freq[word] + Keyword_freq[word] + w_freq[word] \n",
    "\n",
    "    ##final list of sentences after preprocessing\n",
    "    final_input = []\n",
    "    for (index, sentence) in Sentences_list:\n",
    "        final_input.append(\" \".join(sentence))\n",
    "        \n",
    "    keyword_word_freq = []\n",
    "    for word in all_words:\n",
    "        keyword_word_freq.append([word,Keyword_freq[word]])  \n",
    "    #print(keyword_word_freq)\n",
    "    title_word_freq = []\n",
    "    for word in all_words:\n",
    "        title_word_freq.append([word, title_freq[word]])\n",
    "\n",
    "    Sentences_list\n",
    "    List_of_sent = dict()\n",
    "    for (key, sent) in Sentences_list:\n",
    "        List_of_sent[key] = \" \".join(sent)\n",
    "\n",
    "    final_Word_freq\n",
    "    finalwordfreq = []\n",
    "    for key , value in final_Word_freq.items():\n",
    "        finalwordfreq.append([key,value])\n",
    "    #finalwordfreq\n",
    "    return (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "            ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq)\n",
    "    \n",
    "(title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    " ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess() \n",
    "\n",
    "def bigrams_freq(bi_count_list):\n",
    "    words_freq_bigram = dict()\n",
    "    bi_count = []\n",
    "    for list_bi in bi_count_list:\n",
    "        (w1,w2), i = list_bi\n",
    "        if w1 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "        else:\n",
    "            words_freq_bigram[w1] = i\n",
    "        if w2 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "        else:\n",
    "            words_freq_bigram[w2] = i   \n",
    "    for key,value in words_freq_bigram.items():\n",
    "        bi_count.append([key,value])\n",
    "    return bi_count\n",
    "bi_count = bigrams_freq(bi_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq_bigram = dict()\n",
    "for list_bi in bi_count_list:\n",
    "    (w1,w2), i = list_bi\n",
    "    if w1 in words_freq_bigram.keys():\n",
    "        words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "    else:\n",
    "        words_freq_bigram[w1] = i\n",
    "    if w2 in words_freq_bigram.keys():\n",
    "        words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "    else:\n",
    "        words_freq_bigram[w2] = i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights\n",
    "def get_weights(list_of_nodes):\n",
    "    import pandas as ps\n",
    "    import statistics as st\n",
    "    node_weight_dict = dict()\n",
    "    node_list= list_of_nodes\n",
    "    FWL= finalwordfreq\n",
    "    TWL= title_word_freq\n",
    "    KWL= keyword_word_freq\n",
    "    BWL= bi_count \n",
    "    PWL= Pnouns\n",
    "    #BWL = []\n",
    "    #PWL = []\n",
    "    \n",
    "\n",
    "    F=[]     #freq of words  which appeared in nodes WRT document\n",
    "    T=[]     #freq of words  which appeared in nodes WRT title\n",
    "    K=[]     #freq of words  which appeared in nodes WRT keyword list \n",
    "    B=[]     #freq of words which appeared in nodes WRT Bi_Gram list \n",
    "    P=[]     #list of detection the proper noune \n",
    "    FW=[]    \n",
    "    Xt,Xk,Xbi,Xp,Xb=1,1,1,1,1\n",
    "    weighted_node=[]\n",
    "    \n",
    "    \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "    \n",
    "        for j in FWL:\n",
    "            if(i==j[0]):\n",
    "                f=j[1]\n",
    "                F.append(f)\n",
    "                flag=1\n",
    "                \n",
    "        if(flag!=1): \n",
    "            F.append(0) \n",
    "        \n",
    " \n",
    "\n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in TWL:\n",
    "            if(i==j[0]):\n",
    "                t=j[1]\n",
    "                T.append(t)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "             T.append(0)\n",
    "                \n",
    "        \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in KWL:\n",
    "            if(i==j[0]):\n",
    "                k=j[1]\n",
    "                K.append(k)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            K.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in BWL:\n",
    "            if(i==j[0]):\n",
    "                b=j[1]\n",
    "                B.append(b)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            B.append(0)\n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in PWL:\n",
    "            if(i==j):\n",
    "                P.append(1)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            P.append(0)             \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in FWL:\n",
    "        fw=i[1]\n",
    "        FW.append(fw)\n",
    "    \n",
    "    if FW == []:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "    elif len(FW) != 0:\n",
    "        avrage_word_frequency=st.mean(FW)\n",
    "        median_word_frequency=st.median(FW) \n",
    "    else:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "        \n",
    "    AW=abs(avrage_word_frequency-median_word_frequency)\n",
    "    \n",
    "    for i in range(len(node_list)):\n",
    "        WN=F[i]+Xt*AW*T[i]+Xk*AW*K[i]+Xb*AW*B[i]+Xp*AW*P[i]\n",
    "        weighted_node.append(WN)\n",
    "        node_weight_dict[node_list[i]] = WN\n",
    "\n",
    "    return node_weight_dict,weighted_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#edge between source and distenation\n",
    "def get_edge(source,distenation):\n",
    "    return edgeget[source,distenation]\n",
    "\n",
    "\n",
    "#distination node list\n",
    "def getweights_of_dist_nodes(source_destinations,sourcenode,All_nodes_ww):  \n",
    "    x=source_destinations[sourcenode]\n",
    "    distenation_node_list={}\n",
    "    weights=[]\n",
    "    nothing=0\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i])==('E#'))  or ( x[i] not in All_nodes_ww):\n",
    "            nothing=1\n",
    "        else:\n",
    "            distenation_node_list[x[i]]=All_nodes_ww[x[i]]\n",
    "            weights.append((All_nodes_ww[x[i]]))\n",
    "        if len(weights) != 0:\n",
    "            nothing=0\n",
    "    return distenation_node_list,weights,nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm 2 get candidate edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for candidate edges between a source and a destination given a criteria \n",
    "def get_candidate_edges(source_destinations,node_listx,wxx,out_carteria,All_weight,input_nodes,All_SD):\n",
    "    c=[]\n",
    "    xxx = []\n",
    "    no_list,totalweights=node_listx,All_weight\n",
    "\n",
    "    source_node_weight=0\n",
    "    distination_node_weight=0\n",
    "    average_of_the_node_weight=np.average(totalweights)#(np.sum(node_list)/number_of_nodes)\n",
    "    median_of_the_node_list_weight=np.median(totalweights)\n",
    "  \n",
    "    if average_of_the_node_weight>median_of_the_node_list_weight:#can be replaced with source=max(avg,median)\n",
    "        source_node_weight=average_of_the_node_weight\n",
    "    else:\n",
    "        source_node_weight=median_of_the_node_list_weight\n",
    "\n",
    "    for node,weight in no_list.items():\n",
    "        \n",
    "        if weight>=source_node_weight:#بيعرف اذا كانت بدايه الجمله ولا لا \n",
    "            outnodes1,outnodesweights,flagnothing=getweights_of_dist_nodes(source_destinations,node,no_list)#فنكشن جوا الجي بتجبلي كل الديستناشن بتاعت النودواختار بينهم هسقط مين\n",
    "            outnodes2,nodes2weights,flag2nothing=getweights_of_dist_nodes(All_SD,node,input_nodes)\n",
    "            outnodes1=[(i,j) for (i,j) in outnodes1.items() if j!=0 ] \n",
    "            outnodes2 =[(i,j) for (i,j) in outnodes2.items() if j!=0 ] \n",
    "        \n",
    "            if flagnothing==0:\n",
    "                #destination nodes exsist\n",
    "                #get their average and max weights for avg and max criterias\n",
    "                average_weight=np.average(nodes2weights)\n",
    "                max_weight=max(nodes2weights)\n",
    "            else:\n",
    "                #no destination nodes \n",
    "                average_weight=0\n",
    "                max_weight=0\n",
    "            # get threshold of destination weights according to criteria \n",
    "            if (out_carteria ==\"avg\"):\n",
    "                distination_node_weight=average_weight\n",
    "            elif (out_carteria ==\"node_avg\"):\n",
    "                distination_node_weight=source_node_weight\n",
    "            elif (out_carteria==\"max\"):\n",
    "                distination_node_weight=max_weight\n",
    "            else:\n",
    "                distination_node_weight=average_weight\n",
    "\n",
    "\n",
    "            if (bool(outnodes1)==False):\n",
    "                # no destination nodes \n",
    "                nothing=1  \n",
    "            else: \n",
    "                # check for the candidate edges\n",
    "                for out,weight_of_out_node in outnodes1:\n",
    "                    if weight_of_out_node>=distination_node_weight and flagnothing==0:\n",
    "                        c.append(get_edge(node,out))# append the edge\n",
    "                        xxx.append([node,out])\n",
    "\n",
    "    cx=[]\n",
    "    for i in range(0,len(c)):\n",
    "        cx.append(c[i][0])\n",
    "    return cx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_order(count_summ, index):\n",
    "    return(count_summ - index)/count_summ\n",
    "\n",
    "def compute_rank(candidate_summary):#, W, core_words_list, bigrams_list, lsa_weight):\n",
    "    ranks = {}\n",
    "    candidate_summary = reorder(candidate_summary)\n",
    "    \n",
    "    for i, sent in enumerate(candidate_summary):\n",
    "        \n",
    "        rank = sentence_order(len(candidate_summary), i)\n",
    "        ranks[sent] = rank\n",
    "        \n",
    "    sort_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)    \n",
    "    return ranks   \n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "def get_words_count(sent):\n",
    "    len_sent = 0\n",
    "    for i in sent.split(\" \"):\n",
    "        len_sent = len_sent +1\n",
    "    return len_sent\n",
    "\n",
    "# reorder the summary based on the order from the input text\n",
    "def reorder(summary):\n",
    "    index_summ = []\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    order = dict()\n",
    "    final_order = []\n",
    "    for sent in summary:\n",
    "        if sent in value_sent:\n",
    "            index_sent = value_sent.index(sent)\n",
    "            index_summ.append(index_sent)\n",
    "            order[index_sent] = sent\n",
    "            \n",
    "    for i in sorted(order):\n",
    "        final_order.append(order[i])\n",
    "    #print(sorted(index_summ))\n",
    "    return final_order\n",
    "\n",
    "def concat(final_order):\n",
    "    return(\"\\n\".join(final_order))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post processing and algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(list_of_candidates_summeries):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    sentences_ranks = compute_rank(candidate_summary)\n",
    "   \n",
    "    sentences_ordered =  [key for key,value  in sentences_ranks.items()]\n",
    "    summary_length =0\n",
    "    visited_sentences = []\n",
    "    \n",
    "    sentences = sentences_ordered\n",
    "\n",
    "    tfidfvect = TfidfVectorizer(stop_words='english')\n",
    "    X = tfidfvect.fit_transform(sentences)\n",
    "      \n",
    "    ##########\n",
    "    if len(sentences) <1:\n",
    "        n_clusters = len(sentences)\n",
    "    else:\n",
    "        n_clusters = 1\n",
    "    clf = KMeans(n_clusters = n_clusters,init = 'k-means++', max_iter = 1000)\n",
    "    labels = clf.fit_predict(X)\n",
    "    clustered_sentences =[[] for i in range(5)]\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['data_index'] = sentences\n",
    "    cluster_map['cluster'] = clf.labels_\n",
    "    #print(cluster_map)\n",
    "    return n_clusters,cluster_map\n",
    "\n",
    "def algorithm_4(summary_length,threshold,max_words_limit):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    summary_length = 0\n",
    "    while (len(visited_sentences) < len(candidate_summary)) and ((summary_length)< max_words_limit) :\n",
    "        for i in range(n_clusters):\n",
    "            \n",
    "            clustered_sentences = cluster_map[cluster_map.cluster == i]['data_index'].tolist()\n",
    "    \n",
    "            if summary_length >= (max_words_limit+threshold):\n",
    "                break\n",
    "            for sent in clustered_sentences:\n",
    "                if sent not in visited_sentences :\n",
    "                    temp_length = summary_length + get_words_count(sent)\n",
    "                    if temp_length < max_words_limit :\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif(temp_length >= max_words_limit) and (temp_length <= (max_words_limit + threshold)):\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif temp_length > (max_words_limit + threshold):\n",
    "                        visited_sentences.append(sent)\n",
    "        \n",
    "    return final_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to calculate Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(outsumm,Summary):\n",
    "    score={}\n",
    "\n",
    "    generated_summary = outsumm\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    s_R1 =[]\n",
    "    s_R2 = []\n",
    "    s_RL = []\n",
    "    for summ in Summary:\n",
    "        score = scorer.score( '\\n'.join(summ) ,generated_summary)\n",
    "        s_R1.append(score['rouge1'][1])\n",
    "        s_R2.append(score['rouge2'][1])\n",
    "        s_RL.append(score['rougeLsum'][1])\n",
    "\n",
    "    scores = [max(s_R1), max(s_R2) , max(s_RL)]\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a vertex to the dictionary\n",
    "def add_vertex(v):\n",
    "  global graph\n",
    "  global vertices_no\n",
    "  if v not in graph:\n",
    "    vertices_no = vertices_no + 1\n",
    "    #print(\"vertices_no \",vertices_no, v)\n",
    "    graph[v] = []\n",
    "\n",
    "# Add an edge between vertex v1 and v2 with edge weight e\n",
    "#v1 is the source node and v2 is the distenation node\n",
    "edgeget={}\n",
    "def add_edge(v1, v2, e_order, e_value):\n",
    "  global graph\n",
    "  # Check if vertex v1 is a valid vertex\n",
    "  if ((v1 in graph) and (v2 in graph)):\n",
    "    #print(v1,v2,e_order,e_value)  \n",
    "    temp = [v2, e_order, e_value]\n",
    "    edgeget[v1,v2]=[e_value]\n",
    "    graph[v1].append(temp)\n",
    "\n",
    "# Print the graph\n",
    "def print_graph():\n",
    "  global graph\n",
    "  for vertex in graph:\n",
    "    for edges in graph[vertex]:\n",
    "      #print(\"evet\")\n",
    "      print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "# get the destinations\n",
    "def get_destinations(source):\n",
    "    global graph\n",
    "    distinations=[]\n",
    "    if(source in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0] not in distinations):\n",
    "                distinations.append(elements[0])\n",
    "    return distinations\n",
    "\n",
    "# get the destinations\n",
    "def get_edges(source,dist):\n",
    "    global graph\n",
    "    Edge=\"\"\n",
    "    if(source in graph and dist in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0]==dist):\n",
    "                Edge= Edge + elements[2]\n",
    "    return Edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria):\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    input_text = candidate_summ\n",
    "    last_summ_len = candidate_summ_len\n",
    "    Last_summ = candidate_summ\n",
    "    sent_idx = can_edges\n",
    "    can_edges=[]\n",
    "    List_of_sent = list_pre_cansumm\n",
    "    list_pre_cansumm = []\n",
    "    list_of_candidates_summeries = []\n",
    "    All_nodes=[]\n",
    "    nodes_list = []\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "  \n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "        Types=pos_tag(Words)\n",
    "        for Type in Types:\n",
    "            if (Type[1] == 'NN' or Type[1] == 'NNP' or Type[1] == 'NNS' or Type[1] == 'NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "    \n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\") \n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        LIST_OF_NODES=[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "        \n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "                \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "            \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[index] = (source_destinations,list_of_nodes)\n",
    "        \n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        index = index + 1\n",
    "        \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        \n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[index] and (List_of_sent[index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "        index = index + 1\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    candidate_summ_len = 0\n",
    "    for index ,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "    return list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to get index of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sent):\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    index_sent = value_sent.index(sent)\n",
    "    return index_sent    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final function\n",
    "## takes in input and outputs a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_scores =[]\n",
    "final_scores_1=[]\n",
    "final_scores_2=[]\n",
    "final_scores_L=[]\n",
    "final_scores_1F=[]\n",
    "final_scores_2F=[]\n",
    "final_scores_LF=[]\n",
    "final_scores_11 = []\n",
    "final_scores_21 =[]\n",
    "final_scores_L1 = []\n",
    "\n",
    "final_scores_12 = []\n",
    "final_scores_22 = []\n",
    "final_scores_L2 = []\n",
    "\n",
    "\n",
    "\n",
    "for i in Text_in:\n",
    "    global vertices_no\n",
    "    global keyvalue\n",
    "    global key_value_list\n",
    "    global lang_Dict\n",
    "    global Sentences_list\n",
    "    global final_input\n",
    "    global all_bigrams\n",
    "    global bi_count_list\n",
    "    global word_freq\n",
    "    global all_words\n",
    "    global finalwordfreq\n",
    "    global List_of_sent\n",
    "    global title_word_freq\n",
    "    global keyword_word_freq\n",
    "    global All_nodes\n",
    "    global list_of_candidates_summeries\n",
    "    global list_pre_cansumm\n",
    "    global Word_Freq\n",
    "    global bi_count\n",
    "    global Cand_edge\n",
    "    global graph \n",
    "    global Pnouns\n",
    "    out_carteria=\"avg\"\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    vertices_no =0\n",
    "    can_edges = []\n",
    "    \n",
    "    title,keywords,keyvalue, key_value_list,Summary, lang_Dict = read_input(Text_in[i],i) \n",
    "    \n",
    "    (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "         ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess()\n",
    "\n",
    "    Last_summ = keyvalue.values()\n",
    "    last_summ_len = 0\n",
    "    for value in keyvalue.values():\n",
    "        last_summ_len = last_summ_len + get_words_count(value)\n",
    "    \n",
    "    \n",
    "    All_nodes=[]\n",
    "    list_of_candidates_summeries=[]\n",
    "    list_pre_cansumm = []\n",
    "\n",
    "    ##tokanization  \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    ##lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    \n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    #print(S.values())\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}\n",
    "    Pnouns = []\n",
    "    for sent_index, sentence in enumerate(List_of_sent.values()):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        \n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "        \n",
    "        Types=pos_tag(Words)\n",
    "\n",
    "        for Type in Types:\n",
    "            if (Type[1] == 'NN' or Type[1] == 'NNP' or Type[1] == 'NNS' or Type[1] == 'NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "  \n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "                if(Type[1] == 'NNP'):\n",
    "                    Pnouns.append(\"\")\n",
    "            \n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "\n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "        \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[sent_index] = (source_destinations,list_of_nodes)\n",
    "\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "\n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    for sent_index, sentence in enumerate(S.values()): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[sent_index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        \n",
    "        if len(list_of_can_edges)>=1:\n",
    "            if(List_of_sent[sent_index] and (List_of_sent[sent_index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[sent_index])\n",
    "                can_edges.append(sent_index)\n",
    "                \n",
    "\n",
    "    max_words_limit= 100\n",
    "    threshold = 15\n",
    "    final_summary = []\n",
    "    visited_sentences = []\n",
    "    summary_length = 0\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    \n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "    \n",
    "\n",
    "    while(True):\n",
    "        if(candidate_summ_len > (max_words_limit + threshold)):\n",
    "            if(candidate_summ_len != last_summ_len):\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria) \n",
    "\n",
    "            elif(out_carteria != \"max\"):\n",
    "                out_carteria=\"max\"\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria)\n",
    "            else:\n",
    "                list_of_candidates_summeries = candidate_summ\n",
    "                break\n",
    "        elif(candidate_summ_len < (max_words_limit)):\n",
    "            candidate_summ = Last_summ\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "        else:\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "            \n",
    "    n_clusters,cluster_map = post_processing(list_of_candidates_summeries)\n",
    "    final_summary = algorithm_4(summary_length,threshold,max_words_limit)\n",
    "    final = reorder(final_summary)\n",
    "    outsumm = concat(final)\n",
    "    scores = get_score(outsumm,Summary)\n",
    "\n",
    "\n",
    "    final_scores_1.append(scores[0])\n",
    "    final_scores_2.append(scores[1])\n",
    "    final_scores_L.append(scores[2])\n",
    "\n",
    "\n",
    "avrage_1=st.mean(final_scores_1)\n",
    "avrage_2=st.mean(final_scores_2)\n",
    "avrage_L=st.mean(final_scores_L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5084016570399649 0.2901603558578633 0.4725996368789331\n"
     ]
    }
   ],
   "source": [
    "print(avrage_1,avrage_2,avrage_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
