{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "C:\\Users\\lenovo\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import pyarabic.araby as araby\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "#!pip install scispacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster import hierarchy\n",
    "from abbreviations import schwartz_hearst\n",
    "import qalsadi.analex as qa\n",
    "import pyarabic.araby as araby\n",
    "import naftawayh.wordtag\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in the input \n",
    "## takes string input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_ar(text):\n",
    "    \n",
    "    title = []\n",
    "    keywords =[]\n",
    "\n",
    "    lang_Dict = []\n",
    "    for line in araby.sentence_tokenize(text):\n",
    "        for word in line.split(' '):\n",
    "            if word not in lang_Dict:\n",
    "                lang_Dict.append(word)\n",
    "    \n",
    "    iter_dict = lang_Dict\n",
    "    lang_Dict = []\n",
    "    for i in iter_dict:\n",
    "        if '\\n' in i:\n",
    "            lang_Dict = lang_Dict + i.split(\"\\n\")\n",
    "        else:\n",
    "            lang_Dict.append(i)\n",
    "            \n",
    "    lang_Dict = [word.replace(\".\",\"\") for word in lang_Dict]\n",
    "    keyvalue = dict() \n",
    "\n",
    "    i = 0\n",
    "    for index, line in enumerate(araby.sentence_tokenize(text)):\n",
    "        new_t = []\n",
    "        final = []\n",
    "        if \"\\n\" in line:\n",
    "            line = line.split('\\n' )\n",
    "            new_t = new_t + line\n",
    "        else:\n",
    "            new_t.append(line)\n",
    "        for m in new_t:\n",
    "            if \".\" in m:\n",
    "                l = m.split('.' )\n",
    "                final = final + l\n",
    "            else:\n",
    "                final.append(m)\n",
    "        for v in final:\n",
    "            if (v.strip() != '') and (v.strip() != \" \"):\n",
    "                keyvalue[i] = str(v)\n",
    "                i = i + 1\n",
    "\n",
    "    key_value_list =[]\n",
    "    for item in keyvalue.items():\n",
    "        key_value_list.append(item)\n",
    "    return title,keywords,keyvalue, key_value_list, lang_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function rplaces the appreviation in the text \n",
    "#for example  Natural language processing --> NLP\n",
    "def replace_acronymss(keyvalue):\n",
    "        sent_dict = dict() \n",
    "        whole_text = \"\"\n",
    "        for key, text in keyvalue.items():\n",
    "            whole_text = whole_text + \"#*# \" + text\n",
    "        whole_text = whole_text.lower()\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=whole_text, most_common_definition=True)\n",
    "        for key,value in pairs.items():\n",
    "            whole_text = whole_text.replace(value, key)\n",
    "        output_text = whole_text.split(\"#*# \")\n",
    "        i = 0;\n",
    "        for l in output_text:\n",
    "            if l != \"\":\n",
    "                sentence = Brackets(l)\n",
    "                sent_dict[i] = sentence\n",
    "                i = i+1\n",
    "        return sent_dict\n",
    "    \n",
    "def tokenize_sent(sent_dict):\n",
    "    keyvalueToken = dict() \n",
    "    TorF = True\n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        words = nltk.word_tokenize(value)\n",
    "        words = [re.sub(r'(\\n-)','',word) for word in words]\n",
    "        words = [re.sub(r'(\\n)',' ',word) for word in words]\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                 new_words = new_words + word.split(\" \")\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        words = new_words   \n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \"-\" in word:\n",
    "                TorF = True\n",
    "                check = word.split(\"-\")\n",
    "                for i in check:\n",
    "                    if i not in lang_Dict:\n",
    "                        TorF = False\n",
    "                if TorF:\n",
    "                    new_words = new_words + check\n",
    "                else:\n",
    "                    new_words.append(word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "\n",
    "        keyvalueToken[key] = new_words    \n",
    "    return keyvalueToken\n",
    "\n",
    "#continue sent reduction\n",
    "def conjunctions_ar(tokenize_sent_dict):\n",
    "    new_sent_dict = dict()\n",
    "    conjunctive_adverbs= [\"وفقا لذلك\",\"أيضاً\",\"على أي حال\",\"إلى جانب ذلك\",\"بالتأكيد\",\"وبالتالي\",\"طرداً وعكساً\",\"أخيراً\",\"علاوة على ذلك\",\"بيد أنّ\",\"بالتبعية\",\"فعلاً\",\"إنّ\",\"بدلاً من أن\",\"بطريقة مماثلة\",\"على الرغم من أنّ\",\"ومع ذلك\",\"بغض النظر\",\"بصرف النظر\",\"بعبارة أخرى\",\"بعبارة أخرى يمكن القول بأن\",\"معناه بعبارة أكثر دقة\",\"جل ما في الأمر أنّ\",\"أو على الأصح\",\"بالتحديد\",\"على وجة التحديد\",\"لا يعني بالضرورة\"]\n",
    "    \n",
    "    for key, text in tokenize_sent_dict.items():\n",
    "        new_sent = [word for word in text if word.lower() not in conjunctive_adverbs]\n",
    "        new_sent_dict[key] = new_sent\n",
    "\n",
    "    return new_sent_dict\n",
    "\n",
    "#d Sentence reduction \n",
    "def Brackets(text):\n",
    "    \n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\().*?(\\)]\", \"\", text)\n",
    "    \n",
    "    text=re.sub(r\"[\\[].*[\\]]\",r\"\",text)\n",
    "    text=re.sub(r'[0-9]+',r'',text)\n",
    "    text=re.sub(r\"[\\().*(/)]\",r\"\",text)\n",
    "    \n",
    "    \n",
    "    text=re.sub(r\"\\s+\",r\" \",text).strip()\n",
    "    return text\n",
    "\n",
    "def AR_POS(Words):\n",
    "    # using two libraries qalsadi and naftawayh\n",
    "\n",
    "    nodes_list = [\"S#\"]\n",
    "    edges_list = [\"\"]\n",
    "    tagger = naftawayh.wordtag.WordTagger()\n",
    "    debug=False;\n",
    "    limit=500\n",
    "    nouns = []\n",
    "    \n",
    "    for i in range(1,(len(Words))):\n",
    "        \n",
    "        analyzer = qa.Analex()\n",
    "        analyzer.set_debug(debug);\n",
    "        result = analyzer.check_text(Words[i]);\n",
    "        T = repr(result[0][0]).split(\",\")[10].split(\"=\")[1]\n",
    "        if 'noun' in T.lower():\n",
    "            nouns.append(Words[i])\n",
    "            nodes_list.append(Words[i])\n",
    "            edges_list.append(\"\")\n",
    "        elif 'verb' in T.lower():\n",
    "            nouns.append(Words[i])\n",
    "            nodes_list.append(Words[i])\n",
    "            edges_list.append(\"\")\n",
    "        elif 'unknown' in T.lower():\n",
    "            if tagger.is_noun(Words[i]):\n",
    "                nouns.append(Words[i])      \n",
    "        else:\n",
    "            edges_list.append(Words[i])\n",
    "            nodes_list.append(\"\")\n",
    "            \n",
    "    return nouns , nodes_list , edges_list\n",
    "#stemmer\n",
    "def stemming_words(list_of_words):\n",
    "    stem_list_of_words=[]\n",
    "    for word in list_of_words:\n",
    "        if len(word) >= 5:\n",
    "            if word.startswith(\"وال\") or word.startswith(\"كال\") or word.startswith(\"بال\") or word.startswith(\"فال\") :\n",
    "                word=word[3:len(word)]\n",
    "            elif word.startswith(\"ال\"):\n",
    "                word=word[2:len(word)]\n",
    "            if word.endswith(\"تها\") or  word.endswith(\"تهم\") or  word.endswith(\"تكم\") or  word.endswith(\"تنا\") :\n",
    "                word=word[0:len(word)-2]\n",
    "                word=word.replace(\"ت\",\"ة\")\n",
    "            elif word.endswith(\"ها\") or  word.endswith(\"هم\") or  word.endswith(\"كم\") or  word.endswith(\"نا\") :\n",
    "                word=word[0:len(word)-2]\n",
    "        stem_list_of_words.append(word)\n",
    "    return stem_list_of_words\n",
    "\n",
    "#normalization is to remove taskeel, harakat, shadda and hamza \n",
    "def arabic_normalization(Text):\n",
    "    text=araby.strip_harakat(Text)\n",
    "    text=araby.strip_shadda(text)\n",
    "    text=araby.strip_tatweel(text)\n",
    "    text=araby.strip_tashkeel(text)\n",
    "    text=araby.normalize_hamza(text)\n",
    "    #text=araby.normalize_ligature(text) #use this in post processing to recap what's normalized\n",
    "    #remove set of .? and replace it by space then the symbole\n",
    "    text=re.sub(r\"([,.?!])\",r\" \\1\",text)\n",
    "    #remove sequence of white spaces \n",
    "    text=re.sub(r\"\\s+\",r\" \",text).strip()\n",
    "    return text\n",
    "\n",
    "# to get the ngrames from a list of words\n",
    "def ngramise(sequence):\n",
    "    bigrams = []\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "# removing the hyfen from word an checking if new words exist in the words of the document\n",
    "#if they exist then the hyfen is removed if not then it is considered one word\n",
    "def remove_hyfen(wordslist):\n",
    "    words_new = []\n",
    "    for word in wordslist:\n",
    "        digit = False\n",
    "        lang = False\n",
    "        if \"-\" in word:\n",
    "            words = word.split(\"-\")\n",
    "            for w in words:\n",
    "                if w.isdigit():\n",
    "                    digit = True\n",
    "            if digit:\n",
    "                newwords = [w for w in words]\n",
    "                words_new = words_new + newwords\n",
    "            else:\n",
    "                for w in words:\n",
    "                    if w in lang_Dict:\n",
    "                        lang = True\n",
    "                if lang:\n",
    "                    newwords = [w for w in words]\n",
    "                    words_new = words_new + newwords\n",
    "                else:\n",
    "                    newwords = [word]\n",
    "                    words_new = words_new + newwords\n",
    "        else:\n",
    "            newwords = [word]\n",
    "            words_new = words_new + newwords\n",
    "    return words_new\n",
    "#remove hyphens and tokenizing sentence\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word == \"م\":\n",
    "            word = \"\"\n",
    "        if word != '':\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# get the frequency of each word in the bigrams\n",
    "def bigrams_freq(bi_count_list):\n",
    "    words_freq_bigram = dict()\n",
    "    bi_count = []\n",
    "    for list_bi in bi_count_list:\n",
    "        (w1,w2), i = list_bi\n",
    "        if w1 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "        else:\n",
    "            words_freq_bigram[w1] = i\n",
    "        if w2 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "        else:\n",
    "            words_freq_bigram[w2] = i   \n",
    "    for key,value in words_freq_bigram.items():\n",
    "        bi_count.append([key,value])\n",
    "    return bi_count\n",
    "\n",
    "# some preprocessing steps such as removing stopwords and stemming\n",
    "def arabic_preprocessing (text,markers_flage=1):\n",
    "    markers=[\"في\",\"علي\",\"فوق\",\"تحت\",\"يمين\",\"يسار\",\"و\",\"ثم\",\"او\",\"بل\",\"لكن\",\"حتي\",\"ايضا\",\"قبل\",\"بعد\",\"جيث\",\"ليس\",\"لكن\",\"ولكن\",\"الخ\",\".\",\",\",\"?\",\":\",\";\",\"/\"]\n",
    "    arabic_stop_word=set(stopwords.words('arabic'))\n",
    "    arabic_stop_words=list(arabic_stop_word)\n",
    "    arabic_stop_words.append(\"جدا\")\n",
    "    arabic_stop_words.append(\"الخ\")\n",
    "    arabic_stop_words.append(\"إلخ\")\n",
    "    arabic_stop_words.append(\"ءلخ\")\n",
    "    list_of_words=word_tokenize(text)\n",
    "    list_of_stem=stemming_words(list_of_words)\n",
    "    dict_of_stem={}\n",
    "\n",
    "    for i in range(len(list_of_stem)):\n",
    "        dict_of_stem[list_of_stem[i]]=list_of_words[i]\n",
    "\n",
    "    if markers_flage == 1:\n",
    "        list_of_stem=[x for x in list_of_stem if x not in arabic_stop_words]\n",
    "    stem_text=\"\"\n",
    "    for st in list_of_stem:\n",
    "        stem_text=stem_text+\" \"+st   \n",
    "\n",
    "    return list_of_words,list_of_stem,dict_of_stem\n",
    "# more preprocessing steps \n",
    "def EF_ar(sent_dict):\n",
    "    final_sent = dict()\n",
    "    all_words = []\n",
    "    stopWords = set(stopwords.words('arabic'))\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    \n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        value = value.split(\" \")\n",
    "        value = remove_hyfen(value)\n",
    "        value = remove_punctuation(value)\n",
    "        filtered_sentence = [w for w in value if w not in stopWords] \n",
    "        #replace everything not in the list with space\n",
    "        filtered_sentence = [re.sub(\"'\", '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub('\"', '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(pattern, '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(r'[0-9]+', '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word for word in filtered_sentence if len(word.strip()) >1]\n",
    "        filtered_sentence = [word.strip() for word in filtered_sentence]\n",
    " \n",
    "        final_sent[key] = filtered_sentence\n",
    "        all_words = all_words + filtered_sentence\n",
    "    word_frequency = list(Counter(all_words).items())\n",
    "    return word_frequency, final_sent , all_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_ar(keyvalue):\n",
    "    \n",
    "    sent_dict = replace_acronymss(keyvalue)\n",
    "    tokenize_sent_dict = tokenize_sent(sent_dict)\n",
    "    new_sent_dict = conjunctions_ar(tokenize_sent_dict)\n",
    "    \n",
    "    sent_list =[]\n",
    "    for item in sent_dict.items():\n",
    "        sent_list.append(item)\n",
    "\n",
    "    all_bigrams = []\n",
    "    for key, value in sent_dict.items():\n",
    "        bigrams = ngramise(value)\n",
    "        all_bigrams = all_bigrams+bigrams\n",
    "\n",
    "    bi_count = dict(Counter(all_bigrams))\n",
    "    delete = [key for key,value in bi_count.items() if value < 2]\n",
    "    # delete the key \n",
    "    for key in delete: del bi_count[key]\n",
    "    bi_count_list =[]\n",
    "    for key , value in bi_count.items():\n",
    "        bi_count_list.append([(key),value])\n",
    "\n",
    "    word_freq ,final_sent, all_words = EF_ar(sent_dict)\n",
    "    Sentences_list = []\n",
    "    for item in final_sent.items():\n",
    "        Sentences_list.append(item)     \n",
    "\n",
    "    #word frequencies\n",
    "    #freq of words in title\n",
    "    title =[]\n",
    "    title_freq = Counter(title)\n",
    "\n",
    "    keywords =[]\n",
    "    Keyword_freq = Counter(keywords)\n",
    "\n",
    "    w_freq = Counter(all_words)\n",
    "    final_Word_freq = dict()\n",
    "    for word in all_words:\n",
    "        if word not in title_freq.keys():\n",
    "            title_freq[word] = 0\n",
    "        if word not in Keyword_freq.keys():\n",
    "            Keyword_freq[word] = 0\n",
    "        final_Word_freq[word] = title_freq[word] + Keyword_freq[word] + w_freq[word] \n",
    "\n",
    "    ##final list of sentences after preprocessing\n",
    "    final_input = []\n",
    "    for (index, sentence) in Sentences_list:\n",
    "        final_input.append(\" \".join(sentence))\n",
    "        \n",
    "    keyword_word_freq = []\n",
    "    for word in all_words:\n",
    "        keyword_word_freq.append([word,Keyword_freq[word]])  \n",
    "    title_word_freq = []\n",
    "    for word in all_words:\n",
    "        title_word_freq.append([word, title_freq[word]])\n",
    "\n",
    "    Sentences_list\n",
    "    List_of_sent = dict()\n",
    "    for (key, sent) in Sentences_list:\n",
    "        List_of_sent[key] = \" \".join(sent)\n",
    "\n",
    "    final_Word_freq\n",
    "    finalwordfreq = []\n",
    "    for key , value in final_Word_freq.items():\n",
    "        finalwordfreq.append([key,value])\n",
    "\n",
    "    return (Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "            ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computation of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of nodes and outputs their weights using their frequencies\n",
    "#weights\n",
    "def get_weights(list_of_nodes):\n",
    "    import pandas as ps\n",
    "    import statistics as st\n",
    "    node_weight_dict = dict()\n",
    "    node_list= list_of_nodes\n",
    "    FWL= finalwordfreq\n",
    "    TWL= title_word_freq\n",
    "    KWL= keyword_word_freq\n",
    "    BWL= bi_count \n",
    "    PWL= Pnouns\n",
    "    \n",
    "\n",
    "    F=[]     #freq of words  which appeared in nodes WRT document\n",
    "    T=[]     #freq of words  which appeared in nodes WRT title\n",
    "    K=[]     #freq of words  which appeared in nodes WRT keyword list \n",
    "    B=[]     #freq of words which appeared in nodes WRT Bi_Gram list \n",
    "    P=[]     #list of detection the proper noune \n",
    "    FW=[]    \n",
    "    Xt,Xk,Xbi,Xp,Xb=1,1,1,1,1\n",
    "    weighted_node=[]\n",
    "    \n",
    "    \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "    \n",
    "        for j in FWL:\n",
    "            if(i==j[0]):\n",
    "                f=j[1]\n",
    "                F.append(f)\n",
    "                flag=1\n",
    "                \n",
    "        if(flag!=1): \n",
    "            F.append(0) \n",
    "\n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in TWL:\n",
    "            if(i==j[0]):\n",
    "                t=j[1]\n",
    "                T.append(t)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "             T.append(0)\n",
    "                \n",
    "        \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in KWL:\n",
    "            if(i==j[0]):\n",
    "                k=j[1]\n",
    "                K.append(k)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            K.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in BWL:\n",
    "            if(i==j[0]):\n",
    "                b=j[1]\n",
    "                B.append(b)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            B.append(0)\n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in PWL:\n",
    "            if(i==j):\n",
    "                P.append(1)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            P.append(0)             \n",
    "    \n",
    "    for i in FWL:\n",
    "        fw=i[1]\n",
    "        FW.append(fw)\n",
    "    \n",
    "    if FW == []:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "    elif len(FW) != 0:\n",
    "        avrage_word_frequency=st.mean(FW)\n",
    "        median_word_frequency=st.median(FW) \n",
    "    else:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "        \n",
    "    AW=abs(avrage_word_frequency-median_word_frequency)\n",
    "    \n",
    "    for i in range(len(node_list)):\n",
    "        WN=F[i]+Xt*AW*T[i]+Xk*AW*K[i]+Xb*AW*B[i]+Xp*AW*P[i]\n",
    "        weighted_node.append(WN)\n",
    "        node_weight_dict[node_list[i]] = WN\n",
    "    \n",
    "    return node_weight_dict,weighted_node\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for graph algorithm (algorithm 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge between source and distenation\n",
    "def get_edge(source,distenation):\n",
    "    return edgeget[source,distenation]\n",
    "\n",
    "\n",
    "#distination node list\n",
    "# gets the weights of the distination nodes given a source node , all node weights and source-destination dictionary \n",
    "def getweights_of_dist_nodes(source_destinations,sourcenode,All_nodes_ww):  \n",
    "    x=source_destinations[sourcenode]\n",
    "    distenation_node_list={}\n",
    "    weights=[]\n",
    "    nothing=0\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i])==('E#')) or ( x[i] not in All_nodes_ww):\n",
    "            #nothing is a flag for destination nodes\n",
    "            #if one then there is none\n",
    "            nothing=1\n",
    "        else:\n",
    "            distenation_node_list[x[i]]=All_nodes_ww[x[i]]\n",
    "            weights.append((All_nodes_ww[x[i]]))\n",
    "        if len(weights) != 0:\n",
    "            nothing=0\n",
    "    return distenation_node_list,weights,nothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2 get candidate edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check for candidate edges between a source and a destination given a criteria \n",
    "def get_candidate_edges(source_destinations,node_listx,wxx,out_carteria,All_weight,input_nodes,All_SD):\n",
    "    c=[]\n",
    "    xxx = []\n",
    "    no_list,totalweights=node_listx,All_weight\n",
    "\n",
    "    source_node_weight=0\n",
    "    distination_node_weight=0\n",
    "    average_of_the_node_weight=np.average(totalweights)#(np.sum(node_list)/number_of_nodes)\n",
    "    median_of_the_node_list_weight=np.median(totalweights)\n",
    "  \n",
    "    if average_of_the_node_weight>median_of_the_node_list_weight:#can be replaced with source=max(avg,median)\n",
    "        source_node_weight=average_of_the_node_weight\n",
    "    else:\n",
    "        source_node_weight=median_of_the_node_list_weight\n",
    "\n",
    "    for node,weight in no_list.items():\n",
    "        \n",
    "        if weight>=source_node_weight:#بيعرف اذا كانت بدايه الجمله ولا لا \n",
    "            outnodes1,outnodesweights,flagnothing=getweights_of_dist_nodes(source_destinations,node,no_list)#فنكشن جوا الجي بتجبلي كل الديستناشن بتاعت النودواختار بينهم هسقط مين\n",
    "            outnodes2,nodes2weights,flag2nothing=getweights_of_dist_nodes(All_SD,node,input_nodes)\n",
    "            outnodes1=[(i,j) for (i,j) in outnodes1.items() if j!=0 ] \n",
    "            outnodes2 =[(i,j) for (i,j) in outnodes2.items() if j!=0 ] \n",
    "        \n",
    "            if flagnothing==0:\n",
    "                #destination nodes exsist\n",
    "                #get their average and max weights for avg and max criterias\n",
    "                average_weight=np.average(nodes2weights)\n",
    "                max_weight=max(nodes2weights)\n",
    "            else:\n",
    "                #no destination nodes \n",
    "                average_weight=0\n",
    "                max_weight=0\n",
    "            # get threshold of destination weights according to criteria \n",
    "            if (out_carteria ==\"avg\"):\n",
    "                distination_node_weight=average_weight\n",
    "            elif (out_carteria ==\"node_avg\"):\n",
    "                distination_node_weight=source_node_weight\n",
    "            elif (out_carteria==\"max\"):\n",
    "                distination_node_weight=max_weight\n",
    "            else:\n",
    "                distination_node_weight=average_weight\n",
    "\n",
    "\n",
    "            if (bool(outnodes1)==False):\n",
    "                # no destination nodes \n",
    "                nothing=1  \n",
    "            else: \n",
    "                # check for the candidate edges\n",
    "                for out,weight_of_out_node in outnodes1:\n",
    "                    if weight_of_out_node>=distination_node_weight and flagnothing==0:\n",
    "                        c.append(get_edge(node,out))# append the edge\n",
    "                        xxx.append([node,out])\n",
    "\n",
    "    cx=[]\n",
    "    for i in range(0,len(c)):\n",
    "        cx.append(c[i][0])\n",
    "    return cx\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for building the Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a vertex to the dictionary\n",
    "def add_vertex(v):\n",
    "    global graph\n",
    "    global vertices_no\n",
    "    if v not in graph:\n",
    "        vertices_no = vertices_no + 1\n",
    "        graph[v] = []\n",
    "\n",
    "# Add an edge between vertex v1 and v2 with edge weight e\n",
    "#v1 is the source node and v2 is the distenation node\n",
    "edgeget={}\n",
    "def add_edge(v1, v2, e_order, e_value):\n",
    "    global graph\n",
    "    # Check if vertex v1 is a valid vertex\n",
    "    if ((v1 in graph) and (v2 in graph)):\n",
    "        temp = [v2, e_order, e_value]\n",
    "        edgeget[v1,v2]=[e_value]\n",
    "        graph[v1].append(temp)\n",
    "\n",
    "# Print the graph\n",
    "def print_graph():\n",
    "    global graph\n",
    "    for vertex in graph:\n",
    "        for edges in graph[vertex]:\n",
    "            print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "# get the destinations\n",
    "def get_destinations(source):\n",
    "    global graph\n",
    "    distinations=[]\n",
    "    if(source in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0] not in distinations):\n",
    "                distinations.append(elements[0])\n",
    "    return distinations\n",
    "\n",
    "# get the edges\n",
    "def get_edges(source,dist):\n",
    "    global graph\n",
    "    Edge=\"\"\n",
    "    if(source in graph and dist in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0]==dist):\n",
    "                Edge= Edge + elements[2]\n",
    "    return Edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weight of the sentence order in the input document\n",
    "def sentence_order(count_summ, index):\n",
    "    return(count_summ - index)/count_summ\n",
    "\n",
    "#given a list of candidate sentences rank them according to ranking criteria\n",
    "def compute_rank(candidate_summary):\n",
    "    ranks = {}\n",
    "    candidate_summary = reorder(candidate_summary)\n",
    "    \n",
    "    for i, sent in enumerate(candidate_summary):\n",
    "        rank = sentence_order(len(candidate_summary), i)\n",
    "        ranks[sent] = rank\n",
    "        \n",
    "    sort_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)    \n",
    "    return ranks   \n",
    "\n",
    "#vectorize sentences for clustering\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "#numner of words in a sentence\n",
    "def get_words_count(sent):\n",
    "    len_sent = 0\n",
    "    for i in sent.split(\" \"):\n",
    "        len_sent = len_sent +1\n",
    "    return len_sent\n",
    "\n",
    "# reorder the summary based on the order from the input text\n",
    "def reorder(summary):\n",
    "    index_summ = []\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    order = dict()\n",
    "    final_order = []\n",
    "    for sent in summary:\n",
    "        if sent in value_sent:\n",
    "            index_sent = value_sent.index(sent)\n",
    "            index_summ.append(index_sent)\n",
    "            order[index_sent] = sent\n",
    "\n",
    "    for i in sorted(order):\n",
    "        final_order.append(order[i])\n",
    "\n",
    "    return final_order\n",
    "\n",
    "#concatenate the sentences in the candidate summary\n",
    "def concat(final_order):\n",
    "    return(\" \".join(final_order))\n",
    " \n",
    "# get the index of a sentence from the input text\n",
    "def get_index(sent):\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    index_sent = value_sent.index(sent)\n",
    "    return index_sent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing and algorithm four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_ar(list_of_candidates_summeries):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    # rank sentneces\n",
    "    sentences_ranks = compute_rank(candidate_summary)\n",
    "    # order them \n",
    "    sentences_ordered =  [key for key,value  in sentences_ranks.items()]\n",
    "    summary_length =0\n",
    "    visited_sentences = []\n",
    "    \n",
    "    sentences = sentences_ordered\n",
    "    \n",
    "    arabic_stop_word=set(stopwords.words('arabic'))\n",
    "    arabic_stop_words=list(arabic_stop_word)\n",
    "    arabic_stop_words.append(\"جدا\")\n",
    "    arabic_stop_words.append(\"الخ\")\n",
    "    arabic_stop_words.append(\"إلخ\")\n",
    "    arabic_stop_words.append(\"ءلخ\")\n",
    "    \n",
    "    tfidfvect = TfidfVectorizer(stop_words=arabic_stop_words)\n",
    "    X = tfidfvect.fit_transform(sentences)\n",
    "\n",
    "    if len(sentences) <1: #clusters can't be more than number of words\n",
    "        n_clusters = len(sentences)\n",
    "    else:\n",
    "        n_clusters = 1\n",
    "    clf = KMeans(n_clusters = n_clusters,init = 'k-means++', max_iter = 1000)\n",
    "    labels = clf.fit_predict(X)\n",
    "    clustered_sentences =[[] for i in range(5)]\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['data_index'] = sentences\n",
    "    cluster_map['cluster'] = clf.labels_\n",
    "    #print(cluster_map)\n",
    "    return n_clusters,cluster_map\n",
    "\n",
    "# clusting algorithm if clusters = 1 then this is just picking top sentneces\n",
    "def algorithm_4_ar(summary_length,threshold,max_words_limit,list_of_candidates_summeries,n_clusters,cluster_map):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    summary_length = 0\n",
    "    while (len(visited_sentences) < len(candidate_summary)) and ((summary_length)< max_words_limit) :\n",
    "        for i in range(n_clusters):\n",
    "            clustered_sentences = cluster_map[cluster_map.cluster == i]['data_index'].tolist()\n",
    "            if summary_length >= (max_words_limit+threshold):\n",
    "                break\n",
    "            for sent in clustered_sentences:\n",
    "                if sent not in visited_sentences :\n",
    "                    temp_length = summary_length + 1\n",
    "                    if temp_length < max_words_limit :\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        print(\"ADD Sentence of index \", get_index(sent))\n",
    "                        break\n",
    "                    elif(temp_length >= max_words_limit) and (temp_length <= (max_words_limit + threshold)):\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        print(\"ADD Sentence of index \", get_index(sent))\n",
    "                        break\n",
    "                    elif temp_length > (max_words_limit + threshold):\n",
    "                        visited_sentences.append(sent)\n",
    "                        print(\"ADD Sentence of index \", get_index(sent))\n",
    "        \n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph_ara(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria):\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    input_text = candidate_summ\n",
    "    last_summ_len = candidate_summ_len\n",
    "    Last_summ = candidate_summ\n",
    "    sent_idx = can_edges\n",
    "    #print(\"sent_idx   \", sent_idx)\n",
    "    can_edges=[]\n",
    "    List_of_sent = list_pre_cansumm\n",
    "    list_pre_cansumm = []\n",
    "    list_of_candidates_summeries = []\n",
    "    All_nodes=[]\n",
    "    nodes_list = []\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    #print(S.values())\n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        sentence=arabic_normalization(sentence)\n",
    "        \n",
    "        Words,list_of_stem,dict_of_stem=arabic_preprocessing(sentence)\n",
    "\n",
    "        words=stemming_words(Words)\n",
    "\n",
    "        Types, nodes_list , edges_list = AR_POS(Words)\n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "                \n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[index] = (source_destinations,list_of_nodes)\n",
    "        \n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        index = index + 1\n",
    "\n",
    "        \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        #list_of_candidates_summeries = []\n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[index] and (List_of_sent[index].strip()) ):\n",
    "                #print(\"PICKED  \", index, \"   \", sent_index)\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "        index = index + 1\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    candidate_summ_len = 0\n",
    "    for index ,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + 1\n",
    "\n",
    "    return Cand_edge,list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function\n",
    "## Takes in an input text and outputs a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_ara(Text):\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    global vertices_no\n",
    "    vertices_no =0\n",
    "    \n",
    "    can_edges = []\n",
    "    global keyvalue\n",
    "    global key_value_list\n",
    "    global lang_Dict\n",
    "\n",
    "    title,keywords,keyvalue, key_value_list, lang_Dict =  read_input_ar(Text) \n",
    "    out_carteria=\"avg\"\n",
    "    print(\"Print key value sentence\")\n",
    "    print(keyvalue)\n",
    "    global Sentences_list\n",
    "    global final_input\n",
    "    global all_bigrams\n",
    "    global bi_count_list\n",
    "    global word_freq\n",
    "    global all_words\n",
    "    global finalwordfreq\n",
    "    #global List_of_sent\n",
    "    global title_word_freq\n",
    "    global keyword_word_freq\n",
    "    global All_nodes\n",
    "    global list_of_candidates_summeries\n",
    "    \n",
    "    global list_pre_cansumm\n",
    "    \n",
    "    (Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "     ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess_ar(keyvalue)\n",
    "    print(\"INPUT after Pre-Processing\")\n",
    "    print(final_input)\n",
    "    global bi_count\n",
    "    global Cand_edge\n",
    "    Cand_edge =[]\n",
    "    Word_Freq = {}\n",
    "    for key, value in finalwordfreq:\n",
    "        Word_Freq[key] = value\n",
    "    \n",
    "\n",
    "    bi_count = bigrams_freq(bi_count_list) \n",
    "    Last_summ = keyvalue.values()\n",
    "    last_summ_len = 0\n",
    "    for value in keyvalue.values():\n",
    "        last_summ_len = last_summ_len + 1\n",
    "\n",
    "\n",
    "    All_nodes=[]\n",
    "    list_of_candidates_summeries=[]\n",
    "    list_pre_cansumm = []\n",
    "\n",
    "    ##tokanization  \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    ##lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    # Add an edge between vertex v1 and v2 with edge weight e\n",
    "    #v1 is the source node and v2 is the distenation node\n",
    "    edgeget={}\n",
    "\n",
    "    \n",
    "    # driver code\n",
    "    global graph \n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    \n",
    "    input_nodes = []\n",
    "    sent_nodes = {}\n",
    "    global Pnouns\n",
    "    Pnouns = []\n",
    "    \n",
    "    for sent_index, sentence in enumerate(List_of_sent.values()):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        sentence=arabic_normalization(sentence)\n",
    "        Words,list_of_stem,dict_of_stem=arabic_preprocessing(sentence)\n",
    "        words=stemming_words(Words)\n",
    "        Types, nodes_list , edges_list = AR_POS(Words)\n",
    "        for node in nodes_list:\n",
    "            if node != \"\":\n",
    "                Cand_edge.append(node)\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "        \n",
    "        counter =0\n",
    "        order =0\n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "                \n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[sent_index] = (source_destinations,list_of_nodes)\n",
    "        \n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    for sent_index, sentence in enumerate(S.values()): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[sent_index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        #list_of_candidates_summeries = []\n",
    "        if len(list_of_can_edges)>= 1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[sent_index] and (List_of_sent[sent_index].strip()) ):\n",
    "                #print(\"picked \", sent_index)\n",
    "                #print(keyvalue[sent_index])\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[sent_index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "    global final_summary\n",
    "    global visited_sentences\n",
    "    #max_words_limit= int(len(keyvalue)*0.5)\n",
    "    max_words_limit= 5\n",
    "    threshold = 1\n",
    "    final_summary = []\n",
    "    visited_sentences = []\n",
    "    summary_length = 0\n",
    "    #print(\"1\")\n",
    "    #print(list_of_candidates_summeries)   \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    #print(\"2\")\n",
    "    #print(candidate_summ)\n",
    "    \n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + 1\n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "\n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "\n",
    "    while(True):\n",
    "        if(candidate_summ_len > (max_words_limit + threshold)):\n",
    "            if(candidate_summ_len != last_summ_len):\n",
    "                Cand_edge,list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph_ara(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria) \n",
    "\n",
    "            elif(out_carteria != \"max\"):\n",
    "                out_carteria=\"max\"\n",
    "                Cand_edge,list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph_ara(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria)\n",
    "            else:\n",
    "                list_of_candidates_summeries = candidate_summ\n",
    "                break\n",
    "        elif(candidate_summ_len < (max_words_limit)):\n",
    "            candidate_summ = Last_summ\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "        else:\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "    \n",
    "    n_clusters,cluster_map = post_processing_ar(list_of_candidates_summeries)\n",
    "    final_summary = algorithm_4_ar(summary_length,threshold,max_words_limit,list_of_candidates_summeries,n_clusters,cluster_map)\n",
    "    final = reorder(final_summary)\n",
    "    outsumm = concat(final)\n",
    "    \n",
    "    return outsumm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to run the code \n",
    "## summary = summarization_ara(your input text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يحتاج الفرد بين فترة وأخرى إلى كسر الروتين والعادة اليومية لتجديد نشاطه، وإن أكثر ما يوقف الإنسان هي حالة الرتابة والثبات على عادة واحدة فقط.\n",
    "كل شيء في الحياة يحتاج إلى التجديد والتغيير حتى نستشعر روح المغامرة وروح الابتكار، لذا بعد كل إجازة ينتظرنا شيء مملوء عزيمة، تتجدد معه النوايا، وتتجدد معه النظرة المتفائلة، ماذا سنقدم من جديد؟ وكأنها بداية صفحة جديدة، نخطط ونبتكر طريقة معينة للعطاء في العمل وتجديد النشاط وتنويع الأساليب التي تطور العمل.\n",
    "الإجازة جميلة جداً، نغير فيها نمطية حياتنا ونرتاح، ونستمتع بالأشياء مغايرة من سفر وزيارات واسترخاء، وكل منا له طريقته الخاصة في استغلال إجازته، وهي في الحقيقة تكسر بين عادة وأخرى، وبين نمط وآخر، وبين الركود والنشاط، لذا تجدد الإجازة كل شيء فينا.\n",
    "دائماً نسأل أنفسنا ماذا نقدم بعد الإجازة؟ وكيف نستقبل العمل ونحن في نشاط وحب؟ هذه الأسئلة إثراء لنا، لكن للأسف نلاحظ أن هناك من يشوه صورة العمل وكأن الحياة من المفترض بها أن تكون كلها إجازة، وهنا لا بد أن نتعمق في مفهوم العمل والإجازة، هناك من ينظر للعمل كواجب أو من أجل الراتب فقط، وهناك من يعيش حالة من الوعي العميقة في نظرته للحياة، وأن العمل جزء جميل من حياة الإنسان، نحن في العمل نفكر ونخطط ونجتهد ونبذل الطاقة والجهد في تقديم أحسن ما لدينا، وهذا مرتبط بالوعي لأهمية رسالتنا في الحياة، وأن كلاً منا له رسالة مهنية واجتماعية وعائلية، فمن يعي ذلك سيعيش مع رسالته في تناغم جميل وعجيب، لذا سيعرف ويدرك كيف يستفيد من أوقات إجازته في التجديد والراحة، ويدرك أيضاً أهمية العمل بالنسبة له، وأنه ليس واجباً فقط بل حياته، كيف يقضيها؟ وكيف يوازن بين ما يحبه ودوره ورسالته والقدرة على الاستمتاع في كل منهما والتي نطلق على القدرة كلمة التناغم؟.\n",
    "التناغم يجعلنا ننتقل من حال لآخر، ومن مرحلة لأخرى، متقبلين لكل تغيير مؤقت أو دائم، هذا التغيير هو من يقضي على الرتابة والروتين، ويجعلنا في حالة دائمة مستغرقة في جمال الحياة، الشيء الوحيد الذي يقتل الإنسان هو حالة الجمود والثبات.\n",
    "لذا كيف ستكون العودة إلى العمل؟ وبأي روح نستقبله؟ وبأي عزيمة وإصرار نتقبل يوماً جديداً ونحن أكثر بهجة وقوةً؟\n",
    "من المهم ترك الصورة التقليدية للعمل والصورة المشوهة له، ونستبدلها بالصورة الجميلة، وأنها حياة ممتلئة بالنشاط والحركة والإنتاج.\n",
    "العمل يحرك الفكر ويجدده، والعمل يحرك الجسد وينشطه، والعمل يجعلنا جميعاً نستشعر قيمة الأشياء التي نقوم بها، وأن لنا دوراً كبيراً في نهضة بلدنا ومجتمعنا والأكيد لأنفسنا أيضاً، كل ذلك يتطلب نفض تراب الكسل، وإدراك أهمية العمل بنظرة جديدة ومفهوم جديد.\n",
    "\n",
    "نقلا عن الرياض"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"يحتاج الفرد بين فترة وأخرى إلى كسر الروتين والعادة اليومية لتجديد نشاطه، وإن أكثر ما يوقف الإنسان هي حالة الرتابة والثبات على عادة واحدة فقط.\n",
    "كل شيء في الحياة يحتاج إلى التجديد والتغيير حتى نستشعر روح المغامرة وروح الابتكار، لذا بعد كل إجازة ينتظرنا شيء مملوء عزيمة، تتجدد معه النوايا، وتتجدد معه النظرة المتفائلة، ماذا سنقدم من جديد؟ وكأنها بداية صفحة جديدة، نخطط ونبتكر طريقة معينة للعطاء في العمل وتجديد النشاط وتنويع الأساليب التي تطور العمل.\n",
    "الإجازة جميلة جداً، نغير فيها نمطية حياتنا ونرتاح، ونستمتع بالأشياء مغايرة من سفر وزيارات واسترخاء، وكل منا له طريقته الخاصة في استغلال إجازته، وهي في الحقيقة تكسر بين عادة وأخرى، وبين نمط وآخر، وبين الركود والنشاط، لذا تجدد الإجازة كل شيء فينا.\n",
    "دائماً نسأل أنفسنا ماذا نقدم بعد الإجازة؟ وكيف نستقبل العمل ونحن في نشاط وحب؟ هذه الأسئلة إثراء لنا، لكن للأسف نلاحظ أن هناك من يشوه صورة العمل وكأن الحياة من المفترض بها أن تكون كلها إجازة، وهنا لا بد أن نتعمق في مفهوم العمل والإجازة، هناك من ينظر للعمل كواجب أو من أجل الراتب فقط، وهناك من يعيش حالة من الوعي العميقة في نظرته للحياة، وأن العمل جزء جميل من حياة الإنسان، نحن في العمل نفكر ونخطط ونجتهد ونبذل الطاقة والجهد في تقديم أحسن ما لدينا، وهذا مرتبط بالوعي لأهمية رسالتنا في الحياة، وأن كلاً منا له رسالة مهنية واجتماعية وعائلية، فمن يعي ذلك سيعيش مع رسالته في تناغم جميل وعجيب، لذا سيعرف ويدرك كيف يستفيد من أوقات إجازته في التجديد والراحة، ويدرك أيضاً أهمية العمل بالنسبة له، وأنه ليس واجباً فقط بل حياته، كيف يقضيها؟ وكيف يوازن بين ما يحبه ودوره ورسالته والقدرة على الاستمتاع في كل منهما والتي نطلق على القدرة كلمة التناغم؟.\n",
    "التناغم يجعلنا ننتقل من حال لآخر، ومن مرحلة لأخرى، متقبلين لكل تغيير مؤقت أو دائم، هذا التغيير هو من يقضي على الرتابة والروتين، ويجعلنا في حالة دائمة مستغرقة في جمال الحياة، الشيء الوحيد الذي يقتل الإنسان هو حالة الجمود والثبات.\n",
    "لذا كيف ستكون العودة إلى العمل؟ وبأي روح نستقبله؟ وبأي عزيمة وإصرار نتقبل يوماً جديداً ونحن أكثر بهجة وقوةً؟\n",
    "من المهم ترك الصورة التقليدية للعمل والصورة المشوهة له، ونستبدلها بالصورة الجميلة، وأنها حياة ممتلئة بالنشاط والحركة والإنتاج.\n",
    "العمل يحرك الفكر ويجدده، والعمل يحرك الجسد وينشطه، والعمل يجعلنا جميعاً نستشعر قيمة الأشياء التي نقوم بها، وأن لنا دوراً كبيراً في نهضة بلدنا ومجتمعنا والأكيد لأنفسنا أيضاً، كل ذلك يتطلب نفض تراب الكسل، وإدراك أهمية العمل بنظرة جديدة ومفهوم جديد.\n",
    "\n",
    "نقلا عن الرياض\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print key value sentence\n",
      "{0: 'يحتاج الفرد بين فترة وأخرى إلى كسر الروتين والعادة اليومية لتجديد نشاطه،', 1: 'وإن أكثر ما يوقف الإنسان هي حالة الرتابة والثبات على عادة واحدة فقط', 2: 'كل شيء في الحياة يحتاج إلى التجديد والتغيير حتى نستشعر روح المغامرة وروح الابتكار،', 3: 'لذا بعد كل إجازة ينتظرنا شيء مملوء عزيمة،', 4: 'تتجدد معه النوايا،', 5: 'وتتجدد معه النظرة المتفائلة،', 6: 'ماذا سنقدم من جديد؟', 7: 'وكأنها بداية صفحة جديدة،', 8: 'نخطط ونبتكر طريقة معينة للعطاء في العمل وتجديد النشاط وتنويع الأساليب التي تطور العمل', 9: 'الإجازة جميلة جداً،', 10: 'نغير فيها نمطية حياتنا ونرتاح،', 11: 'ونستمتع بالأشياء مغايرة من سفر وزيارات واسترخاء،', 12: 'وكل منا له طريقته الخاصة في استغلال إجازته،', 13: 'وهي في الحقيقة تكسر بين عادة وأخرى،', 14: 'وبين نمط وآخر،', 15: 'وبين الركود والنشاط،', 16: 'لذا تجدد الإجازة كل شيء فينا', 17: 'دائماً نسأل أنفسنا ماذا نقدم بعد الإجازة؟', 18: 'وكيف نستقبل العمل ونحن في نشاط وحب؟', 19: 'هذه الأسئلة إثراء لنا،', 20: 'لكن للأسف نلاحظ أن هناك من يشوه صورة العمل وكأن الحياة من المفترض بها أن تكون كلها إجازة،', 21: 'وهنا لا بد أن نتعمق في مفهوم العمل والإجازة،', 22: 'هناك من ينظر للعمل كواجب أو من أجل الراتب فقط،', 23: 'وهناك من يعيش حالة من الوعي العميقة في نظرته للحياة،', 24: 'وأن العمل جزء جميل من حياة الإنسان،', 25: 'نحن في العمل نفكر ونخطط ونجتهد ونبذل الطاقة والجهد في تقديم أحسن ما لدينا،', 26: 'وهذا مرتبط بالوعي لأهمية رسالتنا في الحياة،', 27: 'وأن كلاً منا له رسالة مهنية واجتماعية وعائلية،', 28: 'فمن يعي ذلك سيعيش مع رسالته في تناغم جميل وعجيب،', 29: 'لذا سيعرف ويدرك كيف يستفيد من أوقات إجازته في التجديد والراحة،', 30: 'ويدرك أيضاً أهمية العمل بالنسبة له،', 31: 'وأنه ليس واجباً فقط بل حياته،', 32: 'كيف يقضيها؟ وكيف يوازن بين ما يحبه ودوره ورسالته والقدرة على الاستمتاع في كل منهما والتي نطلق على القدرة كلمة التناغم؟', 33: 'التناغم يجعلنا ننتقل من حال لآخر، ومن مرحلة لأخرى، متقبلين لكل تغيير مؤقت أو دائم، هذا التغيير هو من يقضي على الرتابة والروتين، ويجعلنا في حالة دائمة مستغرقة في جمال الحياة، الشيء الوحيد الذي يقتل الإنسان هو حالة الجمود والثبات', 34: 'لذا كيف ستكون العودة إلى العمل؟ وبأي روح نستقبله؟ وبأي عزيمة وإصرار نتقبل يوماً جديداً ونحن أكثر بهجة وقوةً؟', 35: 'من المهم ترك الصورة التقليدية للعمل والصورة المشوهة له، ونستبدلها بالصورة الجميلة، وأنها حياة ممتلئة بالنشاط والحركة والإنتاج', 36: 'العمل يحرك الفكر ويجدده، والعمل يحرك الجسد وينشطه، والعمل يجعلنا جميعاً نستشعر قيمة الأشياء التي نقوم بها، وأن لنا دوراً كبيراً في نهضة بلدنا ومجتمعنا والأكيد لأنفسنا أيضاً، كل ذلك يتطلب نفض تراب الكسل، وإدراك أهمية العمل بنظرة جديدة ومفهوم جديد', 37: 'نقلا عن الرياض'}\n",
      "INPUT after Pre-Processing\n",
      "['يحتاج الفرد فترة وأخرى كسر الروتين والعادة اليومية لتجديد نشاطه،', 'يوقف الإنسان حالة الرتابة والثبات عادة واحدة فقط', 'شيء الحياة يحتاج التجديد والتغيير نستشعر روح المغامرة وروح الابتكار،', 'لذا إجازة ينتظرنا شيء مملوء عزيمة،', 'تتجدد معه النوايا،', 'وتتجدد معه النظرة المتفائلة،', 'سنقدم جديد؟', 'وكأنها بداية صفحة جديدة،', 'نخطط ونبتكر طريقة معينة للعطاء العمل وتجديد النشاط وتنويع الأساليب تطور العمل', 'الإجازة جميلة جداً،', 'نغير نمطية حياتنا ونرتاح،', 'ونستمتع بالأشياء مغايرة سفر وزيارات واسترخاء،', 'وكل منا طريقته الخاصة استغلال إجازته،', 'وهي الحقيقة تكسر عادة وأخرى،', 'وبين نمط وآخر،', 'وبين الركود والنشاط،', 'لذا تجدد الإجازة شيء فينا', 'دائماً نسأل أنفسنا نقدم الإجازة؟', 'وكيف نستقبل العمل ونحن نشاط وحب؟', 'الأسئلة إثراء لنا،', 'للأسف نلاحظ يشوه صورة العمل وكأن الحياة المفترض تكون كلها إجازة،', 'وهنا بد نتعمق مفهوم العمل والإجازة،', 'ينظر للعمل كواجب أجل الراتب فقط،', 'وهناك يعيش حالة الوعي العميقة نظرته للحياة،', 'وأن العمل جزء جميل حياة الإنسان،', 'العمل نفكر ونخطط ونجتهد ونبذل الطاقة والجهد تقديم أحسن لدينا،', 'وهذا مرتبط بالوعي لأهمية رسالتنا الحياة،', 'وأن كلاً منا رسالة مهنية واجتماعية وعائلية،', 'يعي سيعيش رسالته تناغم جميل وعجيب،', 'لذا سيعرف ويدرك يستفيد أوقات إجازته التجديد والراحة،', 'ويدرك أيضاً أهمية العمل بالنسبة له،', 'وأنه واجباً فقط حياته،', 'يقضيها؟ وكيف يوازن يحبه ودوره ورسالته والقدرة الاستمتاع منهما والتي نطلق القدرة كلمة التناغم؟', 'التناغم يجعلنا ننتقل حال لآخر، مرحلة لأخرى، متقبلين لكل تغيير مؤقت دائم، التغيير يقضي الرتابة والروتين، ويجعلنا حالة دائمة مستغرقة جمال الحياة، الشيء الوحيد يقتل الإنسان حالة الجمود والثبات', 'لذا ستكون العودة العمل؟ وبأي روح نستقبله؟ وبأي عزيمة وإصرار نتقبل يوماً جديداً ونحن بهجة وقوةً؟', 'المهم ترك الصورة التقليدية للعمل والصورة المشوهة له، ونستبدلها بالصورة الجميلة، وأنها حياة ممتلئة بالنشاط والحركة والإنتاج', 'العمل يحرك الفكر ويجدده، والعمل يحرك الجسد وينشطه، والعمل يجعلنا جميعاً نستشعر قيمة الأشياء نقوم بها، وأن دوراً كبيراً نهضة بلدنا ومجتمعنا والأكيد لأنفسنا أيضاً، يتطلب نفض تراب الكسل، وإدراك أهمية العمل بنظرة جديدة ومفهوم جديد', 'نقلا الرياض']\n",
      "ADD Sentence of index  1\n",
      "ADD Sentence of index  2\n",
      "ADD Sentence of index  3\n",
      "ADD Sentence of index  12\n",
      "ADD Sentence of index  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'وإن أكثر ما يوقف الإنسان هي حالة الرتابة والثبات على عادة واحدة فقط كل شيء في الحياة يحتاج إلى التجديد والتغيير حتى نستشعر روح المغامرة وروح الابتكار، لذا بعد كل إجازة ينتظرنا شيء مملوء عزيمة، وكل منا له طريقته الخاصة في استغلال إجازته، لكن للأسف نلاحظ أن هناك من يشوه صورة العمل وكأن الحياة من المفترض بها أن تكون كلها إجازة،'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_ara(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
