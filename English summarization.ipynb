{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#importing libraries needed for the model\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#import qalsadi.analex as qa\n",
    "#import pyarabic.araby as araby\n",
    "#import naftawayh.wordtag\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import os\n",
    "#import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, lemmatize_word, expand_contraction\n",
    "from text_preprocessing import remove_punctuation as punct\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.language import Language\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "def create_lang_detector(nlp, name):\n",
    "    return abbreviation_pipe\n",
    "Language.factory(\"abbreviation_pipe\", func=create_lang_detector)\n",
    "nlp.add_pipe(\"abbreviation_pipe\")\n",
    "\n",
    "\n",
    "from abbreviations import schwartz_hearst\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "import statistics as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in an input\n",
    "## input is format of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a text extract key-value dictionary with sentenece as values and keys as their index in the list\n",
    "# extract all words in the text into a list \n",
    "def read_input(text):\n",
    "    \n",
    "    lang_Dict = []\n",
    "    for line in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(line):\n",
    "            if word not in lang_Dict:\n",
    "                lang_Dict.append(word)\n",
    "    iter_dict = lang_Dict\n",
    "    lang_Dict = []\n",
    "    for i in iter_dict:\n",
    "        if '\\n' in i:\n",
    "            lang_Dict = lang_Dict + i.split(\"\\n\")\n",
    "        else:\n",
    "            lang_Dict.append(i)\n",
    "     \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    \n",
    "    lang_dict = []\n",
    "    for word in lang_Dict:\n",
    "        if word.strip():\n",
    "            word = word.lower()\n",
    "            #word = expand_contraction(word)\n",
    "            word = re.sub(pattern, '', word)\n",
    "            lang_dict.append(word)\n",
    "            \n",
    "    lang_Dict = []\n",
    "    for word in lang_dict:\n",
    "        if \" \" in word:\n",
    "            for w in word.split(\" \"):\n",
    "                if w.strip():\n",
    "                    lang_Dict.append(w)\n",
    "        elif word and word.strip():\n",
    "            lang_Dict.append(word)\n",
    "    keyvalue = dict() \n",
    "\n",
    "    index= 0\n",
    "    for line in nltk.sent_tokenize(text):\n",
    "        if(line and (line.strip()) ):\n",
    "            i = index\n",
    "            keyvalue[i] = line.strip()\n",
    "            index = index + 1\n",
    "            \n",
    "    key_value_list =[]\n",
    "    for item in keyvalue.items():\n",
    "        key_value_list.append(item)\n",
    "    \n",
    "    return keyvalue, key_value_list, lang_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## necessary functions for preprocessing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for the pre-processing phase\n",
    "\n",
    "#this function rplaces the appreviation in the text \n",
    "#for example  Natural language processing --> NLP\n",
    "def replace_acronymss(keyvalue):\n",
    "        sent_dict = dict() \n",
    "        whole_text = \"\"\n",
    "        for key, text in keyvalue.items():\n",
    "            whole_text = whole_text + \"#*# \" + text\n",
    "        whole_text = whole_text.lower()\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=whole_text, most_common_definition=True)\n",
    "        for key,value in pairs.items():\n",
    "            whole_text = whole_text.replace(value, key)\n",
    "        output_text = whole_text.split(\"#*# \")\n",
    "        i = 0;\n",
    "        for l in output_text:\n",
    "            if l != \"\":\n",
    "                sentence = Brackets(l)\n",
    "                sent_dict[i] = sentence\n",
    "                i = i+1\n",
    "        return sent_dict\n",
    "    \n",
    "#takes in words and expands contractions  \n",
    "#for example expanding contractions won't --> will not \n",
    "#remove hyphens and tokenizing sentence\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = expand_contraction(word)\n",
    "        if word.lower() == \"n't\":\n",
    "            word = \"not\"\n",
    "        if word != '':\n",
    "            word = word.lower()\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "#this function takes in a dictionary of values as sentence and the keys are their index in a sentnece \n",
    "#and splits the sentence into the list of words in it\n",
    "#words that contain '\\n-' in them are re-combined \n",
    "def tokenize_sent(sent_dict):\n",
    "    keyvalueToken = dict() \n",
    "    TorF = True\n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        words = nltk.word_tokenize(value)\n",
    "        words = [re.sub(r'(\\n-)','',word) for word in words]\n",
    "        words = [re.sub(r'(\\n)',' ',word) for word in words]\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                 new_words = new_words + word.split(\" \")\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        \n",
    "        keyvalueToken[key] = new_words\n",
    "        \n",
    "    return keyvalueToken\n",
    "\n",
    "# this is a list of conjunctions to be removed from the tokenized sentence\n",
    "# takes in a dictionary of values as tokenized sentences and keys as their index in the input document\n",
    "# outputs a dictionary with same keys but values are tokenized sentences with conjunctions removed from them\n",
    "def conjunctions(tokenize_sent_dict):\n",
    "    new_sent_dict = dict()\n",
    "    conjunctive_adverbs= [\"accordingly\",\"comparatively\",\"contrarily\", \n",
    "                      \"also\",\" further\", \"nevertheless\", \"indeed\",\" otherwise\",\n",
    "                      \"result\",\"as\",\"equally\" ,\"conversely\" ,\"besides\",\n",
    "                      \"furthermore\" ,\"nonetheless\" ,\"fact\" ,\"consequently\",\n",
    "                     \" likewise\" ,\"however\" ,\"addition\" \",moreover\" ,\"surprisingly\",\n",
    "                     \" hence\" ,\"similarly \",\"comparison\" ,\"still \",\"therefore \",\n",
    "                      \"contrast\" ,\"thus\" ,\"instead\"  ,\"rather\"\n",
    "                      , \"other\", \"additionally\", \"addition\",'although','already',\"including\",'according',\n",
    "                    'totally','likely','unlikely','perhaps',\"s\",'naturally',\n",
    "                      \"finally\", \"based\",\"on\",\"that\" , \"besides\",\"namely\",\"anyway\",\n",
    "                     \"then\",\"next\",\"thereafter\",\"certainly\",\"now\",\"finally\",\"meanwhile\",\n",
    "                     \"subsequently\",\"yet\",\"elsewhere\",\"thereafter\",\"undoubtedly\",\n",
    "                     \"incidentally\",\"otherwise\",\"regardless\",\"begin\",\"in\"]\n",
    "    \n",
    "    for key, text in tokenize_sent_dict.items():\n",
    "        new_sent = [word for word in text if word.lower() not in conjunctive_adverbs]\n",
    "        new_sent_dict[key] = new_sent\n",
    "\n",
    "    return new_sent_dict\n",
    "\n",
    "\n",
    "# to remove brackets and everything in between\n",
    "def Brackets(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# given a sentence and a dictionary \n",
    "#check if the sentence is in this dictionary\n",
    "#if it does return the key if not return \"key doesn't exist\"\n",
    "def get_key(val,my_dict): \n",
    "    for key, values in my_dict.items(): \n",
    "        for value in values:\n",
    "            if val == value: \n",
    "                return key \n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "# first iterate over sentences to get all synonym list in the document in a dictionary\n",
    "# where the key is the first word in the text and the values are all it's synonyms that appeare in the text after it\n",
    "def Synonym_replace(new_sent_dict):\n",
    "    synonymList = []\n",
    "    text = {}\n",
    "    for key, sentence in new_sent_dict.items():\n",
    "        for word in sentence:\n",
    "            if word not in synonymList:\n",
    "                wordNetSynset =  wn.synsets(word)\n",
    "                values = []\n",
    "                for synSet in wordNetSynset:\n",
    "                    for synWords in synSet.lemmas():\n",
    "                        if synWords.name() in synonymList:\n",
    "                            continue\n",
    "                        else:\n",
    "                            synonymList.append(synWords.name())\n",
    "                            values.append(synWords.name())\n",
    "                text[word] = values\n",
    "    return text\n",
    "# then replace all synonms in the dictionary with their key  to unify all synonms\n",
    "def replace_word_synonym(new_sent_dict):\n",
    "    \n",
    "    text = []\n",
    "    sent_dict = dict()\n",
    "    dict_synonym = Synonym_replace(new_sent_dict)\n",
    "    for key, value in new_sent_dict.items():\n",
    "        \n",
    "        text = []\n",
    "        for word in value:\n",
    "            if get_key(word,dict_synonym) != \"key doesn't exist\":\n",
    "                text.append(get_key(word,dict_synonym))\n",
    "            else:\n",
    "                text.append(word)\n",
    "        sent_dict[key] = text\n",
    "    \n",
    "    return sent_dict\n",
    "\n",
    "# to get the ngrames from a list of words\n",
    "def ngramise(sequence):\n",
    "    bigrams = []\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "# removing the hyfen from word an checking if new words exist in the words of the document\n",
    "#if they exist then the hyfen is removed if not then it is considered one word\n",
    "def remove_hyfen(wordslist):\n",
    "    words_new = []\n",
    "    for word in wordslist:\n",
    "        digit = False\n",
    "        lang = False\n",
    "        if \"-\" in word:\n",
    "            words = word.split(\"-\")\n",
    "            for w in words:\n",
    "                if w.isdigit():\n",
    "                    digit = True\n",
    "            if digit:\n",
    "                newwords = [w for w in words]\n",
    "                words_new = words_new + newwords\n",
    "            else:\n",
    "                for w in words:\n",
    "                    if w in lang_Dict:\n",
    "                        lang = True\n",
    "                if lang:\n",
    "                    newwords = [w for w in words]\n",
    "                    words_new = words_new + newwords\n",
    "                else:\n",
    "                    newwords = [word]\n",
    "                    words_new = words_new + newwords\n",
    "        else:\n",
    "            newwords = [word]\n",
    "            words_new = words_new + newwords\n",
    "    return words_new\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Word frequency computation\n",
    "# remove stop words, punctuations and digits\n",
    "#lemmatize text using wordnet lemmatizer with POS for better results\n",
    "# output final list of sentences, list of words and their frequency, list of all words\n",
    "def EF(sent_dict):\n",
    "    final_sent = dict()\n",
    "    all_words = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #word_tokens = word_tokenize(text) \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    \n",
    "    for key, value in sent_dict.items():\n",
    "        value = remove_hyfen(value)\n",
    "        value = remove_punctuation(value)\n",
    "        filtered_sentence = [w for w in value if w not in stopWords] \n",
    "        #filtered_sentence = [re.sub(r'[\" \"]+', '', word) for word in filtered_sentence]\n",
    "        #replace everything not in the list with space\n",
    "        filtered_sentence = [re.sub(\"'\", '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub('\"', '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(pattern, '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(r'[0-9]+', '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word for word in filtered_sentence if len(word.strip()) >1]\n",
    "        filtered_sentence = [word for word in filtered_sentence if (re.sub(r\"[^a-zA-Z-]+\", '', word).strip())]\n",
    "        #filtered_sentence = [re.sub(r\"[^a-zA-Z-]+\", '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word.strip() for word in filtered_sentence]\n",
    "\n",
    "        wordnet_lemmatizer = WordNetLemmatizer() \n",
    "            \n",
    "        filtered_sentence = [wordnet_lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in filtered_sentence if  word != \" \" and word != '']\n",
    "        \n",
    "        final_sent[key] = filtered_sentence\n",
    "        all_words = all_words + filtered_sentence\n",
    "    word_frequency = list(Counter(all_words).items())\n",
    "    return word_frequency, final_sent , all_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess phase takes keyvalue dictionary after reading text an preprocess it\n",
    "# word frequency in text, title, keywords\n",
    "# outputs a list of sentences, and list of all words, dictionary containing a key-value list of sentences for\n",
    "# the processing phase\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \n",
    "    #detection of appreviation\n",
    "    sent_dict = replace_acronymss(keyvalue)\n",
    "\n",
    "    tokenize_sent_dict = tokenize_sent(sent_dict)\n",
    "    \n",
    "    new_sent_dict = conjunctions(tokenize_sent_dict)\n",
    "    \n",
    "    #e Replacement of word synonyms\n",
    "    sent_dict = replace_word_synonym(new_sent_dict)\n",
    "    sent_list =[]\n",
    "    for item in sent_dict.items():\n",
    "        sent_list.append(item)\n",
    "\n",
    "    all_bigrams = []\n",
    "    for key, value in sent_dict.items():\n",
    "        bigrams = ngramise(value)\n",
    "        all_bigrams = all_bigrams+bigrams\n",
    "    #only keep bi-grams that appear atleast twice\n",
    "    bi_count = dict(Counter(all_bigrams))\n",
    "    delete = [key for key,value in bi_count.items() if value < 2]\n",
    "    # delete the key \n",
    "    for key in delete: del bi_count[key]\n",
    "    bi_count_list =[]\n",
    "    for key , value in bi_count.items():\n",
    "        bi_count_list.append([(key),value])\n",
    "        \n",
    "    \n",
    "    word_freq ,final_sent, all_words = EF(sent_dict)\n",
    "    Sentences_list = []\n",
    "    for item in final_sent.items():\n",
    "        Sentences_list.append(item)     \n",
    "    \n",
    "    #word frequencies\n",
    "    #freq of words in title\n",
    "    title =[]\n",
    "    title_freq = Counter(title)\n",
    "\n",
    "    keywords =[]\n",
    "    Keyword_freq = Counter(keywords)\n",
    "    #Keyword_freq = keywords\n",
    "    #print(Keyword_freq)\n",
    "    w_freq = Counter(all_words)\n",
    "    final_Word_freq = dict()\n",
    "    for word in all_words:\n",
    "        if word not in title_freq.keys():\n",
    "            title_freq[word] = 0\n",
    "        if word not in Keyword_freq.keys():\n",
    "            Keyword_freq[word] = 0\n",
    "        final_Word_freq[word] = title_freq[word] + Keyword_freq[word] + w_freq[word] \n",
    "\n",
    "    ##final list of sentences after preprocessing\n",
    "    final_input = []\n",
    "    for (index, sentence) in Sentences_list:\n",
    "        final_input.append(\" \".join(sentence))\n",
    "        \n",
    "    keyword_word_freq = []\n",
    "    for word in all_words:\n",
    "        keyword_word_freq.append([word,Keyword_freq[word]])  \n",
    "    \n",
    "    title_word_freq = []\n",
    "    for word in all_words:\n",
    "        title_word_freq.append([word, title_freq[word]])\n",
    "\n",
    "    Sentences_list\n",
    "    List_of_sent = dict()\n",
    "    for (key, sent) in Sentences_list:\n",
    "        List_of_sent[key] = \" \".join(sent)\n",
    "\n",
    "    final_Word_freq\n",
    "    finalwordfreq = []\n",
    "    for key , value in final_Word_freq.items():\n",
    "        finalwordfreq.append([key,value])\n",
    "    \n",
    "    return (Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "            ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to get the frequency of words in bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in list of bi-grams and outputs a list of (word, frequency) \n",
    "def bigrams_freq(bi_count_list):\n",
    "    words_freq_bigram = dict()\n",
    "    bi_count = []\n",
    "    for list_bi in bi_count_list:\n",
    "        (w1,w2), i = list_bi\n",
    "        if w1 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "        else:\n",
    "            words_freq_bigram[w1] = i\n",
    "        if w2 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "        else:\n",
    "            words_freq_bigram[w2] = i   \n",
    "    for key,value in words_freq_bigram.items():\n",
    "        bi_count.append([key,value])\n",
    "    return bi_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight computation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of nodes and outputs their weights using their frequencies\n",
    "#weights\n",
    "def get_weights(list_of_nodes):\n",
    "    import pandas as ps\n",
    "    import statistics as st\n",
    "    node_weight_dict = dict()\n",
    "    node_list= list_of_nodes\n",
    "    FWL= finalwordfreq\n",
    "    TWL= title_word_freq\n",
    "    KWL= keyword_word_freq\n",
    "    BWL= bi_count \n",
    "    PWL= Pnouns\n",
    "    #BWL = []\n",
    "    #PWL = []\n",
    "    \n",
    "\n",
    "    F=[]     #freq of words  which appeared in nodes WRT document\n",
    "    T=[]     #freq of words  which appeared in nodes WRT title\n",
    "    K=[]     #freq of words  which appeared in nodes WRT keyword list \n",
    "    B=[]     #freq of words which appeared in nodes WRT Bi_Gram list \n",
    "    P=[]     #list of detection the proper noune \n",
    "    FW=[]    \n",
    "    Xt,Xk,Xbi,Xp,Xb=1,1,1,1,1\n",
    "    weighted_node=[]\n",
    "    \n",
    "    \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "    \n",
    "        for j in FWL:\n",
    "            if(i==j[0]):\n",
    "                f=j[1]\n",
    "                F.append(f)\n",
    "                flag=1\n",
    "                \n",
    "        if(flag!=1): \n",
    "            F.append(0) \n",
    "        \n",
    " \n",
    "\n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in TWL:\n",
    "            if(i==j[0]):\n",
    "                t=j[1]\n",
    "                T.append(t)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "             T.append(0)\n",
    "                \n",
    "        \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in KWL:\n",
    "            if(i==j[0]):\n",
    "                k=j[1]\n",
    "                K.append(k)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            K.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in BWL:\n",
    "            if(i==j[0]):\n",
    "                b=j[1]\n",
    "                B.append(b)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            B.append(0)\n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in PWL:\n",
    "            if(i==j):\n",
    "                P.append(1)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            P.append(0)             \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in FWL:\n",
    "        fw=i[1]\n",
    "        FW.append(fw)\n",
    "    \n",
    "    if FW == []:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "    elif len(FW) != 0:\n",
    "        avrage_word_frequency=st.mean(FW)\n",
    "        median_word_frequency=st.median(FW) \n",
    "    else:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "        \n",
    "    AW=abs(avrage_word_frequency-median_word_frequency)\n",
    "    \n",
    "    for i in range(len(node_list)):\n",
    "        WN=F[i]+Xt*AW*T[i]+Xk*AW*K[i]+Xb*AW*B[i]+Xp*AW*P[i]\n",
    "        weighted_node.append(WN)\n",
    "        node_weight_dict[node_list[i]] = WN\n",
    "    \n",
    "    return node_weight_dict,weighted_node\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for algorithm two Getting the candidate edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge between source and distenation\n",
    "def get_edge(source,distenation):\n",
    "    return edgeget[source,distenation]\n",
    "\n",
    "#distination node list\n",
    "# gets the weights of the distination nodes given a source node , all node weights and source-destination dictionary \n",
    "def getweights_of_dist_nodes(source_destinations,sourcenode,All_nodes_ww):  \n",
    "    x=source_destinations[sourcenode]\n",
    "    distenation_node_list={}\n",
    "    weights=[]\n",
    "    nothing=0\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i])==('E#')) or ( x[i] not in All_nodes_ww):\n",
    "            #nothing is a flag for destination nodes\n",
    "            #if one then there is none\n",
    "            nothing=1\n",
    "        else:\n",
    "            distenation_node_list[x[i]]=All_nodes_ww[x[i]]\n",
    "            weights.append((All_nodes_ww[x[i]]))\n",
    "        if len(weights) != 0:\n",
    "            nothing=0\n",
    "    return distenation_node_list,weights,nothing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for candidate edges between a source and a destination given a criteria \n",
    "def get_candidate_edges(source_destinations,node_listx,wxx,out_carteria,All_weight,input_nodes,All_SD):\n",
    "    c=[]\n",
    "    xxx = []\n",
    "    no_list,totalweights=node_listx,All_weight\n",
    "\n",
    "    source_node_weight=0\n",
    "    distination_node_weight=0\n",
    "    average_of_the_node_weight=np.average(totalweights)#(np.sum(node_list)/number_of_nodes)\n",
    "    median_of_the_node_list_weight=np.median(totalweights)\n",
    "  \n",
    "    if average_of_the_node_weight>median_of_the_node_list_weight:#can be replaced with source=max(avg,median)\n",
    "        source_node_weight=average_of_the_node_weight\n",
    "    else:\n",
    "        source_node_weight=median_of_the_node_list_weight\n",
    "\n",
    "    for node,weight in no_list.items():\n",
    "        \n",
    "        if weight>=source_node_weight:#بيعرف اذا كانت بدايه الجمله ولا لا \n",
    "            outnodes1,outnodesweights,flagnothing=getweights_of_dist_nodes(source_destinations,node,no_list)#فنكشن جوا الجي بتجبلي كل الديستناشن بتاعت النودواختار بينهم هسقط مين\n",
    "            outnodes2,nodes2weights,flag2nothing=getweights_of_dist_nodes(All_SD,node,input_nodes)\n",
    "            outnodes1=[(i,j) for (i,j) in outnodes1.items() if j!=0 ] \n",
    "            outnodes2 =[(i,j) for (i,j) in outnodes2.items() if j!=0 ] \n",
    "        \n",
    "            if flagnothing==0:\n",
    "                #destination nodes exsist\n",
    "                #get their average and max weights for avg and max criterias\n",
    "                average_weight=np.average(nodes2weights)\n",
    "                max_weight=max(nodes2weights)\n",
    "            else:\n",
    "                #no destination nodes \n",
    "                average_weight=0\n",
    "                max_weight=0\n",
    "            # get threshold of destination weights according to criteria \n",
    "            if (out_carteria ==\"avg\"):\n",
    "                distination_node_weight=average_weight\n",
    "            elif (out_carteria ==\"node_avg\"):\n",
    "                distination_node_weight=source_node_weight\n",
    "            elif (out_carteria==\"max\"):\n",
    "                distination_node_weight=max_weight\n",
    "            else:\n",
    "                distination_node_weight=average_weight\n",
    "\n",
    "\n",
    "            if (bool(outnodes1)==False):\n",
    "                # no destination nodes \n",
    "                nothing=1  \n",
    "            else: \n",
    "                # check for the candidate edges\n",
    "                for out,weight_of_out_node in outnodes1:\n",
    "                    if weight_of_out_node>=distination_node_weight and flagnothing==0:\n",
    "                        c.append(get_edge(node,out))# append the edge\n",
    "                        xxx.append([node,out])\n",
    "\n",
    "    cx=[]\n",
    "    for i in range(0,len(c)):\n",
    "        cx.append(c[i][0])\n",
    "    return cx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for the post processing phase (algorithm 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weight of the sentence order in the input document\n",
    "def sentence_order(count_summ, index):\n",
    "    return(count_summ - index)/count_summ\n",
    "\n",
    "#given a list of candidate sentences rank them according to ranking criteria\n",
    "def compute_rank(candidate_summary):\n",
    "    ranks = {}\n",
    "    candidate_summary = reorder(candidate_summary)\n",
    "    \n",
    "    for i, sent in enumerate(candidate_summary):\n",
    "        rank = sentence_order(len(candidate_summary), i)\n",
    "        ranks[sent] = rank\n",
    "        \n",
    "    sort_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)    \n",
    "    return ranks   \n",
    "\n",
    "#vectorize sentences for clustering\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "#numner of words in a sentence\n",
    "def get_words_count(sent):\n",
    "    len_sent = 0\n",
    "    for i in sent.split(\" \"):\n",
    "        len_sent = len_sent +1\n",
    "    return len_sent\n",
    "\n",
    "# reorder the summary based on the order from the input text\n",
    "def reorder(summary):\n",
    "    index_summ = []\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    order = dict()\n",
    "    final_order = []\n",
    "    for sent in summary:\n",
    "        if sent in value_sent:\n",
    "            index_sent = value_sent.index(sent)\n",
    "            index_summ.append(index_sent)\n",
    "            order[index_sent] = sent\n",
    "            \n",
    "    for i in sorted(order):\n",
    "        final_order.append(order[i])\n",
    "\n",
    "    return final_order\n",
    "\n",
    "#concatenate the sentences in the candidate summary\n",
    "def concat(final_order):\n",
    "    return(\"\\n\".join(final_order))\n",
    " \n",
    "# get the index of a sentence from the input text\n",
    "def get_index(sent):\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    index_sent = value_sent.index(sent)\n",
    "    return index_sent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the sentences and preparing for clusting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank sentences then cluster them in 5 clusters\n",
    "def post_processing(list_of_candidates_summeries):\n",
    "    global n_clusters\n",
    "    global cluster_map\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    sentences_ranks = compute_rank(candidate_summary)\n",
    "   \n",
    "    sentences_ordered =  [key for key,value  in sentences_ranks.items()]\n",
    "    summary_length =0\n",
    "    visited_sentences = []\n",
    "    \n",
    "    sentences = sentences_ordered\n",
    "\n",
    "    tfidfvect = TfidfVectorizer(stop_words='english')\n",
    "    X = tfidfvect.fit_transform(sentences)\n",
    "\n",
    "    # number of clusters is one for no clusting\n",
    "    if len(sentences) <1: # number of clusters can't be more than number of sentences\n",
    "        n_clusters = len(sentences)\n",
    "    else:\n",
    "        n_clusters = 1\n",
    "    clf = KMeans(n_clusters = n_clusters,init = 'k-means++', max_iter = 1000)\n",
    "    labels = clf.fit_predict(X)\n",
    "    clustered_sentences =[[] for i in range(5)]\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['data_index'] = sentences\n",
    "    cluster_map['cluster'] = clf.labels_\n",
    "    #print(cluster_map) to visualize clustered sentences\n",
    "    return n_clusters,cluster_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 4 (clustring) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_4(summary_length,threshold,max_words_limit,list_of_candidates_summeries,n_clusters,cluster_map):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    summary_length = 0\n",
    "    while (len(visited_sentences) < len(candidate_summary)) and ((summary_length)< max_words_limit) :\n",
    "        for i in range(n_clusters):\n",
    "        \n",
    "            clustered_sentences = cluster_map[cluster_map.cluster == i]['data_index'].tolist()\n",
    "            \n",
    "            if summary_length >= (max_words_limit+threshold):\n",
    "                #if we reached the limit break\n",
    "                break\n",
    "            for sent in clustered_sentences:\n",
    "                if sent not in visited_sentences :\n",
    "                    #check sentences in the cluster that was not added to candidate sentences\n",
    "                    temp_length = summary_length + get_words_count(sent)\n",
    "                    if temp_length < max_words_limit :\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        #if the length of summary doesn't exceed the maximum length add the sentence to the final summary\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif(temp_length >= max_words_limit) and (temp_length <= (max_words_limit + threshold)):\n",
    "                        #else check if it is less than the max length + a given threshold \n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif temp_length > (max_words_limit + threshold):\n",
    "                        #else this sentence can't be added to the final summary as it exceeds  the length and the threshold\n",
    "                        visited_sentences.append(sent)\n",
    "        \n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for the graph algorithm (algorithm 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the  graph\n",
    "\n",
    "# Add a vertex to the dictionary\n",
    "def add_vertex(v):\n",
    "    global graph\n",
    "    global vertices_no\n",
    "    if v not in graph:\n",
    "        vertices_no = vertices_no + 1\n",
    "        graph[v] = []\n",
    "\n",
    "# Add an edge between vertex v1 and v2 with edge weight e\n",
    "#v1 is the source node and v2 is the distenation node\n",
    "edgeget={}\n",
    "def add_edge(v1, v2, e_order, e_value):\n",
    "    global graph\n",
    "    # Check if vertex v1 is a valid vertex\n",
    "    if ((v1 in graph) and (v2 in graph)):\n",
    "        temp = [v2, e_order, e_value]\n",
    "        edgeget[v1,v2]=[e_value]\n",
    "        graph[v1].append(temp)\n",
    "\n",
    "# Print the graph\n",
    "def print_graph():\n",
    "    global graph\n",
    "    for vertex in graph:\n",
    "        for edges in graph[vertex]:\n",
    "            print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "# get the destinations\n",
    "def get_destinations(source):\n",
    "    global graph\n",
    "    distinations=[]\n",
    "    if(source in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0] not in distinations):\n",
    "                distinations.append(elements[0])\n",
    "    return distinations\n",
    "\n",
    "# get the edges\n",
    "def get_edges(source,dist):\n",
    "    global graph\n",
    "    Edge=\"\"\n",
    "    if(source in graph and dist in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0]==dist):\n",
    "                Edge= Edge + elements[2]\n",
    "    return Edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The iterative part of the algorithm \n",
    "##  iterates over candidate summary until it stabilize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_graph(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria):\n",
    "   \n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    input_text = candidate_summ\n",
    "    last_summ_len = candidate_summ_len\n",
    "    Last_summ = candidate_summ\n",
    "    sent_idx = can_edges\n",
    "    can_edges=[]\n",
    "    List_of_sent = list_pre_cansumm\n",
    "    list_pre_cansumm = []\n",
    "    list_of_candidates_summeries = []\n",
    "    All_nodes=[]\n",
    "    nodes_list = []\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    index = 0\n",
    "    \n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "\n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "\n",
    "        Types=pos_tag(Words)\n",
    "       ## check for both nouns and verbs\n",
    "        for Type in Types:\n",
    "            if (Type[1]=='NN' or Type[1]=='NNP' or Type[1]=='NNS' or Type[1]=='NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    "\n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        LIST_OF_NODES=[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "        \n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "            \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "        \n",
    "        #****************************************************************************\n",
    "        #all nodes in the sentence\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[index] = (source_destinations,list_of_nodes) #nodes in a sentences\n",
    "    \n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        index = index + 1\n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes) # getting the weights of all nodes in a sentence\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in enumerate(List_of_sent): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[index] and (List_of_sent[index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    candidate_summ_len = 0\n",
    "    for index ,sent in zip(sent_idx,List_of_sent):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "    return list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final summarization function\n",
    "## main function takes in an input string and outputs a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this wraps up all of the model \n",
    "#input a text\n",
    "#output a summary\n",
    "def summarization(Text_in):\n",
    "\n",
    "    global vertices_no\n",
    "    global keyvalue\n",
    "    global key_value_list\n",
    "    global lang_Dict\n",
    "    global Sentences_list\n",
    "    global final_input\n",
    "    global all_bigrams\n",
    "    global bi_count_list\n",
    "    global word_freq\n",
    "    global all_words\n",
    "    global finalwordfreq\n",
    "    global List_of_sent\n",
    "    global title_word_freq\n",
    "    global keyword_word_freq\n",
    "    global All_nodes\n",
    "    global list_of_candidates_summeries\n",
    "    global list_pre_cansumm\n",
    "    global Word_Freq\n",
    "    global bi_count\n",
    "    global Cand_edge\n",
    "    out_carteria=\"avg\"\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    vertices_no =0\n",
    "    can_edges = []\n",
    "\n",
    "    keyvalue, key_value_list,lang_Dict = read_input(Text_in) \n",
    "    \n",
    "    (Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "     ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess() \n",
    "\n",
    "    Cand_edge =[]\n",
    "    Word_Freq = {}\n",
    "    for key, value in finalwordfreq:\n",
    "        Word_Freq[key] = value\n",
    "    \n",
    "    bi_count = bigrams_freq(bi_count_list) \n",
    "\n",
    "    Last_summ = keyvalue.values()\n",
    "    last_summ_len = 0\n",
    "    for value in keyvalue.values():\n",
    "        last_summ_len = last_summ_len + get_words_count(value)\n",
    "\n",
    "\n",
    "    All_nodes=[]\n",
    "    list_of_candidates_summeries=[]\n",
    "    list_pre_cansumm = []\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    ##lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    # Add an edge between vertex v1 and v2 with edge weight e\n",
    "    #v1 is the source node and v2 is the distenation node\n",
    "    edgeget={}\n",
    "\n",
    "    \n",
    "    # driver code\n",
    "    global graph \n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}\n",
    "    global Pnouns\n",
    "    Pnouns = []\n",
    "\n",
    "    for sent_index, sentence in enumerate(List_of_sent.values()):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        \n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "        \n",
    "        Types=pos_tag(Words)\n",
    "        for Type in Types:\n",
    "            if (Type[1]=='NN' or Type[1]=='NNP' or Type[1]=='NNS' or Type[1]=='NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "                if(Type[1] == 'NNP'):\n",
    "                    Pnouns.append(\"\")\n",
    "            \n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    " \n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "\n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "        \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "        #****************************************************************************\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[sent_index] = (source_destinations,list_of_nodes)\n",
    "\n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        print_graph() \n",
    "        #****************************************************************************     \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    for sent_index, sentence in enumerate(S.values()): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[sent_index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[sent_index] and (List_of_sent[sent_index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[sent_index])\n",
    "                \n",
    "\n",
    "    global final_summary\n",
    "    global visited_sentences\n",
    "    max_words_limit= 70\n",
    "    threshold = 15\n",
    "    final_summary = []\n",
    "    visited_sentences = []\n",
    "    summary_length = 0\n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "\n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "    \n",
    "    \n",
    "    while(True):\n",
    "        if(candidate_summ_len > (max_words_limit + threshold)):\n",
    "            if(candidate_summ_len != last_summ_len):\n",
    "                Cand_edge,list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria) \n",
    "\n",
    "            elif(out_carteria != \"max\"):\n",
    "                out_carteria=\"max\"\n",
    "                Cand_edge,list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(Cand_edge,can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria)\n",
    "            else:\n",
    "                list_of_candidates_summeries = candidate_summ\n",
    "                break\n",
    "        elif(candidate_summ_len < (max_words_limit)):\n",
    "            candidate_summ = Last_summ\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "        else:\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "    \n",
    "    n_clusters,cluster_map = post_processing(list_of_candidates_summeries)\n",
    "    final_summary = algorithm_4(summary_length,threshold,max_words_limit,list_of_candidates_summeries,n_clusters,cluster_map)\n",
    "  \n",
    "    \n",
    "    final = reorder(final_summary)\n",
    "    outsumm = concat(final)\n",
    "    return outsumm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to run the code \n",
    "## summary = summarization(input your text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
