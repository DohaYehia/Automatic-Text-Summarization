{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, remove_email, remove_url, lemmatize_word,expand_contraction\n",
    "from text_preprocessing import remove_punctuation as punct\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "## importing libraries\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#post-Processing\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster import hierarchy\n",
    "#from sklearn.cluster import AgglomerativeClustring\n",
    "from rouge_score import rouge_scorer\n",
    "!pip install abbreviations\n",
    "from abbreviations import schwartz_hearst\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path of the dataset DUC2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import os\n",
    "doc_path = \"DucDataset/DUC2002_Summarization_Documents/docs\"\n",
    "doc_sent = \"DucDataset/DUC2002_Summarization_Documents/docs.with.sentence.breaks\"\n",
    "summ1_path = \"DucDataset/DUC2002_Summarization_Documents/summaries\"\n",
    "files = os.listdir(doc_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in the different types of file each has it's own way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_perdocs_xml(path):\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        xml = fp.read()\n",
    "\n",
    "    header_patt = r'<SUM.*?TYPE=\"PERDOC\"\\s+SIZE=\"(.*?)\"\\s+DOCREF=\"(.*?)\"' \\\n",
    "        '\\s+SELECTOR=\"(.*?)\"\\s+SUMMARIZER=\"(.*?)\"\\s*>'\n",
    "    summary_patt = header_patt + r'(.*?)</SUM>'\n",
    "\n",
    "    headers = [m for m in re.findall(header_patt, xml, flags=re.DOTALL)]\n",
    "    headers_found = len(headers)\n",
    "\n",
    "    data = {}\n",
    "    for match in re.findall(summary_patt, xml, flags=re.DOTALL):\n",
    "        size, doc_id, selector, summarizer, summary_text = match\n",
    "        doc_id = doc_id.strip()\n",
    "        sentences = summary_text\n",
    "        sentences = sentences.strip().replace('\\n', ' ')\n",
    "        \n",
    "        sentences = sent_tokenize(sentences)\n",
    "        data[doc_id] = sentences\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "    soup = BeautifulSoup(file)\n",
    "    text = []\n",
    "    for a in soup.find_all('text'):\n",
    "        for s in a.find_all('s'):\n",
    "            text.append(s.string)\n",
    "    return text\n",
    "    \n",
    "def parse_input_docs(paths,fn,text_path):\n",
    "\n",
    "    if fn.startswith(\"WSJ\"):\n",
    "        doc_id, Headline, text , t  = parse_wsj(paths)\n",
    "    elif fn.startswith(\"SJMN\"):\n",
    "        doc_id, Headline, text,t  = parse_sjmn(paths)\n",
    "    elif fn.startswith(\"FT\"):\n",
    "        doc_id, Headline, text = parse_ft(paths)\n",
    "    elif fn.startswith(\"AP\"):\n",
    "        doc_id, Headline, text  = parse_ap(paths)\n",
    "    elif fn.startswith(\"LA\"):\n",
    "        doc_id, Headline, text  = parse_la(paths)\n",
    "    elif fn.startswith(\"FBIS\"):\n",
    "        doc_id, Headline, text  = parse_fbis(paths)\n",
    "    else:\n",
    "        doc_id=\" \"\n",
    "        Headline = \" \"\n",
    "        text = \" \" \n",
    "    if text != \" \":\n",
    "        text = parse(text_path)\n",
    "    if fn.startswith(\"WSJ\") or fn.startswith(\"SJMN\"):\n",
    "        if text != \" \":\n",
    "            text = t + text \n",
    "        else:\n",
    "            text = t\n",
    "\n",
    "\n",
    "    data = {\"doc_id\": doc_id,\"title\": Headline ,\"input_data\": text}\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_fbis(file_path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADER>(.*?)</HEADER>+', file)\n",
    "    if len(Headline[0].split('Document Title:')) > 1:\n",
    "        Headline = Headline[0].split('Document Title:')[-1]\n",
    "    else:\n",
    "        Headline = []\n",
    "    \n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = re.sub(r\"</?F.*?>\", r\" \", text) \n",
    "    text = sent_tokenize(text)       \n",
    "    return doc_id, Headline, text \n",
    "\n",
    "def parse_sjmn(path):\n",
    "\n",
    "    sentences = []\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)\n",
    "    headline = Headline\n",
    "    leadpara = re.findall(r'<LEADPARA>(.*?)</LEADPARA>+', file)[0]\n",
    "    leadpara = sent_tokenize(leadpara)\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "    out = text\n",
    "    return doc_id, headline, out, leadpara\n",
    "\n",
    "def parse_wsj(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HL>(.*?)</HL>+', file)\n",
    "    headline = Headline\n",
    "    t = re.findall(r'<LP>(.*?)</LP>+', file)\n",
    "\n",
    "    if t :\n",
    "        t = sent_tokenize(t[0])\n",
    "    else :\n",
    "        t = []\n",
    "    \n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, headline, text ,t\n",
    "\n",
    "def parse_ft(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)[0]\n",
    "    headline = Headline.split(\"/\")[1:]\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, headline, text \n",
    "\n",
    "def parse_ap(path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEAD>(.*?)</HEAD>+', file)\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = sent_tokenize(text)\n",
    "            \n",
    "    return doc_id, Headline, text \n",
    "def parse_la(file_path):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        file = fp.read()\n",
    "        fp.closed\n",
    "\n",
    "    file = file.strip().replace('\\n', ' ')\n",
    "    doc_id = re.findall(r'<DOCNO>(.*?)</DOCNO>+', file)[0]\n",
    "    Headline = re.findall(r'<HEADLINE>(.*?)</HEADLINE>+', file)\n",
    "    Headline = re.findall(r'<P>(.*?)</P>+', Headline[0])\n",
    "\n",
    "    text = re.findall(r'<TEXT>(.*?)</TEXT>+', file)[0]\n",
    "    text = re.findall(r'<P>(.*?)</P>+', text)\n",
    "    #text = tokenizer.tokenize(text)\n",
    "            \n",
    "    return doc_id, Headline, text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1_files = os.listdir(summ1_path)\n",
    "Summaries = {}\n",
    "ref_data1 = {}\n",
    "for file in ref1_files:\n",
    "    for files in os.listdir( summ1_path + \"/\" + file):\n",
    "        if files == \"perdocs\":\n",
    "            check = summ1_path + \"/\" + file + \"/perdocs\"\n",
    "            data =  parse_perdocs_xml(check)\n",
    "            input_path = doc_path + \"/\" +file[:-1]\n",
    "            in_ids = os.listdir(input_path)\n",
    "            ref_data1[file] = data\n",
    "            for key,value in data.items():\n",
    "                doc_id = key\n",
    "                if doc_id in Summaries.keys():\n",
    "                    Summaries[doc_id].append(value)\n",
    "                else:\n",
    "                    Summaries[doc_id] = [value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(doc_path)\n",
    "file_data = []\n",
    "check_dups =[]\n",
    "Text_in = {}\n",
    "for file in files:\n",
    "    file_path = doc_path + \"/\" + file\n",
    "    in_files = os.listdir(file_path)\n",
    "    for In_in_file in in_files:\n",
    "        file_path = doc_path + \"/\" + file + \"/\" + In_in_file\n",
    "        text_path = doc_sent + \"/\" + file + \"/\" + In_in_file + \".S\"\n",
    "        In_data = parse_input_docs(file_path, In_in_file,text_path)\n",
    "        doc_id = In_data['doc_id'].strip()\n",
    "        title = In_data['title']\n",
    "        input_data = In_data['input_data']\n",
    "        if doc_id not in check_dups:\n",
    "            Text_in[doc_id] = {'doc_id': doc_id, 'title': title, 'data_input': input_data}\n",
    "            check_dups.append(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (0.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scispacy.abbreviation.AbbreviationDetector at 0x1c4d4d2e400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "def create_lang_detector(nlp, name):\n",
    "    return abbreviation_pipe\n",
    "Language.factory(\"abbreviation_pipe\", func=create_lang_detector)\n",
    "nlp.add_pipe(\"abbreviation_pipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for extracting variables needed from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(i,doc_id):\n",
    "\n",
    "    title_line = i['title']\n",
    "    text = i['data_input']\n",
    "    \n",
    "    Summary = Summaries[doc_id]\n",
    "    \n",
    "    title = []\n",
    "    keywords =[]\n",
    "    if(len(title_line) != 0):\n",
    "        for i in title_line:\n",
    "            title = title + word_tokenize(i)\n",
    "            \n",
    "    title = [item.lower() for item in title]\n",
    "        \n",
    "    lang_Dict = []\n",
    "    for line in text:\n",
    "        for word in line.split(\" \"):\n",
    "            if word not in lang_Dict:\n",
    "                lang_Dict.append(word)\n",
    "    iter_dict = lang_Dict\n",
    "    lang_Dict = []\n",
    "    for i in iter_dict:\n",
    "        if '\\n' in i:\n",
    "            lang_Dict = lang_Dict + i.split(\"\\n\")\n",
    "        else:\n",
    "            lang_Dict.append(i)\n",
    "     \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    \n",
    "    lang_dict = []\n",
    "    for word in lang_Dict:\n",
    "        if word.strip():\n",
    "            word = word.lower()\n",
    "            word = expand_contraction(word)\n",
    "            word = re.sub(pattern, '', word)\n",
    "            lang_dict.append(word)\n",
    "            \n",
    "    lang_Dict = []\n",
    "    for word in lang_dict:\n",
    "        if \" \" in word:\n",
    "            for w in word.split(\" \"):\n",
    "                if w.strip():\n",
    "                    lang_Dict.append(w)\n",
    "        else:\n",
    "            lang_Dict.append(word)\n",
    "    keyvalue = dict() \n",
    "\n",
    "    index= 0\n",
    "    for line in text:\n",
    "        if(line and (line.strip()) ):\n",
    "            i = index\n",
    "            keyvalue[i] = line\n",
    "            index = index + 1\n",
    "\n",
    "    key_value_list =[]\n",
    "    for item in keyvalue.items():\n",
    "        key_value_list.append(item)\n",
    "    \n",
    "    return title,keywords,keyvalue, key_value_list,Summary, lang_Dict\n",
    "title,keywords,keyvalue, key_value_list,Summary, lang_Dict = read_input(Text_in['AP880911-0016'],'AP880911-0016')\n",
    "#print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronymss(keyvalue):\n",
    "        sent_dict = dict() \n",
    "        whole_text = \"\"\n",
    "        for key, text in keyvalue.items():\n",
    "            whole_text = whole_text + \"#*# \" + text\n",
    "        whole_text = whole_text.lower()\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=whole_text, most_common_definition=True)\n",
    "        for key,value in pairs.items():\n",
    "            whole_text = whole_text.replace(value, key)\n",
    "        output_text = whole_text.split(\"#*# \")\n",
    "        i = 0;\n",
    "        for l in output_text:\n",
    "            if l != \"\":\n",
    "                sentence = Brackets(l)\n",
    "                sent_dict[i] = sentence\n",
    "                i = i+1\n",
    "        return sent_dict\n",
    "\n",
    "#remove hyphens and tokenizing sentence\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = expand_contraction(word)\n",
    "        if word.lower() == \"n't\":\n",
    "            word = \"not\"\n",
    "        if word != '':\n",
    "            word = word.lower()\n",
    "            #word = expand_contraction(word)\n",
    "            new_words.append(word)\n",
    "    #print(words)\n",
    "    #print(\"after remove punck\")\n",
    "    return new_words\n",
    "\n",
    "def tokenize_sent(sent_dict):\n",
    "    keyvalueToken = dict() \n",
    "    TorF = True\n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        words = nltk.word_tokenize(expand_contraction(value))\n",
    "        words = [re.sub(r'(\\n-)','',word) for word in words]\n",
    "        words = [re.sub(r'(\\n)',' ',word) for word in words]\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                 new_words = new_words + word.split(\" \")\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        \n",
    "        keyvalueToken[key] = new_words\n",
    "        \n",
    "    return keyvalueToken\n",
    "#continue sent reduction\n",
    "def conjunctions(tokenize_sent_dict):\n",
    "    new_sent_dict = dict()\n",
    "    conjunctive_adverbs= [\"accordingly\",\"comparatively\",\"contrarily\", \n",
    "                      \"also\",\" further\", \"nevertheless\", \"indeed\",\" otherwise\",\n",
    "                      \"result\",\"as\",\"equally\" ,\"conversely\" ,\"besides\",\n",
    "                      \"furthermore\" ,\"nonetheless\" ,\"fact\" ,\"consequently\",\n",
    "                     \" likewise\" ,\"however\" ,\"addition\" \",moreover\" ,\"surprisingly\",\n",
    "                     \" hence\" ,\"similarly \",\"comparison\" ,\"still \",\"therefore \",\n",
    "                      \"contrast\" ,\"thus\" ,\"instead\"  ,\"rather\"\n",
    "                      , \"other\", \"additionally\", \"addition\",'although','already',\"including\",'according',\n",
    "                    'totally','likely','unlikely','perhaps',\"s\",'naturally',\n",
    "                      \"finally\", \"based\",\"on\",\"that\" , \"besides\",\"namely\",\"anyway\",\n",
    "                     \"then\",\"next\",\"thereafter\",\"certainly\",\"now\",\"finally\",\"meanwhile\",\n",
    "                     \"subsequently\",\"yet\",\"elsewhere\",\"thereafter\",\"undoubtedly\",\n",
    "                     \"incidentally\",\"otherwise\",\"regardless\",\"begin\",\"in\"]\n",
    "    \n",
    "    for key, text in tokenize_sent_dict.items():\n",
    "        new_sent = [word for word in text if word.lower() not in conjunctive_adverbs]\n",
    "        new_sent_dict[key] = new_sent\n",
    "\n",
    "    return new_sent_dict\n",
    "\n",
    "#d Sentence reduction \n",
    "def Brackets(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def get_key(val,my_dict): \n",
    "    for key, values in my_dict.items(): \n",
    "        for value in values:\n",
    "            if val == value: \n",
    "                return key \n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "def replace_word_synonym(new_sent_dict):\n",
    "    \n",
    "    text = []\n",
    "    sent_dict = dict()\n",
    "    dict_synonym = Synonym_replace(new_sent_dict)\n",
    "    for key, value in new_sent_dict.items():\n",
    "        \n",
    "        text = []\n",
    "        for word in value:\n",
    "            if get_key(word,dict_synonym) != \"key doesn't exist\":\n",
    "                text.append(get_key(word,dict_synonym))\n",
    "            else:\n",
    "                text.append(word)\n",
    "        sent_dict[key] = text\n",
    "    \n",
    "    return sent_dict\n",
    "\n",
    "def Synonym_replace(new_sent_dict):\n",
    "    synonymList = []\n",
    "    text = {}\n",
    "    for key, sentence in new_sent_dict.items():\n",
    "        for word in sentence:\n",
    "            if word not in synonymList:\n",
    "                wordNetSynset =  wn.synsets(word)\n",
    "                values = []\n",
    "                for synSet in wordNetSynset:\n",
    "                    for synWords in synSet.lemmas():\n",
    "                        if synWords.name() in synonymList:\n",
    "                            continue\n",
    "                        else:\n",
    "                            synonymList.append(synWords.name())\n",
    "                            values.append(synWords.name())\n",
    "                text[word] = values\n",
    "    return text\n",
    "#f Use words N-grams\n",
    "def ngramise(sequence):\n",
    "    bigrams = []\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def remove_hyfen(wordslist):\n",
    "    words_new = []\n",
    "    for word in wordslist:\n",
    "        digit = False\n",
    "        lang = False\n",
    "        if \"-\" in word:\n",
    "            words = word.split(\"-\")\n",
    "            for w in words:\n",
    "                if w.isdigit():\n",
    "                    digit = True\n",
    "            if digit:\n",
    "                newwords = [w for w in words]\n",
    "                words_new = words_new + newwords\n",
    "            else:\n",
    "                for w in words:\n",
    "                    if w in lang_Dict:\n",
    "                        lang = True\n",
    "                if lang:\n",
    "                    newwords = [w for w in words]\n",
    "                    words_new = words_new + newwords\n",
    "                else:\n",
    "                    newwords = [word]\n",
    "                    words_new = words_new + newwords\n",
    "        else:\n",
    "            newwords = [word]\n",
    "            words_new = words_new + newwords\n",
    "    return words_new\n",
    "#4.3. Word frequency computation\n",
    "def EF(sent_dict):\n",
    "    final_sent = dict()\n",
    "    all_words = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #word_tokens = word_tokenize(text) \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    \n",
    "    for key, value in sent_dict.items():\n",
    "        value = remove_hyfen(value)\n",
    "        value = remove_punctuation(value)\n",
    "        filtered_sentence = [w for w in value if w not in stopWords] \n",
    "        #filtered_sentence = [re.sub(r'[\" \"]+', '', word) for word in filtered_sentence]\n",
    "        #replace everything not in the list with space\n",
    "        filtered_sentence = [re.sub(\"'\", '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub('\"', '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(pattern, '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(r'[0-9]+', '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word for word in filtered_sentence if len(word.strip()) >1]\n",
    "        filtered_sentence = [word for word in filtered_sentence if (re.sub(r\"[^a-zA-Z-]+\", '', word).strip())]\n",
    "        #filtered_sentence = [re.sub(r\"[^a-zA-Z-]+\", '', word) for word in filtered_sentence]\n",
    "        filtered_sentence = [word.strip() for word in filtered_sentence]\n",
    "\n",
    "        wordnet_lemmatizer = WordNetLemmatizer() \n",
    "            \n",
    "        filtered_sentence = [wordnet_lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in filtered_sentence if  word != \" \" and word != '']\n",
    "        \n",
    "        final_sent[key] = filtered_sentence\n",
    "        all_words = all_words + filtered_sentence\n",
    "    word_frequency = list(Counter(all_words).items())\n",
    "    return word_frequency, final_sent , all_words\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    #detection of appreviation\n",
    "    sent_dict = replace_acronymss(keyvalue)\n",
    "\n",
    "    tokenize_sent_dict = tokenize_sent(sent_dict)\n",
    "    \n",
    "    new_sent_dict = conjunctions(tokenize_sent_dict)\n",
    "    \n",
    "    #e Replacement of word synonyms\n",
    "    sent_dict = replace_word_synonym(new_sent_dict)\n",
    "    sent_list =[]\n",
    "    for item in sent_dict.items():\n",
    "        sent_list.append(item)\n",
    "\n",
    "    all_bigrams = []\n",
    "    for key, value in sent_dict.items():\n",
    "        bigrams = ngramise(value)\n",
    "        all_bigrams = all_bigrams+bigrams\n",
    "\n",
    "    bi_count = dict(Counter(all_bigrams))\n",
    "    delete = [key for key,value in bi_count.items() if value < 2]\n",
    "    # delete the key \n",
    "    for key in delete: del bi_count[key]\n",
    "    bi_count_list =[]\n",
    "    for key , value in bi_count.items():\n",
    "        bi_count_list.append([(key),value])\n",
    "        \n",
    "    \n",
    "    word_freq ,final_sent, all_words = EF(sent_dict)\n",
    "    Sentences_list = []\n",
    "    for item in final_sent.items():\n",
    "        Sentences_list.append(item)     \n",
    "    \n",
    "    #word frequencies\n",
    "    #freq of words in title\n",
    "    title_freq = Counter(title)\n",
    "\n",
    "    Keyword_freq = Counter(keywords)\n",
    "    #Keyword_freq = keywords\n",
    "    #print(Keyword_freq)\n",
    "    w_freq = Counter(all_words)\n",
    "    final_Word_freq = dict()\n",
    "    for word in all_words:\n",
    "        if word not in title_freq.keys():\n",
    "            title_freq[word] = 0\n",
    "        if word not in Keyword_freq.keys():\n",
    "            Keyword_freq[word] = 0\n",
    "        final_Word_freq[word] = title_freq[word] + Keyword_freq[word] + w_freq[word] \n",
    "\n",
    "    ##final list of sentences after preprocessing\n",
    "    final_input = []\n",
    "    for (index, sentence) in Sentences_list:\n",
    "        final_input.append(\" \".join(sentence))\n",
    "        \n",
    "    keyword_word_freq = []\n",
    "    for word in all_words:\n",
    "        keyword_word_freq.append([word,Keyword_freq[word]])  \n",
    "    #print(keyword_word_freq)\n",
    "    title_word_freq = []\n",
    "    for word in all_words:\n",
    "        title_word_freq.append([word, title_freq[word]])\n",
    "\n",
    "    Sentences_list\n",
    "    List_of_sent = dict()\n",
    "    for (key, sent) in Sentences_list:\n",
    "        List_of_sent[key] = \" \".join(sent)\n",
    "\n",
    "    final_Word_freq\n",
    "    finalwordfreq = []\n",
    "    for key , value in final_Word_freq.items():\n",
    "        finalwordfreq.append([key,value])\n",
    "    #finalwordfreq\n",
    "    return (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "            ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq)\n",
    "    \n",
    "(title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    " ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess() \n",
    "\n",
    "def bigrams_freq(bi_count_list):\n",
    "    words_freq_bigram = dict()\n",
    "    bi_count = []\n",
    "    for list_bi in bi_count_list:\n",
    "        (w1,w2), i = list_bi\n",
    "        if w1 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "        else:\n",
    "            words_freq_bigram[w1] = i\n",
    "        if w2 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "        else:\n",
    "            words_freq_bigram[w2] = i   \n",
    "    for key,value in words_freq_bigram.items():\n",
    "        bi_count.append([key,value])\n",
    "    return bi_count\n",
    "bi_count = bigrams_freq(bi_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq_bigram = dict()\n",
    "for list_bi in bi_count_list:\n",
    "    (w1,w2), i = list_bi\n",
    "    if w1 in words_freq_bigram.keys():\n",
    "        words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "    else:\n",
    "        words_freq_bigram[w1] = i\n",
    "    if w2 in words_freq_bigram.keys():\n",
    "        words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "    else:\n",
    "        words_freq_bigram[w2] = i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights\n",
    "def get_weights(list_of_nodes):\n",
    "    import pandas as ps\n",
    "    import statistics as st\n",
    "    node_weight_dict = dict()\n",
    "    node_list= list_of_nodes\n",
    "    FWL= finalwordfreq\n",
    "    TWL= title_word_freq\n",
    "    KWL= keyword_word_freq\n",
    "    BWL= bi_count \n",
    "    PWL= Pnouns\n",
    "    #BWL = []\n",
    "    #PWL = []\n",
    "    \n",
    "\n",
    "    F=[]     #freq of words  which appeared in nodes WRT document\n",
    "    T=[]     #freq of words  which appeared in nodes WRT title\n",
    "    K=[]     #freq of words  which appeared in nodes WRT keyword list \n",
    "    B=[]     #freq of words which appeared in nodes WRT Bi_Gram list \n",
    "    P=[]     #list of detection the proper noune \n",
    "    FW=[]    \n",
    "    Xt,Xk,Xbi,Xp,Xb=1,1,1,1,1\n",
    "    weighted_node=[]\n",
    "    \n",
    "    \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "    \n",
    "        for j in FWL:\n",
    "            if(i==j[0]):\n",
    "                f=j[1]\n",
    "                F.append(f)\n",
    "                flag=1\n",
    "                \n",
    "        if(flag!=1): \n",
    "            F.append(0) \n",
    "        \n",
    " \n",
    "\n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in TWL:\n",
    "            if(i==j[0]):\n",
    "                t=j[1]\n",
    "                T.append(t)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "             T.append(0)\n",
    "                \n",
    "        \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in KWL:\n",
    "            if(i==j[0]):\n",
    "                k=j[1]\n",
    "                K.append(k)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            K.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in BWL:\n",
    "            if(i==j[0]):\n",
    "                b=j[1]\n",
    "                B.append(b)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            B.append(0)\n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in PWL:\n",
    "            if(i==j):\n",
    "                P.append(1)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            P.append(0)             \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in FWL:\n",
    "        fw=i[1]\n",
    "        FW.append(fw)\n",
    "    \n",
    "    if FW == []:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "    elif len(FW) != 0:\n",
    "        avrage_word_frequency=st.mean(FW)\n",
    "        median_word_frequency=st.median(FW) \n",
    "    else:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "        \n",
    "    AW=abs(avrage_word_frequency-median_word_frequency)\n",
    "    \n",
    "    for i in range(len(node_list)):\n",
    "        WN=F[i]+Xt*AW*T[i]+Xk*AW*K[i]+Xb*AW*B[i]+Xp*AW*P[i]\n",
    "        weighted_node.append(WN)\n",
    "        node_weight_dict[node_list[i]] = WN\n",
    "    #print(K)\n",
    "    #print (F,T,K,FW,avrage_word_frequency,median_word_frequency)    \n",
    "    #print(weighted_node) \n",
    "    #print(node_weight_dict)\n",
    "    return node_weight_dict,weighted_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_nodes_list():\n",
    "    #return All_nodes,weighted_node\n",
    "import json\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "#edge between source and distenation\n",
    "def get_edge(source,distenation):\n",
    "    return edgeget[source,distenation]\n",
    "\n",
    "\n",
    "#distination node list\n",
    "def getweights_of_dist_nodes(source_destinations,sourcenode,All_nodes_ww):  \n",
    "    x=source_destinations[sourcenode]\n",
    "    distenation_node_list={}\n",
    "    weights=[]\n",
    "    nothing=0\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i])==('E#')) or ((x[i]) not in json.dumps(All_nodes_ww)) or ( x[i] not in All_nodes_ww):\n",
    "          nothing=1\n",
    "        else:\n",
    "          distenation_node_list[x[i]]=All_nodes_ww[x[i]]\n",
    "          weights.append((All_nodes_ww[x[i]]))\n",
    "        if len(weights) != 0:\n",
    "            nothing=0\n",
    "    return distenation_node_list,weights,nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm 2 get candidate edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for candidate edges between a source and a destination given a criteria \n",
    "def get_candidate_edges(source_destinations,node_listx,wxx,out_carteria,All_weight,input_nodes,All_SD):\n",
    "    c=[]\n",
    "    xxx = []\n",
    "    no_list,totalweights=node_listx,All_weight\n",
    "\n",
    "    source_node_weight=0\n",
    "    distination_node_weight=0\n",
    "    average_of_the_node_weight=np.average(totalweights)#(np.sum(node_list)/number_of_nodes)\n",
    "    median_of_the_node_list_weight=np.median(totalweights)\n",
    "  \n",
    "    if average_of_the_node_weight>median_of_the_node_list_weight:#can be replaced with source=max(avg,median)\n",
    "        source_node_weight=average_of_the_node_weight\n",
    "    else:\n",
    "        source_node_weight=median_of_the_node_list_weight\n",
    "\n",
    "    for node,weight in no_list.items():\n",
    "        \n",
    "        if weight>=source_node_weight:#بيعرف اذا كانت بدايه الجمله ولا لا \n",
    "            outnodes1,outnodesweights,flagnothing=getweights_of_dist_nodes(source_destinations,node,no_list)#فنكشن جوا الجي بتجبلي كل الديستناشن بتاعت النودواختار بينهم هسقط مين\n",
    "            outnodes2,nodes2weights,flag2nothing=getweights_of_dist_nodes(All_SD,node,input_nodes)\n",
    "            outnodes1=[(i,j) for (i,j) in outnodes1.items() if j!=0 ] \n",
    "            outnodes2 =[(i,j) for (i,j) in outnodes2.items() if j!=0 ] \n",
    "        \n",
    "            if flagnothing==0:\n",
    "                #destination nodes exsist\n",
    "                #get their average and max weights for avg and max criterias\n",
    "                average_weight=np.average(nodes2weights)\n",
    "                max_weight=max(nodes2weights)\n",
    "            else:\n",
    "                #no destination nodes \n",
    "                average_weight=0\n",
    "                max_weight=0\n",
    "            # get threshold of destination weights according to criteria \n",
    "            if (out_carteria ==\"avg\"):\n",
    "                distination_node_weight=average_weight\n",
    "            elif (out_carteria ==\"node_avg\"):\n",
    "                distination_node_weight=source_node_weight\n",
    "            elif (out_carteria==\"max\"):\n",
    "                distination_node_weight=max_weight\n",
    "            else:\n",
    "                distination_node_weight=average_weight\n",
    "\n",
    "\n",
    "            if (bool(outnodes1)==False):\n",
    "                # no destination nodes \n",
    "                nothing=1  \n",
    "            else: \n",
    "                # check for the candidate edges\n",
    "                for out,weight_of_out_node in outnodes1:\n",
    "                    if weight_of_out_node>=distination_node_weight and flagnothing==0:\n",
    "                        c.append(get_edge(node,out))# append the edge\n",
    "                        xxx.append([node,out])\n",
    "\n",
    "    cx=[]\n",
    "    for i in range(0,len(c)):\n",
    "        cx.append(c[i][0])\n",
    "    return cx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_order(count_summ, index):\n",
    "    return(count_summ - index)/count_summ\n",
    "\n",
    "def compute_rank(candidate_summary):#, W, core_words_list, bigrams_list, lsa_weight):\n",
    "    ranks = {}\n",
    "    candidate_summary = reorder(candidate_summary)\n",
    "    \n",
    "    for i, sent in enumerate(candidate_summary):\n",
    "        \n",
    "        rank = sentence_order(len(candidate_summary), i)\n",
    "        ranks[sent] = rank\n",
    "        #print(\"index\",i,\"sentence\",sent,\"rank\",rank)\n",
    "    sort_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)    \n",
    "    return ranks   \n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "def get_words_count(sent):\n",
    "    len_sent = 0\n",
    "    for i in sent.split(\" \"):\n",
    "        len_sent = len_sent +1\n",
    "    return len_sent\n",
    "\n",
    "# reorder the summary based on the order from the input text\n",
    "def reorder(summary):\n",
    "    index_summ = []\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    order = dict()\n",
    "    final_order = []\n",
    "    for sent in summary:\n",
    "        if sent in value_sent:\n",
    "            index_sent = value_sent.index(sent)\n",
    "            index_summ.append(index_sent)\n",
    "            order[index_sent] = sent\n",
    "            \n",
    "    for i in sorted(order):\n",
    "        final_order.append(order[i])\n",
    "    #print(sorted(index_summ))\n",
    "    return final_order\n",
    "\n",
    "def concat(final_order):\n",
    "    return(\"\\n\".join(final_order))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_cluster(x, model=None):\n",
    "    if model == None:\n",
    "        model = KMeans()\n",
    "    model = sorted_cluster_centers_(model, x)\n",
    "    model = sorted_labels_(model, x)\n",
    "    return model\n",
    "\n",
    "def sorted_cluster_centers_(model, x):\n",
    "    model.fit(x)\n",
    "    new_centroids = []\n",
    "    magnitude = []\n",
    "    for center in model.cluster_centers_:\n",
    "        magnitude.append(np.sqrt(center.dot(center)))\n",
    "    idx_argsort = np.argsort(magnitude)\n",
    "    model.cluster_centers_ = model.cluster_centers_[idx_argsort]\n",
    "    return model\n",
    "\n",
    "def sorted_labels_(sorted_model, x):\n",
    "    sorted_model.labels_ = sorted_model.predict(x)\n",
    "    return sorted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post processing and algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(list_of_candidates_summeries):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    #print('candidate summ    ',candidate_summary)\n",
    "    sentences_ranks = compute_rank(candidate_summary)\n",
    "   \n",
    "    sentences_ordered =  [key for key,value  in sentences_ranks.items()]\n",
    "    summary_length =0\n",
    "    visited_sentences = []\n",
    "    sentences = sentences_ordered\n",
    "\n",
    "    tfidfvect = TfidfVectorizer(stop_words='english')\n",
    "    X = tfidfvect.fit_transform(sentences)\n",
    "      \n",
    "    if len(sentences) <1:\n",
    "        n_clusters = len(sentences)\n",
    "    else:\n",
    "        n_clusters = 1\n",
    "    clf = KMeans(n_clusters = n_clusters,init = 'k-means++', max_iter = 1000)\n",
    "    labels = clf.fit_predict(X)\n",
    "    clustered_sentences =[[] for i in range(5)]\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['data_index'] = sentences\n",
    "    cluster_map['cluster'] = clf.labels_\n",
    "    #print(cluster_map)\n",
    "    return n_clusters,cluster_map\n",
    "n_clusters,cluster_map = post_processing(list_of_candidates_summeries)\n",
    "\n",
    "def algorithm_4(summary_length,threshold,max_words_limit):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    summary_length = 0\n",
    "    while (len(visited_sentences) < len(candidate_summary)) and ((summary_length)< max_words_limit) :\n",
    "        for i in range(n_clusters):\n",
    "            clustered_sentences = cluster_map[cluster_map.cluster == i]['data_index'].tolist()\n",
    "            \n",
    "            if summary_length >= (max_words_limit+threshold):\n",
    "                break\n",
    "            for sent in clustered_sentences:\n",
    "                if sent not in visited_sentences :\n",
    "                    temp_length = summary_length + get_words_count(sent)\n",
    "                    if temp_length < max_words_limit :\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif(temp_length >= max_words_limit) and (temp_length <= (max_words_limit + threshold)):\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif temp_length > (max_words_limit + threshold):\n",
    "                        visited_sentences.append(sent)\n",
    "                        \n",
    "    return final_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to calculate Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(outsumm,Summary):\n",
    "    score={}\n",
    "\n",
    "    #generated_summary=\" In a dramatic finish, the San Francisco 49ers football team won the Super Bowl on Sunday with a 20–16 victory over tCincinnati Bengals. Joe Montana's 10-yard touchdown pass to John Taylor with 34 seconds left provided the winningmargin. The victory was achieved through the brilliance of Montana and Jerry Rice, the wide receiver who caught 12passes for 222 yards, both Super Bowl records. It also gave 49ers Coach Bill Walsh his third Super Bowl win. The firsttouchdown didn't come until 44 minutes into the 60-minute game, when Cincinnati's Stanford Jennings took a kickoff 93yards for a score\"\n",
    "    generated_summary = outsumm\n",
    "    #refrence1=\"The San Francisco 49ers won the Super Bowl Sunday with a dramatic 20–16 victory over the Cincinnati Bengals. JoeMontana's 10-yard touchdown pass to John Taylor with 34 seconds remaining provided the win. The pass gave Montana aSuper Bowl record with 357 yards passing. The victory was achieved through the brilliance of Montana and Jerry Rice, thewide receiver who caught 12 passes for 222 yards, both Super Bowl records. Rice was named the game's most valuableplayer. It was the fifth straight win for a National Football Conference team and the third Super Bowl win for Coach Bill\"\n",
    "    #refrence2=\"Joe Montana's 10-yard touchdown pass to John Taylor with 34 seconds left in the game gave the San Francisco 49ers a20–16 Super Bowl victory over the Cincinnati Bengals on Sunday. The touchdown gave Montana a Super Bowl record with357 yards passing. Wide receiver Jerry Rice caught 12 passes for 222 yards, both Super Bowl records, and was named thegame's most valuable player. It was the third Super Bowl win for 49ers Coach Bill Walsh. It was the fifth straight win for theNFC team and the most dramatic. The previous four had an average score of 41–1\"\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    s_R1 =[]\n",
    "    s_R2 = []\n",
    "    s_RL = []\n",
    "    for summ in Summary:\n",
    "        score = scorer.score('\\n'.join(summ) ,generated_summary)\n",
    "        s_R1.append(score['rouge1'][1])\n",
    "        s_R2.append(score['rouge2'][1])\n",
    "        s_RL.append(score['rougeLsum'][1])\n",
    "\n",
    "    scores = [max(s_R1), max(s_R2) , max(s_RL)]\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a vertex to the dictionary\n",
    "def add_vertex(v):\n",
    "  global graph\n",
    "  global vertices_no\n",
    "  if v not in graph:\n",
    "    vertices_no = vertices_no + 1\n",
    "    #print(\"vertices_no \",vertices_no, v)\n",
    "    graph[v] = []\n",
    "\n",
    "# Add an edge between vertex v1 and v2 with edge weight e\n",
    "#v1 is the source node and v2 is the distenation node\n",
    "edgeget={}\n",
    "def add_edge(v1, v2, e_order, e_value):\n",
    "  global graph\n",
    "  # Check if vertex v1 is a valid vertex\n",
    "  if ((v1 in graph) and (v2 in graph)):\n",
    "    #print(v1,v2,e_order,e_value)  \n",
    "    temp = [v2, e_order, e_value]\n",
    "    edgeget[v1,v2]=[e_value]\n",
    "    graph[v1].append(temp)\n",
    "\n",
    "# Print the graph\n",
    "def print_graph():\n",
    "  global graph\n",
    "  for vertex in graph:\n",
    "    for edges in graph[vertex]:\n",
    "      #print(\"evet\")\n",
    "      print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "# get the destinations\n",
    "def get_destinations(source):\n",
    "    global graph\n",
    "    distinations=[]\n",
    "    if(source in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0] not in distinations):\n",
    "                distinations.append(elements[0])\n",
    "    return distinations\n",
    "\n",
    "# get the destinations\n",
    "def get_edges(source,dist):\n",
    "    global graph\n",
    "    Edge=\"\"\n",
    "    if(source in graph and dist in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0]==dist):\n",
    "                Edge= Edge + elements[2]\n",
    "    return Edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria):\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    input_text = candidate_summ\n",
    "    last_summ_len = candidate_summ_len\n",
    "    Last_summ = candidate_summ\n",
    "    sent_idx = can_edges\n",
    "    can_edges=[]\n",
    "    List_of_sent = list_pre_cansumm\n",
    "    list_pre_cansumm = []\n",
    "    list_of_candidates_summeries = []\n",
    "    All_nodes=[]\n",
    "    nodes_list = []\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    #print(S.values())\n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "    #    print(sentence)\n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "    #    print(Words)\n",
    "        Types=pos_tag(Words)\n",
    "    #    print(Types)\n",
    "        for Type in Types:\n",
    "            if (Type[1]=='NN' or Type[1]=='NNP' or Type[1]=='NNS' or Type[1]=='NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "            #            print(Type[0])\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    "        #print(\"nodes_edges: \",nodes_list)\n",
    "        #print(\"edges_nodes: \",edges_list) \n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        LIST_OF_NODES=[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "        #print(\"final node= \",final_node)\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        #list_of_nodes.append(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            ##print(\"evet \", nodes_list[counter],counter,nodes_list[37])\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "            #print(\"counter \",counter)\n",
    "        #print(\"order= \",order) \n",
    "        #add_edge(final_node, \"E#\", order, \"\")\n",
    "        \n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "        #for s in LIST_OF_NODES:\n",
    "            #if(s not in source_destinations.keys()):\n",
    "                #source_destinations[s]=get_destinations(s)\n",
    "            #else:\n",
    "                #source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "            \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "        #print (\"graph: \", graph)\n",
    "        #\n",
    "        \n",
    "        #****************************************************************************\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[index] = (source_destinations,list_of_nodes)\n",
    "    \n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        index = index + 1\n",
    "        \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        \n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[index] and (List_of_sent[index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "        index = index + 1\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    candidate_summ_len = 0\n",
    "    for index ,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "    return list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to get index of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sent):\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    index_sent = value_sent.index(sent)\n",
    "    return index_sent    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final function\n",
    "## iterates over all data in DUC2002 and gets Rouge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5220808625952076 0.27511553829391766 0.48183647921283745\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#path = \"DucDataset/DUC2001_Summarization_Documents/data/testtraining/duc2002testtraining\"\n",
    "#files = os.listdir(path)\n",
    "final_scores =[]\n",
    "final_scores_1=[]\n",
    "final_scores_2=[]\n",
    "final_scores_L=[]\n",
    "final_scores_1F=[]\n",
    "final_scores_2F=[]\n",
    "final_scores_LF=[]\n",
    "final_scores_11 = []\n",
    "final_scores_21 =[]\n",
    "final_scores_L1 = []\n",
    "\n",
    "final_scores_12 = []\n",
    "final_scores_22 = []\n",
    "final_scores_L2 = []\n",
    "\n",
    "\n",
    "\n",
    "for i in Text_in:\n",
    "    can_edges = []\n",
    "    \n",
    "    title,keywords,keyvalue, key_value_list,Summary, lang_Dict = read_input(Text_in[i], i) \n",
    "    out_carteria=\"avg\"\n",
    "    \n",
    "    (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "         ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess()\n",
    "    #print(finalwordfreq)\n",
    "    #print(keywords)\n",
    "    #print(len(Sentences_list)-1)\n",
    "    Last_summ = keyvalue.values()\n",
    "    last_summ_len = 0\n",
    "    for value in keyvalue.values():\n",
    "        last_summ_len = last_summ_len + get_words_count(value)\n",
    "    \n",
    "    \n",
    "    All_nodes=[]\n",
    "    list_of_candidates_summeries=[]\n",
    "    list_pre_cansumm = []\n",
    "    ##asume\n",
    "    #S={0:\"Embedded software development, that is, the development of embedded software, such as used for controlling consumer products, requires the development process to be integrated with the development of the controlled physical product\"}\n",
    "\n",
    "    ##tokanization  \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    ##lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    # Add a vertex to the dictionary\n",
    "    def add_vertex(v):\n",
    "      global graph\n",
    "      global vertices_no\n",
    "      if v not in graph:\n",
    "        vertices_no = vertices_no + 1\n",
    "        #print(\"vertices_no \",vertices_no, v)\n",
    "        graph[v] = []\n",
    "\n",
    "    # Add an edge between vertex v1 and v2 with edge weight e\n",
    "    #v1 is the source node and v2 is the distenation node\n",
    "    edgeget={}\n",
    "    def add_edge(v1, v2, e_order, e_value):\n",
    "      global graph\n",
    "      # Check if vertex v1 is a valid vertex\n",
    "      if ((v1 in graph) and (v2 in graph)):\n",
    "        #print(v1,v2,e_order,e_value)  \n",
    "        temp = [v2, e_order, e_value]\n",
    "        edgeget[v1,v2]=[e_value]\n",
    "        graph[v1].append(temp)\n",
    "\n",
    "    # Print the graph\n",
    "    def print_graph():\n",
    "      global graph\n",
    "      for vertex in graph:\n",
    "        for edges in graph[vertex]:\n",
    "          #print(\"evet\")\n",
    "          print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "    # get the destinations\n",
    "    def get_destinations(source):\n",
    "        global graph\n",
    "        distinations=[]\n",
    "        if(source in graph):\n",
    "            for elements in graph.get(source):\n",
    "                if(elements[0] not in distinations):\n",
    "                    distinations.append(elements[0])\n",
    "        return distinations\n",
    "\n",
    "    # get the destinations\n",
    "    def get_edges(source,dist):\n",
    "        global graph\n",
    "        Edge=\"\"\n",
    "        if(source in graph and dist in graph):\n",
    "            for elements in graph.get(source):\n",
    "                if(elements[0]==dist):\n",
    "                    Edge= Edge + elements[2]\n",
    "        return Edge\n",
    "\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    #print(S.values())\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}\n",
    "    Pnouns = []\n",
    "    for sent_index, sentence in enumerate(List_of_sent.values()):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        \n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "        \n",
    "        Types=pos_tag(Words)\n",
    "    #    print(Types)\n",
    "        for Type in Types:\n",
    "            if (Type[1]=='NN' or Type[1]=='NNP' or Type[1]=='NNS' or Type[1]=='NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "            #            print(Type[0])\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "                if(Type[1] == 'NNP'):\n",
    "                    Pnouns.append(\"\")\n",
    "            \n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    "        #print(\"nodes_edges: \",nodes_list)\n",
    "        #print(\"edges_nodes: \",edges_list) \n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "        #print(\"final node= \",final_node)\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        #list_of_nodes.append(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            ##print(\"evet \", nodes_list[counter],counter,nodes_list[37])\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "            #print(\"counter \",counter)\n",
    "        #print(\"order= \",order) \n",
    "        #add_edge(final_node, \"E#\", order, \"\")\n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "        #for s in LIST_OF_NODES:\n",
    "            #if(s not in source_destinations.keys()):\n",
    "                #source_destinations[s]=get_destinations(s)\n",
    "            #else:\n",
    "                #source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "        \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "        #print (\"graph: \", graph)\n",
    "        #\n",
    "        #print_graph()\n",
    "        #****************************************************************************\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[sent_index] = (source_destinations,list_of_nodes)\n",
    "\n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        #print(\"list_of_nodes \",list_of_nodes)\n",
    "        #print(\"sentence =\",sentence)\n",
    "        #All_nodes = All_nodes + list_of_nodes\n",
    "        #****************************************************************************\n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    for sent_index, sentence in enumerate(S.values()): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[sent_index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        \n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[sent_index] and (List_of_sent[sent_index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[sent_index])\n",
    "                can_edges.append(sent_index)\n",
    "                \n",
    "\n",
    "    max_words_limit= 100\n",
    "    threshold = 15\n",
    "    final_summary = []\n",
    "    visited_sentences = []\n",
    "    summary_length = 0\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    \n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "    \n",
    "\n",
    "    while(True):\n",
    "        if(candidate_summ_len > (max_words_limit + threshold)):\n",
    "            if(candidate_summ_len != last_summ_len):\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria) \n",
    "\n",
    "            elif(out_carteria != \"max\"):\n",
    "                out_carteria=\"max\"\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria)\n",
    "            else:\n",
    "                list_of_candidates_summeries = candidate_summ\n",
    "                break\n",
    "        elif(candidate_summ_len < (max_words_limit)):\n",
    "            candidate_summ = Last_summ\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "        else:\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "            \n",
    "    n_clusters,cluster_map = post_processing(list_of_candidates_summeries)\n",
    "    final_summary = algorithm_4(summary_length,threshold,max_words_limit)\n",
    "    final = reorder(final_summary)\n",
    "    outsumm = concat(final)\n",
    "    scores = get_score(outsumm,Summary)\n",
    "\n",
    "\n",
    "    ##this avrage with respect to Recall\n",
    "\n",
    "    final_scores_1.append(scores[0])\n",
    "    final_scores_2.append(scores[1])\n",
    "    final_scores_L.append(scores[2])\n",
    "    #print(scores['rouge(1)'][0],\"   \",scores['rouge(2)'][0], \"  \",scores['rouge(L)'][0])\n",
    "\n",
    "avrage_1=st.mean(final_scores_1)\n",
    "avrage_2=st.mean(final_scores_2)\n",
    "avrage_L=st.mean(final_scores_L)\n",
    "print(avrage_1,avrage_2,avrage_L)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
