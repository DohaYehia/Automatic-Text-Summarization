{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install scispacy\n",
    "#!pip install rouge_score\n",
    "#!pip install gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install nltk\n",
    "#!pip install abbreviations\n",
    "#!pip install text-preprocessing\n",
    "#!pip install glove-python-binary\n",
    "#!pip install statistics\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download ('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import os\n",
    "from text_preprocessing import preprocess_text\n",
    "import pyarabic.araby as araby\n",
    "!pip install abbreviations\n",
    "from abbreviations import schwartz_hearst\n",
    "rom sklearn.feature_extraction.text import TfidfVectorizer\n",
    "!pip install translation\n",
    "from text_preprocessing import to_lower, remove_email, remove_url, lemmatize_word,expand_contraction\n",
    "from text_preprocessing import remove_punctuation as punct\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "## importing libraries\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#post-Processing\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster import hierarchy\n",
    "#from sklearn.cluster import AgglomerativeClustring\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scispacy.abbreviation.AbbreviationDetector at 0x196936aa8e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "def create_lang_detector(nlp, name):\n",
    "    return abbreviation_pipe\n",
    "Language.factory(\"abbreviation_pipe\", func=create_lang_detector)\n",
    "nlp.add_pipe(\"abbreviation_pipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading EASC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"arabic_datasets/EASC/EASC-UTF-8/Articles\"\n",
    "summ_path = \"arabic_datasets/EASC/EASC-UTF-8/MTurk\"\n",
    "files = os.listdir(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(doc_path)\n",
    "Data_summ = {}\n",
    "Data = []\n",
    "Summary = []\n",
    "for file in files:\n",
    "    check = doc_path + \"/\" + file \n",
    "    check_files = os.listdir(check)\n",
    "    input_path = check + \"/\" + check_files[0]\n",
    "    summary_path = summ_path + \"/\" + file\n",
    "    summ_files = os.listdir(summary_path)\n",
    "    five_summ = []\n",
    "    for summ_file in summ_files:\n",
    "        summ_path_data = summary_path + \"/\" + summ_file \n",
    "        with open(summ_path_data,encoding='utf8') as fp:\n",
    "            summary = fp.read()\n",
    "            five_summ.append([summary])\n",
    "        \n",
    "    with open(input_path,encoding='utf8') as fp:\n",
    "        data = fp.read()\n",
    "        fp.closed\n",
    "    \n",
    "        fp.closed\n",
    "    Data_summ[data] = five_summ\n",
    "    Data.append([data])\n",
    "    Summary.append(five_summ)\n",
    "    \n",
    "#print(Data_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Data and Summary using TextBlob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran(i):\n",
    "    data = Data[i]\n",
    "    text = data[0]\n",
    "\n",
    "    en_blob = TextBlob(text) \n",
    "    translate_text = en_blob.translate(to='en')\n",
    "    \n",
    "    summ = Summary[i]\n",
    "    time.sleep(2)\n",
    "    tr_summary = []\n",
    "    for s in summ:\n",
    "        en_blob = TextBlob(str(s)) \n",
    "        translate_summ = en_blob.translate(to='en')\n",
    "        tr_summary.append([translate_summ])\n",
    "        time.sleep(2)\n",
    "\n",
    "    return translate_text, tr_summary\n",
    "#data, summ =tran(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "tr_Data = []\n",
    "tr_Summ = []\n",
    "for i in range(0,len(Data)):\n",
    "    data, summ =tran(i)\n",
    "    tr_Data.append(data)\n",
    "    tr_Summ.append(summ)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(data,summ):\n",
    "    text = sent_tokenize(str(data))\n",
    "    Summary = summ\n",
    "    \n",
    "    title = []\n",
    "    keywords =[]\n",
    "        \n",
    "    lang_Dict = []\n",
    "    for line in text:\n",
    "        for word in line.split(\" \"):\n",
    "            if word not in lang_Dict:\n",
    "                lang_Dict.append(word)\n",
    "    iter_dict = lang_Dict\n",
    "    lang_Dict = []\n",
    "    for i in iter_dict:\n",
    "        if '\\n' in i:\n",
    "            lang_Dict = lang_Dict + i.split(\"\\n\")\n",
    "        else:\n",
    "            lang_Dict.append(i)\n",
    "     \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    \n",
    "    lang_dict = []\n",
    "    for word in lang_Dict:\n",
    "        if word.strip():\n",
    "            word = word.lower()\n",
    "            word = expand_contraction(word)\n",
    "            word = re.sub(pattern, '', word)\n",
    "            lang_dict.append(word)\n",
    "            \n",
    "    lang_Dict = []\n",
    "    for word in lang_dict:\n",
    "        if \" \" in word:\n",
    "            for w in word.split(\" \"):\n",
    "                if w.strip():\n",
    "                    lang_Dict.append(w)\n",
    "        else:\n",
    "            lang_Dict.append(word)\n",
    "    keyvalue = dict() \n",
    "\n",
    "    index= 0\n",
    "    for line in text:\n",
    "        if(line and (line.strip()) ):\n",
    "            i = index\n",
    "            keyvalue[i] = line\n",
    "            index = index + 1\n",
    "\n",
    "    key_value_list =[]\n",
    "    for item in keyvalue.items():\n",
    "        key_value_list.append(item)\n",
    "    \n",
    "    return title,keywords,keyvalue, key_value_list,Summary, lang_Dict\n",
    "\n",
    "data= tr_Data[0]\n",
    "summ = tr_Summ[0]\n",
    "title,keywords,keyvalue, key_value_list,Summary, lang_Dict = read_input(data, summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronymss(keyvalue):\n",
    "        sent_dict = dict() \n",
    "        whole_text = \"\"\n",
    "        for key, text in keyvalue.items():\n",
    "            whole_text = whole_text + \"#*# \" + text\n",
    "        whole_text = whole_text.lower()\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=whole_text, most_common_definition=True)\n",
    "        for key,value in pairs.items():\n",
    "            whole_text = whole_text.replace(value, key)\n",
    "        output_text = whole_text.split(\"#*# \")\n",
    "        i = 0;\n",
    "        for l in output_text:\n",
    "            if l != \"\":\n",
    "                sentence = Brackets(l)\n",
    "                sent_dict[i] = sentence\n",
    "                i = i+1\n",
    "        return sent_dict\n",
    "\n",
    "#remove hyphens and tokenizing sentence\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = expand_contraction(word)\n",
    "        if word.lower() == \"n't\":\n",
    "            word = \"not\"\n",
    "        if word != '':\n",
    "            word = word.lower()\n",
    "            #word = expand_contraction(word)\n",
    "            new_words.append(word)\n",
    "    #print(words)\n",
    "    #print(\"after remove punck\")\n",
    "    return new_words\n",
    "\n",
    "def tokenize_sent(sent_dict):\n",
    "    keyvalueToken = dict() \n",
    "    TorF = True\n",
    "    for key, value in sent_dict.items():\n",
    "        \n",
    "        words = nltk.word_tokenize(expand_contraction(value))\n",
    "        words = [re.sub(r'(\\n-)','',word) for word in words]\n",
    "        words = [re.sub(r'(\\n)',' ',word) for word in words]\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                 new_words = new_words + word.split(\" \")\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        \n",
    "        keyvalueToken[key] = new_words\n",
    "        \n",
    "    return keyvalueToken\n",
    "#continue sent reduction\n",
    "def conjunctions(tokenize_sent_dict):\n",
    "    new_sent_dict = dict()\n",
    "    conjunctive_adverbs= [\"accordingly\",\"comparatively\",\"contrarily\", \n",
    "                      \"also\",\" further\", \"nevertheless\", \"indeed\",\" otherwise\",\n",
    "                      \"result\",\"as\",\"equally\" ,\"conversely\" ,\"besides\",\n",
    "                      \"furthermore\" ,\"nonetheless\" ,\"fact\" ,\"consequently\",\n",
    "                     \" likewise\" ,\"however\" ,\"addition\" \",moreover\" ,\"surprisingly\",\n",
    "                     \" hence\" ,\"similarly \",\"comparison\" ,\"still \",\"therefore \",\n",
    "                      \"contrast\" ,\"thus\" ,\"instead\"  ,\"rather\"\n",
    "                      , \"other\", \"additionally\", \"addition\",'although','already',\"including\",'according',\n",
    "                    'totally','likely','unlikely','perhaps',\"s\",'naturally',\n",
    "                      \"finally\", \"based\",\"on\",\"that\" , \"besides\",\"namely\",\"anyway\",\n",
    "                     \"then\",\"next\",\"thereafter\",\"certainly\",\"now\",\"finally\",\"meanwhile\",\n",
    "                     \"subsequently\",\"yet\",\"elsewhere\",\"thereafter\",\"undoubtedly\",\n",
    "                     \"incidentally\",\"otherwise\",\"regardless\",\"begin\",\"in\"]\n",
    "    \n",
    "    for key, text in tokenize_sent_dict.items():\n",
    "        new_sent = [word for word in text if word.lower() not in conjunctive_adverbs]\n",
    "        new_sent_dict[key] = new_sent\n",
    "\n",
    "    return new_sent_dict\n",
    "\n",
    "#d Sentence reduction \n",
    "def Brackets(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def get_key(val,my_dict): \n",
    "    for key, values in my_dict.items(): \n",
    "        for value in values:\n",
    "            if val == value: \n",
    "                return key \n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "def replace_word_synonym(new_sent_dict):\n",
    "    \n",
    "    text = []\n",
    "    sent_dict = dict()\n",
    "    dict_synonym = Synonym_replace(new_sent_dict)\n",
    "    for key, value in new_sent_dict.items():\n",
    "        \n",
    "        text = []\n",
    "        for word in value:\n",
    "            if get_key(word,dict_synonym) != \"key doesn't exist\":\n",
    "                text.append(get_key(word,dict_synonym))\n",
    "            else:\n",
    "                text.append(word)\n",
    "        sent_dict[key] = text\n",
    "    \n",
    "    return sent_dict\n",
    "\n",
    "def Synonym_replace(new_sent_dict):\n",
    "    synonymList = []\n",
    "    text = {}\n",
    "    for key, sentence in new_sent_dict.items():\n",
    "        for word in sentence:\n",
    "            if word not in synonymList:\n",
    "                wordNetSynset =  wn.synsets(word)\n",
    "                values = []\n",
    "                for synSet in wordNetSynset:\n",
    "                    for synWords in synSet.lemmas():\n",
    "                        if synWords.name() in synonymList:\n",
    "                            continue\n",
    "                        else:\n",
    "                            synonymList.append(synWords.name())\n",
    "                            values.append(synWords.name())\n",
    "                text[word] = values\n",
    "    return text\n",
    "#f Use words N-grams\n",
    "def ngramise(sequence):\n",
    "    bigrams = []\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def remove_hyfen(wordslist):\n",
    "    words_new = []\n",
    "    for word in wordslist:\n",
    "        digit = False\n",
    "        lang = False\n",
    "        if \"-\" in word:\n",
    "            words = word.split(\"-\")\n",
    "            for w in words:\n",
    "                if w.isdigit():\n",
    "                    digit = True\n",
    "            if digit:\n",
    "                newwords = [w for w in words]\n",
    "                words_new = words_new + newwords\n",
    "            else:\n",
    "                for w in words:\n",
    "                    if w in lang_Dict:\n",
    "                        lang = True\n",
    "                if lang:\n",
    "                    newwords = [w for w in words]\n",
    "                    words_new = words_new + newwords\n",
    "                else:\n",
    "                    newwords = [word]\n",
    "                    words_new = words_new + newwords\n",
    "        else:\n",
    "            newwords = [word]\n",
    "            words_new = words_new + newwords\n",
    "    return words_new\n",
    "#4.3. Word frequency computation\n",
    "def EF(sent_dict):\n",
    "    final_sent = dict()\n",
    "    all_words = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #word_tokens = word_tokenize(text) \n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    \n",
    "    for key, value in sent_dict.items():\n",
    "        value = remove_hyfen(value)\n",
    "        value = remove_punctuation(value)\n",
    "        filtered_sentence = [w for w in value if w not in stopWords] \n",
    "        filtered_sentence = [re.sub(\"'\", '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub('\"', '', word.lower()) for word in filtered_sentence]\n",
    "        filtered_sentence = [re.sub(pattern, '', word) for word in filtered_sentence]\n",
    "        \n",
    "        filtered_sentence = [re.sub(r'[0-9]+', '', word) for word in filtered_sentence]\n",
    "        \n",
    "        filtered_sentence = [word for word in filtered_sentence if len(word.strip()) >1]\n",
    "        filtered_sentence = [word.strip() for word in filtered_sentence]\n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer() \n",
    "            \n",
    "        filtered_sentence = [wordnet_lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in filtered_sentence if  word != \" \" and word != '']\n",
    "        \n",
    "        final_sent[key] = filtered_sentence\n",
    "        all_words = all_words + filtered_sentence\n",
    "    word_frequency = list(Counter(all_words).items())\n",
    "    return word_frequency, final_sent , all_words\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(keyvalue):\n",
    "    \n",
    "    #detection of appreviation\n",
    "    sent_dict = replace_acronymss(keyvalue)\n",
    "    \n",
    "    tokenize_sent_dict = tokenize_sent(sent_dict)\n",
    "    \n",
    "    new_sent_dict = conjunctions(tokenize_sent_dict)\n",
    "    \n",
    "    #e Replacement of word synonyms\n",
    "    sent_dict = replace_word_synonym(new_sent_dict)\n",
    "    \n",
    "    sent_list =[]\n",
    "    for item in sent_dict.items():\n",
    "        sent_list.append(item)\n",
    "\n",
    "    all_bigrams = []\n",
    "    for key, value in sent_dict.items():\n",
    "        bigrams = ngramise(value)\n",
    "        all_bigrams = all_bigrams+bigrams\n",
    "\n",
    "    bi_count = dict(Counter(all_bigrams))\n",
    "    delete = [key for key,value in bi_count.items() if value < 2]\n",
    "    # delete the key \n",
    "    for key in delete: del bi_count[key]\n",
    "    bi_count_list =[]\n",
    "    for key , value in bi_count.items():\n",
    "        bi_count_list.append([(key),value])\n",
    "        \n",
    "    \n",
    "    word_freq ,final_sent, all_words = EF(sent_dict)\n",
    "    Sentences_list = []\n",
    "    for item in final_sent.items():\n",
    "        Sentences_list.append(item)     \n",
    "    \n",
    "    #word frequencies\n",
    "    #freq of words in title\n",
    "    title_freq = Counter(title)\n",
    "\n",
    "    Keyword_freq = Counter(keywords)\n",
    "  \n",
    "    w_freq = Counter(all_words)\n",
    "    final_Word_freq = dict()\n",
    "    for word in all_words:\n",
    "        if word not in title_freq.keys():\n",
    "            title_freq[word] = 0\n",
    "        if word not in Keyword_freq.keys():\n",
    "            Keyword_freq[word] = 0\n",
    "        final_Word_freq[word] = title_freq[word] + Keyword_freq[word] + w_freq[word] \n",
    "\n",
    "    ##final list of sentences after preprocessing\n",
    "    final_input = []\n",
    "    for (index, sentence) in Sentences_list:\n",
    "        final_input.append(\" \".join(sentence))\n",
    "        \n",
    "    keyword_word_freq = []\n",
    "    for word in all_words:\n",
    "        keyword_word_freq.append([word,Keyword_freq[word]])  \n",
    "   \n",
    "    title_word_freq = []\n",
    "    for word in all_words:\n",
    "        title_word_freq.append([word, title_freq[word]])\n",
    "\n",
    "    Sentences_list\n",
    "    List_of_sent = dict()\n",
    "    for (key, sent) in Sentences_list:\n",
    "        List_of_sent[key] = \" \".join(sent)\n",
    "\n",
    "    final_Word_freq\n",
    "    finalwordfreq = []\n",
    "    for key , value in final_Word_freq.items():\n",
    "        finalwordfreq.append([key,value])\n",
    "    #finalwordfreq\n",
    "    return (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "            ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq)\n",
    "    \n",
    "(title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    " ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess(keyvalue) \n",
    "\n",
    "def bigrams_freq(bi_count_list):\n",
    "    words_freq_bigram = dict()\n",
    "    bi_count = []\n",
    "    for list_bi in bi_count_list:\n",
    "        (w1,w2), i = list_bi\n",
    "        if w1 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w1] = words_freq_bigram[w1] + i\n",
    "        else:\n",
    "            words_freq_bigram[w1] = i\n",
    "        if w2 in words_freq_bigram.keys():\n",
    "            words_freq_bigram[w2] = words_freq_bigram[w2] + i\n",
    "        else:\n",
    "            words_freq_bigram[w2] = i   \n",
    "    for key,value in words_freq_bigram.items():\n",
    "        bi_count.append([key,value])\n",
    "    return bi_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights\n",
    "def get_weights(list_of_nodes):\n",
    "    import pandas as ps\n",
    "    import statistics as st\n",
    "    node_weight_dict = dict()\n",
    "    node_list= list_of_nodes\n",
    "    FWL= finalwordfreq\n",
    "    TWL= title_word_freq\n",
    "    KWL= keyword_word_freq\n",
    "    BWL= bi_count \n",
    "    PWL= Pnouns\n",
    "    \n",
    "\n",
    "    F=[]     #freq of words  which appeared in nodes WRT document\n",
    "    T=[]     #freq of words  which appeared in nodes WRT title\n",
    "    K=[]     #freq of words  which appeared in nodes WRT keyword list \n",
    "    B=[]     #freq of words which appeared in nodes WRT Bi_Gram list \n",
    "    P=[]     #list of detection the proper noune \n",
    "    FW=[]    \n",
    "    Xt,Xk,Xbi,Xp,Xb=1,1,1,1,1\n",
    "    weighted_node=[]\n",
    "    \n",
    "    \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "    \n",
    "        for j in FWL:\n",
    "            if(i==j[0]):\n",
    "                f=j[1]\n",
    "                F.append(f)\n",
    "                flag=1\n",
    "                \n",
    "        if(flag!=1): \n",
    "            F.append(0) \n",
    "        \n",
    " \n",
    "\n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in TWL:\n",
    "            if(i==j[0]):\n",
    "                t=j[1]\n",
    "                T.append(t)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "             T.append(0)\n",
    "                \n",
    "        \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in KWL:\n",
    "            if(i==j[0]):\n",
    "                k=j[1]\n",
    "                K.append(k)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            K.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in BWL:\n",
    "            if(i==j[0]):\n",
    "                b=j[1]\n",
    "                B.append(b)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            B.append(0)\n",
    "            \n",
    "            \n",
    "    for i in node_list:\n",
    "        flag=0\n",
    "        for j in PWL:\n",
    "            if(i==j):\n",
    "                P.append(1)\n",
    "                flag=1\n",
    "        if(flag!=1): \n",
    "            P.append(0)             \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in FWL:\n",
    "        fw=i[1]\n",
    "        FW.append(fw)\n",
    "    \n",
    "    if FW == []:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "    elif len(FW) != 0:\n",
    "        avrage_word_frequency=st.mean(FW)\n",
    "        median_word_frequency=st.median(FW) \n",
    "    else:\n",
    "        avrage_word_frequency= 0\n",
    "        median_word_frequency= 0 \n",
    "        \n",
    "    AW=abs(avrage_word_frequency-median_word_frequency)\n",
    "    \n",
    "    for i in range(len(node_list)):\n",
    "        WN=F[i]+Xt*AW*T[i]+Xk*AW*K[i]+Xb*AW*B[i]+Xp*AW*P[i]\n",
    "        weighted_node.append(WN)\n",
    "        node_weight_dict[node_list[i]] = WN\n",
    "    #print(K)\n",
    "    #print (F,T,K,FW,avrage_word_frequency,median_word_frequency)    \n",
    "    #print(weighted_node) \n",
    "    #print(node_weight_dict)\n",
    "    return node_weight_dict,weighted_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#edge between source and distenation\n",
    "def get_edge(source,distenation):\n",
    "    return edgeget[source,distenation]\n",
    "\n",
    "\n",
    "#distination node list\n",
    "def getweights_of_dist_nodes(source_destinations,sourcenode,All_nodes_ww):  \n",
    "    x=source_destinations[sourcenode]\n",
    "    distenation_node_list={}\n",
    "    weights=[]\n",
    "    nothing=0\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i])==('E#')) or (x[i] not in All_nodes_ww):\n",
    "          nothing=1\n",
    "        else:\n",
    "          distenation_node_list[x[i]]=All_nodes_ww[x[i]]\n",
    "          weights.append((All_nodes_ww[x[i]]))\n",
    "        if len(weights) != 0:\n",
    "            nothing=0\n",
    "    return distenation_node_list,weights,nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for candidate edges between a source and a destination given a criteria \n",
    "def get_candidate_edges(source_destinations,node_listx,wxx,out_carteria,All_weight,input_nodes,All_SD):\n",
    "    c=[]\n",
    "    xxx = []\n",
    "    no_list,totalweights=node_listx,All_weight\n",
    "\n",
    "    source_node_weight=0\n",
    "    distination_node_weight=0\n",
    "    average_of_the_node_weight=np.average(totalweights)#(np.sum(node_list)/number_of_nodes)\n",
    "    median_of_the_node_list_weight=np.median(totalweights)\n",
    "  \n",
    "    if average_of_the_node_weight>median_of_the_node_list_weight:#can be replaced with source=max(avg,median)\n",
    "        source_node_weight=average_of_the_node_weight\n",
    "    else:\n",
    "        source_node_weight=median_of_the_node_list_weight\n",
    "\n",
    "    for node,weight in no_list.items():\n",
    "        \n",
    "        if weight>=source_node_weight:#بيعرف اذا كانت بدايه الجمله ولا لا \n",
    "            outnodes1,outnodesweights,flagnothing=getweights_of_dist_nodes(source_destinations,node,no_list)#فنكشن جوا الجي بتجبلي كل الديستناشن بتاعت النودواختار بينهم هسقط مين\n",
    "            outnodes2,nodes2weights,flag2nothing=getweights_of_dist_nodes(All_SD,node,input_nodes)\n",
    "            outnodes1=[(i,j) for (i,j) in outnodes1.items() if j!=0 ] \n",
    "            outnodes2 =[(i,j) for (i,j) in outnodes2.items() if j!=0 ] \n",
    "        \n",
    "            if flagnothing==0:\n",
    "                #destination nodes exsist\n",
    "                #get their average and max weights for avg and max criterias\n",
    "                average_weight=np.average(nodes2weights)\n",
    "                max_weight=max(nodes2weights)\n",
    "            else:\n",
    "                #no destination nodes \n",
    "                average_weight=0\n",
    "                max_weight=0\n",
    "            # get threshold of destination weights according to criteria \n",
    "            if (out_carteria ==\"avg\"):\n",
    "                distination_node_weight=average_weight\n",
    "            elif (out_carteria ==\"node_avg\"):\n",
    "                distination_node_weight=source_node_weight\n",
    "            elif (out_carteria==\"max\"):\n",
    "                distination_node_weight=max_weight\n",
    "            else:\n",
    "                distination_node_weight=average_weight\n",
    "\n",
    "\n",
    "            if (bool(outnodes1)==False):\n",
    "                # no destination nodes \n",
    "                nothing=1  \n",
    "            else: \n",
    "                # check for the candidate edges\n",
    "                for out,weight_of_out_node in outnodes1:\n",
    "                    if weight_of_out_node>=distination_node_weight and flagnothing==0:\n",
    "                        c.append(get_edge(node,out))# append the edge\n",
    "                        xxx.append([node,out])\n",
    "\n",
    "    cx=[]\n",
    "    for i in range(0,len(c)):\n",
    "        cx.append(c[i][0])\n",
    "    return cx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for post-processing and Algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_order(count_summ, index):\n",
    "    return(count_summ - index)/count_summ\n",
    "\n",
    "def compute_rank(candidate_summary):#, W, core_words_list, bigrams_list, lsa_weight):\n",
    "    ranks = {}\n",
    "    candidate_summary = reorder(candidate_summary)\n",
    "    \n",
    "    for i, sent in enumerate(candidate_summary):\n",
    "        \n",
    "        rank = sentence_order(len(candidate_summary), i)\n",
    "        ranks[sent] = rank\n",
    "        #print(\"index\",i,\"sentence\",sent,\"rank\",rank)\n",
    "    sort_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)    \n",
    "    return ranks   \n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "def get_words_count(sent):\n",
    "    len_sent = 0\n",
    "    for i in sent.split(\" \"):\n",
    "        len_sent = len_sent +1\n",
    "    return len_sent\n",
    "\n",
    "# reorder the summary based on the order from the input text\n",
    "def reorder(summary):\n",
    "    index_summ = []\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    order = dict()\n",
    "    final_order = []\n",
    "    for sent in summary:\n",
    "        if sent in value_sent:\n",
    "            index_sent = value_sent.index(sent)\n",
    "            index_summ.append(index_sent)\n",
    "            order[index_sent] = sent\n",
    "            \n",
    "    for i in sorted(order):\n",
    "        final_order.append(order[i])\n",
    "    return final_order\n",
    "\n",
    "def concat(final_order):\n",
    "    return(\"\\n\".join(final_order))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing and algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(list_of_candidates_summeries):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    sentences_ranks = compute_rank(candidate_summary)\n",
    "   \n",
    "    sentences_ordered =  [key for key,value  in sentences_ranks.items()]\n",
    "    summary_length =0\n",
    "    visited_sentences = []\n",
    "    \n",
    "    sentences = sentences_ordered\n",
    "\n",
    "    tfidfvect = TfidfVectorizer(stop_words='english')\n",
    "    X = tfidfvect.fit_transform(sentences)\n",
    "\n",
    "    if len(sentences) <1:\n",
    "        n_clusters = len(sentences)\n",
    "    else:\n",
    "        n_clusters = 1\n",
    "    clf = KMeans(n_clusters = n_clusters,init = 'k-means++', max_iter = 1000)\n",
    "    labels = clf.fit_predict(X)\n",
    "    clustered_sentences =[[] for i in range(5)]\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['data_index'] = sentences\n",
    "    cluster_map['cluster'] = clf.labels_\n",
    "    #print(cluster_map)\n",
    "    return n_clusters,cluster_map,sentences_ordered \n",
    "\n",
    "def algorithm_4(list_of_candidates_summeries,summary_length,threshold,max_words_limit):\n",
    "    candidate_summary = list_of_candidates_summeries\n",
    "    summary_length = 0\n",
    "    while (len(visited_sentences) < len(candidate_summary)) and ((summary_length)< max_words_limit) :\n",
    "        for i in range(n_clusters):\n",
    "            clustered_sentences = cluster_map[cluster_map.cluster == i]['data_index'].tolist()\n",
    "            if summary_length >= (max_words_limit+threshold):\n",
    "                break\n",
    "            for sent in clustered_sentences:\n",
    "                if sent not in visited_sentences :\n",
    "                    temp_length = summary_length + 1\n",
    "                    if temp_length < max_words_limit :\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif(temp_length >= max_words_limit) and (temp_length <= (max_words_limit + threshold)):\n",
    "                        summary_length = temp_length\n",
    "                        final_summary.append(sent)\n",
    "                        visited_sentences.append(sent)\n",
    "                        break\n",
    "                    elif temp_length > (max_words_limit + threshold):\n",
    "                        visited_sentences.append(sent)\n",
    "    return final_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(outsumm,ref_summ):\n",
    "    score={}\n",
    "\n",
    "    #generated_summary=\" In a dramatic finish, the San Francisco 49ers football team won the Super Bowl on Sunday with a 20–16 victory over tCincinnati Bengals. Joe Montana's 10-yard touchdown pass to John Taylor with 34 seconds left provided the winningmargin. The victory was achieved through the brilliance of Montana and Jerry Rice, the wide receiver who caught 12passes for 222 yards, both Super Bowl records. It also gave 49ers Coach Bill Walsh his third Super Bowl win. The firsttouchdown didn't come until 44 minutes into the 60-minute game, when Cincinnati's Stanford Jennings took a kickoff 93yards for a score\"\n",
    "    generated_summary = outsumm\n",
    "    #refrence1=\"The San Francisco 49ers won the Super Bowl Sunday with a dramatic 20–16 victory over the Cincinnati Bengals. JoeMontana's 10-yard touchdown pass to John Taylor with 34 seconds remaining provided the win. The pass gave Montana aSuper Bowl record with 357 yards passing. The victory was achieved through the brilliance of Montana and Jerry Rice, thewide receiver who caught 12 passes for 222 yards, both Super Bowl records. Rice was named the game's most valuableplayer. It was the fifth straight win for a National Football Conference team and the third Super Bowl win for Coach Bill\"\n",
    "    #refrence2=\"Joe Montana's 10-yard touchdown pass to John Taylor with 34 seconds left in the game gave the San Francisco 49ers a20–16 Super Bowl victory over the Cincinnati Bengals on Sunday. The touchdown gave Montana a Super Bowl record with357 yards passing. Wide receiver Jerry Rice caught 12 passes for 222 yards, both Super Bowl records, and was named thegame's most valuable player. It was the third Super Bowl win for 49ers Coach Bill Walsh. It was the fifth straight win for theNFC team and the most dramatic. The previous four had an average score of 41–1\"\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "    s_R1 =[]\n",
    "    s_R2 = []\n",
    "    s_RL = []\n",
    "    for summ in ref_summ:\n",
    "        score = scorer.score( str(summ[0]) ,generated_summary)\n",
    "        s_R1.append(score['rouge1'][2])\n",
    "        s_R2.append(score['rouge2'][2])\n",
    "        s_RL.append(score['rougeLsum'][2])\n",
    "\n",
    "    scores_max = [max(s_R1), max(s_R2) , max(s_RL)]\n",
    "    scores_avg = [st.mean(s_R1), st.mean(s_R2) , st.mean(s_RL)]\n",
    "    #print(scores_avg)\n",
    "    return scores_max , scores_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions needed for Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a vertex to the dictionary\n",
    "def add_vertex(v):\n",
    "  global graph\n",
    "  global vertices_no\n",
    "  if v not in graph:\n",
    "    vertices_no = vertices_no + 1\n",
    "    #print(\"vertices_no \",vertices_no, v)\n",
    "    graph[v] = []\n",
    "\n",
    "# Add an edge between vertex v1 and v2 with edge weight e\n",
    "#v1 is the source node and v2 is the distenation node\n",
    "edgeget={}\n",
    "def add_edge(v1, v2, e_order, e_value):\n",
    "  global graph\n",
    "  # Check if vertex v1 is a valid vertex\n",
    "  if ((v1 in graph) and (v2 in graph)):\n",
    "    #print(v1,v2,e_order,e_value)  \n",
    "    temp = [v2, e_order, e_value]\n",
    "    edgeget[v1,v2]=[e_value]\n",
    "    graph[v1].append(temp)\n",
    "\n",
    "# Print the graph\n",
    "def print_graph():\n",
    "  global graph\n",
    "  for vertex in graph:\n",
    "    for edges in graph[vertex]:\n",
    "      #print(\"evet\")\n",
    "      print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "# get the destinations\n",
    "def get_destinations(source):\n",
    "    global graph\n",
    "    distinations=[]\n",
    "    if(source in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0] not in distinations):\n",
    "                distinations.append(elements[0])\n",
    "    return distinations\n",
    "\n",
    "# get the destinations\n",
    "def get_edges(source,dist):\n",
    "    global graph\n",
    "    Edge=\"\"\n",
    "    if(source in graph and dist in graph):\n",
    "        for elements in graph.get(source):\n",
    "            if(elements[0]==dist):\n",
    "                Edge= Edge + elements[2]\n",
    "    return Edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria):\n",
    "    All_SD = {}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}      \n",
    "    nodes_list = []\n",
    "    input_text = candidate_summ\n",
    "    last_summ_len = candidate_summ_len\n",
    "    Last_summ = candidate_summ\n",
    "    sent_idx = can_edges\n",
    "    can_edges=[]\n",
    "    List_of_sent = list_pre_cansumm\n",
    "    list_pre_cansumm = []\n",
    "    list_of_candidates_summeries = []\n",
    "    All_nodes=[]\n",
    "    nodes_list = []\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "        \n",
    "        Types=pos_tag(Words)\n",
    "        for Type in Types:\n",
    "            if (Type[1] == 'NN' or Type[1] == 'NNP' or Type[1] == 'NNS' or Type[1] == 'NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\") \n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        LIST_OF_NODES=[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "        \n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    ")\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "            \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "        #print (\"graph: \", graph)\n",
    "        #\n",
    "        \n",
    "        #****************************************************************************\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[index] = (source_destinations,list_of_nodes)\n",
    "    \n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        index = index + 1\n",
    "        \n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    index = 0\n",
    "    for sent_index, sentence in zip(sent_idx,List_of_sent): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        \n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[index] and (List_of_sent[index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[index])\n",
    "                can_edges.append(sent_index)\n",
    "\n",
    "        index = index + 1\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    candidate_summ_len = 0\n",
    "    for index ,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + get_words_count(sent)\n",
    "\n",
    "    return list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sent):\n",
    "    key_sent = list(keyvalue.keys())\n",
    "    value_sent = list(keyvalue.values())\n",
    "    index_sent = value_sent.index(sent)\n",
    "    return index_sent    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function That tests the model and gets the rouge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625724635033125 0.408230014684056 0.48050801400072307\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "final_scores_1m = []\n",
    "final_scores_2m = []\n",
    "final_scores_Lm = []\n",
    "\n",
    "final_scores_1a = []\n",
    "final_scores_2a = []\n",
    "final_scores_La = []\n",
    "\n",
    "\n",
    "for i in range(0,len(Data)):\n",
    "    can_edges = []\n",
    "    data = tr_Data[i]\n",
    "    summ = tr_Summ[i]\n",
    "    title,keywords,keyvalue, key_value_list,summary_ref, lang_Dict = read_input(data,summ) \n",
    "    out_carteria=\"avg\"\n",
    "    \n",
    "    (title,keywords,Sentences_list,final_input,all_bigrams,bi_count_list,word_freq\n",
    "         ,all_words,finalwordfreq,List_of_sent,title_word_freq,keyword_word_freq) = preprocess(keyvalue)\n",
    "    \n",
    "    bi_count = bigrams_freq(bi_count_list)\n",
    "    Last_summ = keyvalue.values()\n",
    "    last_summ_len = 0\n",
    "    for value in keyvalue.values():\n",
    "        last_summ_len = last_summ_len + 1\n",
    "    \n",
    "    \n",
    "    All_nodes=[]\n",
    "    list_of_candidates_summeries=[]\n",
    "    list_pre_cansumm = []\n",
    "    All_SD = {}\n",
    "    \n",
    "    ##tokanization  \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    ##lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    # Add a vertex to the dictionary\n",
    "    def add_vertex(v):\n",
    "      global graph\n",
    "      global vertices_no\n",
    "      if v not in graph:\n",
    "        vertices_no = vertices_no + 1\n",
    "        #print(\"vertices_no \",vertices_no, v)\n",
    "        graph[v] = []\n",
    "\n",
    "    # Add an edge between vertex v1 and v2 with edge weight e\n",
    "    #v1 is the source node and v2 is the distenation node\n",
    "    edgeget={}\n",
    "    def add_edge(v1, v2, e_order, e_value):\n",
    "      global graph\n",
    "      # Check if vertex v1 is a valid vertex\n",
    "      if ((v1 in graph) and (v2 in graph)):\n",
    "        #print(v1,v2,e_order,e_value)  \n",
    "        temp = [v2, e_order, e_value]\n",
    "        edgeget[v1,v2]=[e_value]\n",
    "        graph[v1].append(temp)\n",
    "\n",
    "    # Print the graph\n",
    "    def print_graph():\n",
    "      global graph\n",
    "      for vertex in graph:\n",
    "        for edges in graph[vertex]:\n",
    "          print(vertex, \" -> \", edges[0], \"    edge order:\", edges[1], \"    edge value:\", edges[2])\n",
    "\n",
    "    # get the destinations\n",
    "    def get_destinations(source):\n",
    "        global graph\n",
    "        distinations=[]\n",
    "        if(source in graph):\n",
    "            for elements in graph.get(source):\n",
    "                if(elements[0] not in distinations):\n",
    "                    distinations.append(elements[0])\n",
    "        return distinations\n",
    "\n",
    "    # get the destinations\n",
    "    def get_edges(source,dist):\n",
    "        global graph\n",
    "        Edge=\"\"\n",
    "        if(source in graph and dist in graph):\n",
    "            for elements in graph.get(source):\n",
    "                if(elements[0]==dist):\n",
    "                    Edge= Edge + elements[2]\n",
    "        return Edge\n",
    "\n",
    "    # driver code\n",
    "    graph = {} \n",
    "    # stores the number of vertices in the graph\n",
    "    vertices_no = 0    \n",
    "    S = List_of_sent\n",
    "    source_destinations={}\n",
    "    input_nodes = []\n",
    "    sent_nodes = {}\n",
    "    Pnouns = []\n",
    "    \n",
    "    for sent_index, sentence in enumerate(List_of_sent.values()):\n",
    "        nodes_list = [\"S#\"]\n",
    "        edges_list = [\"\"]\n",
    "        counter =0\n",
    "        edge_value =\"\"\n",
    "        \n",
    "        Words=tokenizer.tokenize(sentence)\n",
    "\n",
    "        Types=pos_tag(Words)\n",
    "        for Type in Types:\n",
    "            if (Type[1] == 'NN' or Type[1] == 'NNP' or Type[1] == 'NNS' or Type[1] == 'NNPS' or Type[1]=='VB' or Type[1]=='VBG' or Type[1]=='VBD' or Type[1]=='VBZ'):\n",
    "                nodes_list.append(Type[0])\n",
    "                edges_list.append(\"\")\n",
    "                if(Type[1] == 'NNP'):\n",
    "                    Pnouns.append(\"\")\n",
    "            \n",
    "            else:\n",
    "                edges_list.append(Type[0])\n",
    "                nodes_list.append(\"\")\n",
    "        #print(\"nodes_edges: \",nodes_list)\n",
    "        #print(\"edges_nodes: \",edges_list) \n",
    "\n",
    "\n",
    "        list_of_nodes =[]\n",
    "        for i in nodes_list:\n",
    "            if(i != \"\"):\n",
    "                add_vertex(i)\n",
    "                list_of_nodes.append(i)\n",
    "                final_node =i\n",
    "        #print(\"final node= \",final_node)\n",
    "\n",
    "        list_of_nodes.remove(\"S#\")    \n",
    "        add_vertex(\"E#\")\n",
    "        #list_of_nodes.append(\"E#\")\n",
    "        nodes_list.append(\"E#\")\n",
    "        edges_list.append(\"\")\n",
    "\n",
    "        counter =0\n",
    "        order =0\n",
    "        \n",
    "        for j in nodes_list :\n",
    "            ##print(\"evet \", nodes_list[counter],counter,nodes_list[37])\n",
    "            if(nodes_list[counter+1] !=\"\"):\n",
    "                add_edge(nodes_list[counter], nodes_list[counter+1], order, \"\")\n",
    "                order =order +1\n",
    "            else:\n",
    "                x= counter\n",
    "                y= edges_list[counter+1]\n",
    "                while(nodes_list[counter+1] ==\"\" and nodes_list[x] !=\"\"):\n",
    "                    counter =counter +1\n",
    "                    if(counter ==len(nodes_list)-2):\n",
    "                        y= y+\" \"+ edges_list[counter+1]\n",
    "                add_edge(nodes_list[x], nodes_list[counter+1], order, y)\n",
    "                order =order +1\n",
    "            counter =counter +1\n",
    "            if(counter == len(nodes_list)-1):\n",
    "                break\n",
    "                \n",
    "        LIST_OF_NODES=[]\n",
    "        for k in list_of_nodes:\n",
    "            if(k not in LIST_OF_NODES):\n",
    "                LIST_OF_NODES.append(k)\n",
    "\n",
    "        \n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in source_destinations.keys()):\n",
    "                source_destinations[s]=get_destinations(s)\n",
    "            else:\n",
    "                source_destinations[s]= source_destinations[s]+ get_destinations(s)\n",
    "\n",
    "        for s in LIST_OF_NODES:\n",
    "            if(s not in All_SD.keys()):\n",
    "                All_SD[s] = get_destinations(s)\n",
    "            else:\n",
    "                All_SD[s] = All_SD[s] + get_destinations(s)\n",
    "\n",
    "        #****************************************************************************\n",
    "        input_nodes = input_nodes + list_of_nodes\n",
    "        sent_nodes[sent_index] = (source_destinations,list_of_nodes)\n",
    "\n",
    "        #****************************************************************************\n",
    "        graph = {}\n",
    "        source_destinations={}\n",
    "        #****************************************************************************\n",
    "    input_nodes = list(set(input_nodes))\n",
    "    dic_of_nodes_and_weights,list_of_weights=get_weights(input_nodes)\n",
    "    All_weight = list_of_weights\n",
    "    \n",
    "    for sent_index, sentence in enumerate(S.values()): \n",
    "        (source_destinations,list_of_nodes) = sent_nodes[sent_index]\n",
    "        nodes_weights = {}\n",
    "        weights = []\n",
    "        for node in list_of_nodes:\n",
    "            nodes_weights[node] = dic_of_nodes_and_weights[node]\n",
    "            weights.append(dic_of_nodes_and_weights[node])\n",
    "\n",
    "        list_of_can_edges=get_candidate_edges(source_destinations,nodes_weights,weights,out_carteria,All_weight,dic_of_nodes_and_weights,All_SD)\n",
    "        if len(list_of_can_edges)>=1:#we will make it a variable depends on the length of the sentence\n",
    "            if(List_of_sent[sent_index] and (List_of_sent[sent_index].strip()) ):\n",
    "                list_of_candidates_summeries.append(keyvalue[sent_index])\n",
    "                list_pre_cansumm.append(List_of_sent[sent_index])\n",
    "                can_edges.append(sent_index)\n",
    "                \n",
    "\n",
    "    max_words_limit = int(len(keyvalue)*0.4)\n",
    "    threshold = 1\n",
    "    final_summary = []\n",
    "    visited_sentences = []\n",
    "    summary_length = 0\n",
    "        \n",
    "    candidate_summ = reorder(list_of_candidates_summeries)\n",
    "    \n",
    "    candidate_summ_len = 0\n",
    "    for index,sent in enumerate(candidate_summ):\n",
    "        candidate_summ_len = candidate_summ_len + 1\n",
    "    \n",
    "\n",
    "    while(True):\n",
    "        if(candidate_summ_len > (max_words_limit + threshold)):\n",
    "            if(candidate_summ_len != last_summ_len):\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria) \n",
    "\n",
    "            elif(out_carteria != \"max\"):\n",
    "                out_carteria=\"max\"\n",
    "                list_pre_cansumm,candidate_summ,candidate_summ_len,Last_summ,last_summ_len,can_edges = iter_graph(can_edges,list_pre_cansumm,candidate_summ,candidate_summ_len,out_carteria)\n",
    "            else:\n",
    "                list_of_candidates_summeries = candidate_summ\n",
    "                break\n",
    "        elif(candidate_summ_len < (max_words_limit)):\n",
    "            candidate_summ = Last_summ\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "        else:\n",
    "            list_of_candidates_summeries = candidate_summ\n",
    "            break\n",
    "    \n",
    "            \n",
    "        \n",
    "    n_clusters,cluster_map,sentences_ordered  = post_processing(list_of_candidates_summeries)\n",
    "\n",
    "    final_summary = algorithm_4(sentences_ordered ,summary_length,threshold,max_words_limit)\n",
    "    final = reorder(final_summary)\n",
    "    outsumm = concat(final)\n",
    "    scores_m , scores_a = get_score(outsumm,summary_ref)\n",
    "\n",
    "    ##this avrage with respect to Recall\n",
    "    \n",
    "    final_scores_1a.append(scores_a[0])\n",
    "    final_scores_2a.append(scores_a[1])\n",
    "    final_scores_La.append(scores_a[2])\n",
    "    \n",
    "\n",
    "\n",
    "avrage_1=st.mean(final_scores_1a)\n",
    "avrage_2=st.mean(final_scores_2a)\n",
    "avrage_L=st.mean(final_scores_La)\n",
    "print(avrage_1,avrage_2,avrage_L)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
